{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6bf330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"mermaid\">\n",
       "    flowchart TD\n",
       "A[Init Runtime] --> B[Detect Type (Heuristics + LLM)]\n",
       "B --> C[Parse & Extract (LLM-first)]\n",
       "C --> D[Fetch Candidates (Crossref/OpenAlex/S2/PubMed/arXiv)]\n",
       "D --> E[Select Best (Consensus Scoring)]\n",
       "E --> T[Reconcile Type]\n",
       "T --> F[Verification Agents (Threaded) + Progress Metrics]\n",
       "F -->|repair| G[Apply Corrections (Authority + Agents)]\n",
       "G --> I[LLM Correction (JSON-only)]\n",
       "I --> X[Enrich From Best]\n",
       "X --> D2[Re-Fetch Candidates]\n",
       "D2 --> E2[Re-Select Best]\n",
       "E2 --> T2[Reconcile Type]\n",
       "T2 --> F2[Re-Verify + loop/stagnation guards]\n",
       "F -->|exit| H[Format IEEE]\n",
       "H --> J[Build CSL-JSON & BibTeX]\n",
       "J --> R[Human Report]\n",
       "style H fill:#e0f7fa,stroke:#006064,stroke-width:1px\n",
       "style R fill:#f1f8e9,stroke:#33691e,stroke-width:1px\n",
       "style D fill:#fff3e0,stroke:#e65100,stroke-width:1px\n",
       "style F fill:#ede7f6,stroke:#4527a0,stroke-width:1px\n",
       "style T fill:#e8f5e9,stroke:#2e7d32,stroke-width:1px\n",
       "\n",
       "    </div>\n",
       "    <script>\n",
       "      (function() {\n",
       "        function init() {\n",
       "          mermaid.initialize({startOnLoad:true});\n",
       "        }\n",
       "        if (!window.mermaid) {\n",
       "          var s = document.createElement('script');\n",
       "          s.src = 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js';\n",
       "          s.onload = init;\n",
       "          document.head.appendChild(s);\n",
       "        } else {\n",
       "          init();\n",
       "        }\n",
       "      })();\n",
       "    </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```mermaid\n",
       "flowchart TD\n",
       "A[Init Runtime] --> B[Detect Type (Heuristics + LLM)]\n",
       "B --> C[Parse & Extract (LLM-first)]\n",
       "C --> D[Fetch Candidates (Crossref/OpenAlex/S2/PubMed/arXiv)]\n",
       "D --> E[Select Best (Consensus Scoring)]\n",
       "E --> T[Reconcile Type]\n",
       "T --> F[Verification Agents (Threaded) + Progress Metrics]\n",
       "F -->|repair| G[Apply Corrections (Authority + Agents)]\n",
       "G --> I[LLM Correction (JSON-only)]\n",
       "I --> X[Enrich From Best]\n",
       "X --> D2[Re-Fetch Candidates]\n",
       "D2 --> E2[Re-Select Best]\n",
       "E2 --> T2[Reconcile Type]\n",
       "T2 --> F2[Re-Verify + loop/stagnation guards]\n",
       "F -->|exit| H[Format IEEE]\n",
       "H --> J[Build CSL-JSON & BibTeX]\n",
       "J --> R[Human Report]\n",
       "style H fill:#e0f7fa,stroke:#006064,stroke-width:1px\n",
       "style R fill:#f1f8e9,stroke:#33691e,stroke-width:1px\n",
       "style D fill:#fff3e0,stroke:#e65100,stroke-width:1px\n",
       "style F fill:#ede7f6,stroke:#4527a0,stroke-width:1px\n",
       "style T fill:#e8f5e9,stroke:#2e7d32,stroke-width:1px\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-14 20:52:17,641 [INFO] httpx: HTTP Request: POST https://kroki.io/mermaid/svg \"HTTP/1.1 400 Bad Request\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"event\": \"http_request\", \"url\": \"https://kroki.io/mermaid/svg\", \"code\": 400}\n",
      "Kroki 400: Mermaid syntax or styling may be invalid. Try removing `style` lines or simplify labels.\n",
      "\n",
      "Tip: create a .env with the keys you use. Example:\n",
      " # One of these providers is enough:\n",
      "OPENAI_API_KEY=sk-...\n",
      "OPENAI_MODEL=gpt-4o-mini\n",
      "\n",
      "# OR Azure OpenAI:\n",
      "AZURE_OPENAI_API_KEY=...\n",
      "AZURE_OPENAI_ENDPOINT=https://<your-resource>.openai.azure.com\n",
      "AZURE_OPENAI_DEPLOYMENT=gpt-4o-base\n",
      "OPENAI_API_VERSION=2024-06-01\n",
      "\n",
      "# OR Anthropic:\n",
      "ANTHROPIC_API_KEY=...\n",
      "ANTHROPIC_MODEL=claude-3-5-sonnet-20240620\n",
      "\n",
      "# OR Ollama (local):\n",
      "OLLAMA_BASE_URL=http://localhost:11434\n",
      "OLLAMA_MODEL=llama3.2\n",
      "\n",
      "# Optional tuning:\n",
      "IEEE_REF_TIMEOUT=12\n",
      "IEEE_REF_CONCURRENCY=8\n",
      "IEEE_REF_CACHE_TTL=3600\n",
      "IEEE_REF_MAX_CORR=3\n",
      "IEEE_REF_MAX_HOPS=12\n",
      "IEEE_REF_STAGNATION=2\n",
      "IEEE_REF_AGENT_THREADS=6\n",
      "IEEE_REF_LOG_LEVEL=INFO\n",
      "IEEE_REF_RECURSION_LIMIT=60\n",
      "\n",
      "# Optional:\n",
      "SEMANTIC_SCHOLAR_API_KEY=...\n",
      "{\"event\": \"llm_provider_selected\", \"provider\": \"azure\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-14 20:52:17,934 [INFO] openai._base_client: Retrying request to /chat/completions in 0.403671 seconds\n",
      "2025-08-14 20:52:18,344 [INFO] openai._base_client: Retrying request to /chat/completions in 0.839721 seconds\n",
      "2025-08-14 20:52:19,192 [WARNING] ieee-ref-langgraph: LLM json() failed: Connection error.\n",
      "2025-08-14 20:52:19,197 [INFO] openai._base_client: Retrying request to /chat/completions in 0.465672 seconds\n",
      "2025-08-14 20:52:19,666 [INFO] openai._base_client: Retrying request to /chat/completions in 0.805921 seconds\n",
      "2025-08-14 20:52:20,480 [WARNING] ieee-ref-langgraph: LLM json() failed: Connection error.\n",
      "/var/folders/9g/tnv_kfvn5kx8s1x4rjvf1gmr0000gn/T/ipykernel_95299/3553259949.py:909: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  parsed = ExtractedModel(**parsed).dict(exclude_none=True)\n",
      "2025-08-14 20:52:20,702 [INFO] httpx: HTTP Request: GET https://api.openalex.org/works?filter=title.search%3ARecurrent+radial+basis+function+network-based+fuzzy+neural+network+control+for+permanent-magnet+linear+synchronous+motor+servo+drive%2C&per-page=5 \"HTTP/1.1 403 FORBIDDEN\"\n",
      "2025-08-14 20:52:20,718 [INFO] httpx: HTTP Request: GET https://export.arxiv.org/api/query?search_query=ti%3A%22Recurrent+radial+basis+function+network-based+fuzzy+neural+network+control+for+permanent-magnet+linear+synchronous+motor+servo+drive%2C%22&start=0&max_results=1 \"HTTP/1.1 200 OK\"\n",
      "2025-08-14 20:52:20,846 [INFO] httpx: HTTP Request: GET https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term=Recurrent+radial+basis+function+network-based+fuzzy+neural+network+control+for+permanent-magnet+linear+synchronous+motor+servo+drive%2C&retmode=json&retmax=1&tool=ieee-ref-agent&email=you%40example.com \"HTTP/1.1 200 OK\"\n",
      "2025-08-14 20:52:20,871 [INFO] httpx: HTTP Request: GET https://api.crossref.org/works?query.title=Recurrent+radial+basis+function+network-based+fuzzy+neural+network+control+for+permanent-magnet+linear+synchronous+motor+servo+drive%2C&rows=5&select=title%2Cauthor%2Ccontainer-title%2Cshort-container-title%2Cissued%2CDOI%2Cpage%2Cvolume%2Cissue%2Cpublished-print%2Cpublished-online%2Ctype \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"event\": \"http_request\", \"url\": \"https://api.openalex.org/works?filter=title.search%3ARecurrent+radial+basis+function+network-based+fuzzy+neural+network+control+for+permanent-magnet+linear+synchronous+motor+servo+drive%2C&per-page=5\", \"code\": 403}\n",
      "{\"event\": \"http_request\", \"url\": \"https://export.arxiv.org/api/query?search_query=ti%3A%22Recurrent+radial+basis+function+network-based+fuzzy+neural+network+control+for+permanent-magnet+linear+synchronous+motor+servo+drive%2C%22&start=0&max_results=1\", \"code\": 200}\n",
      "{\"event\": \"http_request\", \"url\": \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term=Recurrent+radial+basis+function+network-based+fuzzy+neural+network+control+for+permanent-magnet+linear+synchronous+motor+servo+drive%2C&retmode=json&retmax=1&tool=ieee-ref-agent&email=you%40example.com\", \"code\": 200}\n",
      "{\"event\": \"http_request\", \"url\": \"https://api.crossref.org/works?query.title=Recurrent+radial+basis+function+network-based+fuzzy+neural+network+control+for+permanent-magnet+linear+synchronous+motor+servo+drive%2C&rows=5&select=title%2Cauthor%2Ccontainer-title%2Cshort-container-title%2Cissued%2CDOI%2Cpage%2Cvolume%2Cissue%2Cpublished-print%2Cpublished-online%2Ctype\", \"code\": 200}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-14 20:52:21,000 [INFO] httpx: HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=Recurrent+radial+basis+function+network-based+fuzzy+neural+network+control+for+permanent-magnet+linear+synchronous+motor+servo+drive%2C&limit=5&fields=title%2Cvenue%2Cyear%2Cauthors%2CexternalIds%2CpublicationVenue%2CpublicationTypes \"HTTP/1.1 200 OK\"\n",
      "2025-08-14 20:52:21,110 [INFO] httpx: HTTP Request: GET https://api.openalex.org/works?filter=title.search%3ARecurrent+radial+basis+function+network-based+fuzzy+neural+network+control+for+permanent-magnet+linear+synchronous+motor+servo+drive%2C&per-page=5 \"HTTP/1.1 403 FORBIDDEN\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"event\": \"http_request\", \"url\": \"https://api.semanticscholar.org/graph/v1/paper/search?query=Recurrent+radial+basis+function+network-based+fuzzy+neural+network+control+for+permanent-magnet+linear+synchronous+motor+servo+drive%2C&limit=5&fields=title%2Cvenue%2Cyear%2Cauthors%2CexternalIds%2CpublicationVenue%2CpublicationTypes\", \"code\": 200}\n",
      "{\"event\": \"http_request\", \"url\": \"https://api.openalex.org/works?filter=title.search%3ARecurrent+radial+basis+function+network-based+fuzzy+neural+network+control+for+permanent-magnet+linear+synchronous+motor+servo+drive%2C&per-page=5\", \"code\": 403}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-14 20:52:21,808 [INFO] httpx: HTTP Request: GET https://api.openalex.org/works?filter=title.search%3ARecurrent+radial+basis+function+network-based+fuzzy+neural+network+control+for+permanent-magnet+linear+synchronous+motor+servo+drive%2C&per-page=5 \"HTTP/1.1 403 FORBIDDEN\"\n",
      "2025-08-14 20:52:21,819 [INFO] openai._base_client: Retrying request to /chat/completions in 0.445149 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"event\": \"http_request\", \"url\": \"https://api.openalex.org/works?filter=title.search%3ARecurrent+radial+basis+function+network-based+fuzzy+neural+network+control+for+permanent-magnet+linear+synchronous+motor+servo+drive%2C&per-page=5\", \"code\": 403}\n",
      "{\"event\": \"route_after_verify\", \"decision\": \"ApplyCorrections\", \"ver_score\": 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-14 20:52:22,270 [INFO] openai._base_client: Retrying request to /chat/completions in 0.800506 seconds\n",
      "2025-08-14 20:52:23,077 [WARNING] ieee-ref-langgraph: LLM json() failed: Connection error.\n",
      "2025-08-14 20:52:23,135 [INFO] httpx: HTTP Request: GET https://api.crossref.org/works/10.1109/tmag.2006.880995 \"HTTP/1.1 200 OK\"\n",
      "2025-08-14 20:52:23,197 [INFO] httpx: HTTP Request: GET https://api.openalex.org/works?filter=doi%3A10.1109%2Ftmag.2006.880995 \"HTTP/1.1 200 OK\"\n",
      "2025-08-14 20:52:23,266 [INFO] httpx: HTTP Request: GET https://export.arxiv.org/api/query?search_query=ti%3A%22Recurrent+radial+basis+function+network-based+fuzzy+neural+network+control+for+permanent-magnet+linear+synchronous+motor+servo+drive%22&start=0&max_results=1 \"HTTP/1.1 200 OK\"\n",
      "2025-08-14 20:52:23,272 [INFO] httpx: HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/DOI:10.1109/tmag.2006.880995?fields=title%2Cvenue%2Cyear%2Cauthors%2CexternalIds%2CpublicationVenue%2CpublicationTypes \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"event\": \"http_request\", \"url\": \"https://api.crossref.org/works/10.1109/tmag.2006.880995\", \"code\": 200}\n",
      "{\"event\": \"http_request\", \"url\": \"https://api.openalex.org/works?filter=doi%3A10.1109%2Ftmag.2006.880995\", \"code\": 200}\n",
      "{\"event\": \"http_request\", \"url\": \"https://export.arxiv.org/api/query?search_query=ti%3A%22Recurrent+radial+basis+function+network-based+fuzzy+neural+network+control+for+permanent-magnet+linear+synchronous+motor+servo+drive%22&start=0&max_results=1\", \"code\": 200}\n",
      "{\"event\": \"http_request\", \"url\": \"https://api.semanticscholar.org/graph/v1/paper/DOI:10.1109/tmag.2006.880995?fields=title%2Cvenue%2Cyear%2Cauthors%2CexternalIds%2CpublicationVenue%2CpublicationTypes\", \"code\": 200}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-14 20:52:23,375 [INFO] httpx: HTTP Request: GET https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term=Recurrent+radial+basis+function+network-based+fuzzy+neural+network+control+for+permanent-magnet+linear+synchronous+motor+servo+drive&retmode=json&retmax=1&tool=ieee-ref-agent&email=you%40example.com \"HTTP/1.1 200 OK\"\n",
      "2025-08-14 20:52:23,551 [INFO] httpx: HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=Recurrent+radial+basis+function+network-based+fuzzy+neural+network+control+for+permanent-magnet+linear+synchronous+motor+servo+drive&limit=5&fields=title%2Cvenue%2Cyear%2Cauthors%2CexternalIds%2CpublicationVenue%2CpublicationTypes \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"event\": \"http_request\", \"url\": \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term=Recurrent+radial+basis+function+network-based+fuzzy+neural+network+control+for+permanent-magnet+linear+synchronous+motor+servo+drive&retmode=json&retmax=1&tool=ieee-ref-agent&email=you%40example.com\", \"code\": 200}\n",
      "{\"event\": \"http_request\", \"url\": \"https://api.semanticscholar.org/graph/v1/paper/search?query=Recurrent+radial+basis+function+network-based+fuzzy+neural+network+control+for+permanent-magnet+linear+synchronous+motor+servo+drive&limit=5&fields=title%2Cvenue%2Cyear%2Cauthors%2CexternalIds%2CpublicationVenue%2CpublicationTypes\", \"code\": 200}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-14 20:52:23,637 [INFO] httpx: HTTP Request: GET https://api.crossref.org/works?query.title=Recurrent+radial+basis+function+network-based+fuzzy+neural+network+control+for+permanent-magnet+linear+synchronous+motor+servo+drive&rows=5&select=title%2Cauthor%2Ccontainer-title%2Cshort-container-title%2Cissued%2CDOI%2Cpage%2Cvolume%2Cissue%2Cpublished-print%2Cpublished-online%2Ctype \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"event\": \"http_request\", \"url\": \"https://api.crossref.org/works?query.title=Recurrent+radial+basis+function+network-based+fuzzy+neural+network+control+for+permanent-magnet+linear+synchronous+motor+servo+drive&rows=5&select=title%2Cauthor%2Ccontainer-title%2Cshort-container-title%2Cissued%2CDOI%2Cpage%2Cvolume%2Cissue%2Cpublished-print%2Cpublished-online%2Ctype\", \"code\": 200}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-14 20:52:23,897 [INFO] httpx: HTTP Request: GET https://api.openalex.org/works?filter=title.search%3ARecurrent+radial+basis+function+network-based+fuzzy+neural+network+control+for+permanent-magnet+linear+synchronous+motor+servo+drive&per-page=5 \"HTTP/1.1 200 OK\"\n",
      "2025-08-14 20:52:23,945 [INFO] openai._base_client: Retrying request to /chat/completions in 0.439857 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"event\": \"http_request\", \"url\": \"https://api.openalex.org/works?filter=title.search%3ARecurrent+radial+basis+function+network-based+fuzzy+neural+network+control+for+permanent-magnet+linear+synchronous+motor+servo+drive&per-page=5\", \"code\": 200}\n",
      "{\"event\": \"route_after_verify\", \"decision\": \"FormatReference\", \"ver_score\": 11}\n",
      "\n",
      "=== Result ===\n",
      "Resolved Type: journal article\n",
      "Formatted: F.-J. Lin, P.-H. Shen, S.-L. Yang, and P.-H. Chou, \"Recurrent radial basis function network-based fuzzy neural network control for permanent-magnet linear synchronous motor servo drive\", *IEEE Tran. Magn.*, vol. 42, no. 11, pp. 3694–3705, Nov 2006, https://doi.org/10.1109/tmag.2006.880995.\n",
      "Verification OK: True\n",
      "Report:\n",
      " Corrections (field: old → new):\n",
      "- title: 'Recurrent radial basis function network-based fuzzy neural network control for permanent-magnet linear synchronous motor servo drive,' → 'Recurrent Radial Basis Function Network-Based Fuzzy Neural Network Control for Permanent-Magnet Linear Synchronous Motor Servo Drive'\n",
      "- authors: '['F.-J. Lin', 'P.-H. Shen', 'S.-L. Yang', 'and P. H. Chou']' → '['F.-J. Lin', 'P.-H. Shen', 'S.-L. Yang', 'P.-H. Chou']'\n",
      "- journal_name: 'vol. 42' → 'IEEE Transactions on Magnetics'\n",
      "- journal_abbrev: 'None' → 'IEEE Trans. Magn.'\n",
      "- issue: 'us' → '11'\n",
      "- pages: 'None' → '3694-3705'\n",
      "- doi: 'None' → '10.1109/tmag.2006.880995'\n",
      "- month: 'None' → '11'\n",
      "- title_sentence_case: 'Recurrent Radial Basis Function Network-Based Fuzzy Neural Network Control for Permanent-Magnet Linear Synchronous Motor Servo Drive' → 'Recurrent radial basis function network-based fuzzy neural network control for permanent-magnet linear synchronous motor servo drive'\n",
      "- journal_abbrev: 'IEEE Trans. Magn.' → 'IEEE Tran. Magn.'\n",
      "All verification checks passed after corrections.\n",
      "CSL-JSON: {\n",
      "  \"type\": \"article-journal\",\n",
      "  \"title\": \"Recurrent radial basis function network-based fuzzy neural network control for permanent-magnet linear synchronous motor servo drive\",\n",
      "  \"author\": [\n",
      "    {\n",
      "      \"family\": \"Lin\",\n",
      "      \"given\": \"F.-J.\"\n",
      "    },\n",
      "    {\n",
      "      \"family\": \"Shen\",\n",
      "      \"given\": \"P.-H.\"\n",
      "    },\n",
      "    {\n",
      "      \"family\": \"Yang\",\n",
      "      \"given\": \"S.-L.\"\n",
      "    },\n",
      "    {\n",
      "      \"family\": \"Chou\",\n",
      "      \"given\": \"P.-H.\"\n",
      "    }\n",
      "  ],\n",
      "  \"container-title\": \"IEEE Transactions on Magnetics\",\n",
      "  \"container-title-short\": \"IEEE Tran. Magn.\",\n",
      "  \"volume\": \"42\",\n",
      "  \"issue\": \"11\",\n",
      "  \"page\": \"3694-3705\",\n",
      "  \"DOI\": \"10.1109/tmag.2006.880995\",\n",
      "  \"URL\": \"https://doi.org/10.1109/tmag.2006.880995\",\n",
      "  \"issued\": {\n",
      "    \"date-parts\": [\n",
      "      [\n",
      "        2006,\n",
      "        11\n",
      "      ]\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "BibTeX:\n",
      " @article{Lin2006,\n",
      "  author = {F.-J. Lin and P.-H. Shen and S.-L. Yang and P.-H. Chou},\n",
      "  title = {Recurrent radial basis function network-based fuzzy neural network control for permanent-magnet linear synchronous motor servo drive},\n",
      "  journal = {IEEE Transactions on Magnetics},\n",
      "  volume = {42},\n",
      "  number = {11},\n",
      "  pages = {3694-3705},\n",
      "  year = {2006},\n",
      "  url = {https://doi.org/10.1109/tmag.2006.880995}\n",
      "}\n",
      "{\"event\": \"llm_provider_selected\", \"provider\": \"azure\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-14 20:52:24,391 [INFO] openai._base_client: Retrying request to /chat/completions in 0.995746 seconds\n",
      "2025-08-14 20:52:25,394 [WARNING] ieee-ref-langgraph: LLM json() failed: Connection error.\n",
      "2025-08-14 20:52:25,399 [INFO] openai._base_client: Retrying request to /chat/completions in 0.380103 seconds\n",
      "2025-08-14 20:52:25,786 [INFO] openai._base_client: Retrying request to /chat/completions in 0.754034 seconds\n",
      "2025-08-14 20:52:26,547 [WARNING] ieee-ref-langgraph: LLM json() failed: Connection error.\n",
      "/var/folders/9g/tnv_kfvn5kx8s1x4rjvf1gmr0000gn/T/ipykernel_95299/3553259949.py:909: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  parsed = ExtractedModel(**parsed).dict(exclude_none=True)\n",
      "2025-08-14 20:52:26,685 [INFO] httpx: HTTP Request: GET https://api.crossref.org/works/10.1109/72.279193. \"HTTP/1.1 404 Not Found\"\n",
      "2025-08-14 20:52:26,749 [INFO] httpx: HTTP Request: GET https://export.arxiv.org/api/query?search_query=ti%3A%22Memory+neuron+networks+for+identification+and+control+of+dynamical+systems%2C%22&start=0&max_results=1 \"HTTP/1.1 200 OK\"\n",
      "2025-08-14 20:52:26,762 [INFO] httpx: HTTP Request: GET https://api.openalex.org/works?filter=title.search%3AMemory+neuron+networks+for+identification+and+control+of+dynamical+systems%2C&per-page=5 \"HTTP/1.1 403 FORBIDDEN\"\n",
      "2025-08-14 20:52:26,777 [INFO] httpx: HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/DOI:10.1109/72.279193.?fields=title%2Cvenue%2Cyear%2Cauthors%2CexternalIds%2CpublicationVenue%2CpublicationTypes \"HTTP/1.1 404 Not Found\"\n",
      "2025-08-14 20:52:26,802 [INFO] httpx: HTTP Request: GET https://api.openalex.org/works?filter=doi%3A10.1109%2F72.279193. \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"event\": \"http_request\", \"url\": \"https://api.crossref.org/works/10.1109/72.279193.\", \"code\": 404}\n",
      "{\"event\": \"http_request\", \"url\": \"https://export.arxiv.org/api/query?search_query=ti%3A%22Memory+neuron+networks+for+identification+and+control+of+dynamical+systems%2C%22&start=0&max_results=1\", \"code\": 200}\n",
      "{\"event\": \"http_request\", \"url\": \"https://api.openalex.org/works?filter=title.search%3AMemory+neuron+networks+for+identification+and+control+of+dynamical+systems%2C&per-page=5\", \"code\": 403}\n",
      "{\"event\": \"http_request\", \"url\": \"https://api.semanticscholar.org/graph/v1/paper/DOI:10.1109/72.279193.?fields=title%2Cvenue%2Cyear%2Cauthors%2CexternalIds%2CpublicationVenue%2CpublicationTypes\", \"code\": 404}\n",
      "{\"event\": \"http_request\", \"url\": \"https://api.openalex.org/works?filter=doi%3A10.1109%2F72.279193.\", \"code\": 200}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-14 20:52:26,920 [INFO] httpx: HTTP Request: GET https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term=Memory+neuron+networks+for+identification+and+control+of+dynamical+systems%2C&retmode=json&retmax=1&tool=ieee-ref-agent&email=you%40example.com \"HTTP/1.1 200 OK\"\n",
      "2025-08-14 20:52:26,926 [INFO] httpx: HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=Memory+neuron+networks+for+identification+and+control+of+dynamical+systems%2C&limit=5&fields=title%2Cvenue%2Cyear%2Cauthors%2CexternalIds%2CpublicationVenue%2CpublicationTypes \"HTTP/1.1 200 OK\"\n",
      "2025-08-14 20:52:27,018 [INFO] httpx: HTTP Request: GET https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi?db=pubmed&id=39758818&retmode=json&tool=ieee-ref-agent&email=you%40example.com \"HTTP/1.1 200 OK\"\n",
      "2025-08-14 20:52:27,022 [INFO] httpx: HTTP Request: GET https://api.crossref.org/works/10.1109/72.279193. \"HTTP/1.1 404 Not Found\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"event\": \"http_request\", \"url\": \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term=Memory+neuron+networks+for+identification+and+control+of+dynamical+systems%2C&retmode=json&retmax=1&tool=ieee-ref-agent&email=you%40example.com\", \"code\": 200}\n",
      "{\"event\": \"http_request\", \"url\": \"https://api.semanticscholar.org/graph/v1/paper/search?query=Memory+neuron+networks+for+identification+and+control+of+dynamical+systems%2C&limit=5&fields=title%2Cvenue%2Cyear%2Cauthors%2CexternalIds%2CpublicationVenue%2CpublicationTypes\", \"code\": 200}\n",
      "{\"event\": \"http_request\", \"url\": \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi?db=pubmed&id=39758818&retmode=json&tool=ieee-ref-agent&email=you%40example.com\", \"code\": 200}\n",
      "{\"event\": \"http_request\", \"url\": \"https://api.crossref.org/works/10.1109/72.279193.\", \"code\": 404}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-14 20:52:27,170 [INFO] httpx: HTTP Request: GET https://api.openalex.org/works?filter=title.search%3AMemory+neuron+networks+for+identification+and+control+of+dynamical+systems%2C&per-page=5 \"HTTP/1.1 403 FORBIDDEN\"\n",
      "2025-08-14 20:52:27,175 [INFO] httpx: HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/DOI:10.1109/72.279193.?fields=title%2Cvenue%2Cyear%2Cauthors%2CexternalIds%2CpublicationVenue%2CpublicationTypes \"HTTP/1.1 404 Not Found\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"event\": \"http_request\", \"url\": \"https://api.openalex.org/works?filter=title.search%3AMemory+neuron+networks+for+identification+and+control+of+dynamical+systems%2C&per-page=5\", \"code\": 403}\n",
      "{\"event\": \"http_request\", \"url\": \"https://api.semanticscholar.org/graph/v1/paper/DOI:10.1109/72.279193.?fields=title%2Cvenue%2Cyear%2Cauthors%2CexternalIds%2CpublicationVenue%2CpublicationTypes\", \"code\": 404}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-14 20:52:27,691 [INFO] httpx: HTTP Request: GET https://api.crossref.org/works/10.1109/72.279193. \"HTTP/1.1 404 Not Found\"\n",
      "2025-08-14 20:52:27,820 [INFO] httpx: HTTP Request: GET https://api.crossref.org/works?query.title=Memory+neuron+networks+for+identification+and+control+of+dynamical+systems%2C&rows=5&select=title%2Cauthor%2Ccontainer-title%2Cshort-container-title%2Cissued%2CDOI%2Cpage%2Cvolume%2Cissue%2Cpublished-print%2Cpublished-online%2Ctype \"HTTP/1.1 200 OK\"\n",
      "2025-08-14 20:52:27,867 [INFO] httpx: HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/DOI:10.1109/72.279193.?fields=title%2Cvenue%2Cyear%2Cauthors%2CexternalIds%2CpublicationVenue%2CpublicationTypes \"HTTP/1.1 404 Not Found\"\n",
      "2025-08-14 20:52:27,878 [INFO] httpx: HTTP Request: GET https://api.openalex.org/works?filter=title.search%3AMemory+neuron+networks+for+identification+and+control+of+dynamical+systems%2C&per-page=5 \"HTTP/1.1 403 FORBIDDEN\"\n",
      "2025-08-14 20:52:27,886 [INFO] openai._base_client: Retrying request to /chat/completions in 0.453157 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"event\": \"http_request\", \"url\": \"https://api.crossref.org/works/10.1109/72.279193.\", \"code\": 404}\n",
      "{\"event\": \"http_request\", \"url\": \"https://api.crossref.org/works?query.title=Memory+neuron+networks+for+identification+and+control+of+dynamical+systems%2C&rows=5&select=title%2Cauthor%2Ccontainer-title%2Cshort-container-title%2Cissued%2CDOI%2Cpage%2Cvolume%2Cissue%2Cpublished-print%2Cpublished-online%2Ctype\", \"code\": 200}\n",
      "{\"event\": \"http_request\", \"url\": \"https://api.semanticscholar.org/graph/v1/paper/DOI:10.1109/72.279193.?fields=title%2Cvenue%2Cyear%2Cauthors%2CexternalIds%2CpublicationVenue%2CpublicationTypes\", \"code\": 404}\n",
      "{\"event\": \"http_request\", \"url\": \"https://api.openalex.org/works?filter=title.search%3AMemory+neuron+networks+for+identification+and+control+of+dynamical+systems%2C&per-page=5\", \"code\": 403}\n",
      "{\"event\": \"route_after_verify\", \"decision\": \"ApplyCorrections\", \"ver_score\": 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-14 20:52:28,347 [INFO] openai._base_client: Retrying request to /chat/completions in 0.969429 seconds\n",
      "2025-08-14 20:52:29,323 [WARNING] ieee-ref-langgraph: LLM json() failed: Connection error.\n",
      "2025-08-14 20:52:29,373 [INFO] httpx: HTTP Request: GET https://api.crossref.org/works/10.1109/72.279193 \"HTTP/1.1 200 OK\"\n",
      "2025-08-14 20:52:29,431 [INFO] httpx: HTTP Request: GET https://api.openalex.org/works?filter=doi%3A10.1109%2F72.279193 \"HTTP/1.1 200 OK\"\n",
      "2025-08-14 20:52:29,434 [INFO] httpx: HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/DOI:10.1109/72.279193?fields=title%2Cvenue%2Cyear%2Cauthors%2CexternalIds%2CpublicationVenue%2CpublicationTypes \"HTTP/1.1 200 OK\"\n",
      "2025-08-14 20:52:29,517 [INFO] httpx: HTTP Request: GET https://export.arxiv.org/api/query?search_query=ti%3A%22Memory+neuron+networks+for+identification+and+control+of+dynamical+systems%22&start=0&max_results=1 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"event\": \"http_request\", \"url\": \"https://api.crossref.org/works/10.1109/72.279193\", \"code\": 200}\n",
      "{\"event\": \"http_request\", \"url\": \"https://api.openalex.org/works?filter=doi%3A10.1109%2F72.279193\", \"code\": 200}\n",
      "{\"event\": \"http_request\", \"url\": \"https://api.semanticscholar.org/graph/v1/paper/DOI:10.1109/72.279193?fields=title%2Cvenue%2Cyear%2Cauthors%2CexternalIds%2CpublicationVenue%2CpublicationTypes\", \"code\": 200}\n",
      "{\"event\": \"http_request\", \"url\": \"https://export.arxiv.org/api/query?search_query=ti%3A%22Memory+neuron+networks+for+identification+and+control+of+dynamical+systems%22&start=0&max_results=1\", \"code\": 200}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-14 20:52:29,589 [INFO] httpx: HTTP Request: GET https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term=Memory+neuron+networks+for+identification+and+control+of+dynamical+systems&retmode=json&retmax=1&tool=ieee-ref-agent&email=you%40example.com \"HTTP/1.1 200 OK\"\n",
      "2025-08-14 20:52:29,602 [INFO] httpx: HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=Memory+neuron+networks+for+identification+and+control+of+dynamical+systems&limit=5&fields=title%2Cvenue%2Cyear%2Cauthors%2CexternalIds%2CpublicationVenue%2CpublicationTypes \"HTTP/1.1 200 OK\"\n",
      "2025-08-14 20:52:29,684 [INFO] httpx: HTTP Request: GET https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi?db=pubmed&id=39758818&retmode=json&tool=ieee-ref-agent&email=you%40example.com \"HTTP/1.1 200 OK\"\n",
      "2025-08-14 20:52:29,686 [INFO] httpx: HTTP Request: GET https://api.crossref.org/works?query.title=Memory+neuron+networks+for+identification+and+control+of+dynamical+systems&rows=5&select=title%2Cauthor%2Ccontainer-title%2Cshort-container-title%2Cissued%2CDOI%2Cpage%2Cvolume%2Cissue%2Cpublished-print%2Cpublished-online%2Ctype \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"event\": \"http_request\", \"url\": \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term=Memory+neuron+networks+for+identification+and+control+of+dynamical+systems&retmode=json&retmax=1&tool=ieee-ref-agent&email=you%40example.com\", \"code\": 200}\n",
      "{\"event\": \"http_request\", \"url\": \"https://api.semanticscholar.org/graph/v1/paper/search?query=Memory+neuron+networks+for+identification+and+control+of+dynamical+systems&limit=5&fields=title%2Cvenue%2Cyear%2Cauthors%2CexternalIds%2CpublicationVenue%2CpublicationTypes\", \"code\": 200}\n",
      "{\"event\": \"http_request\", \"url\": \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi?db=pubmed&id=39758818&retmode=json&tool=ieee-ref-agent&email=you%40example.com\", \"code\": 200}\n",
      "{\"event\": \"http_request\", \"url\": \"https://api.crossref.org/works?query.title=Memory+neuron+networks+for+identification+and+control+of+dynamical+systems&rows=5&select=title%2Cauthor%2Ccontainer-title%2Cshort-container-title%2Cissued%2CDOI%2Cpage%2Cvolume%2Cissue%2Cpublished-print%2Cpublished-online%2Ctype\", \"code\": 200}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-14 20:52:29,892 [INFO] httpx: HTTP Request: GET https://api.openalex.org/works?filter=title.search%3AMemory+neuron+networks+for+identification+and+control+of+dynamical+systems&per-page=5 \"HTTP/1.1 200 OK\"\n",
      "2025-08-14 20:52:29,942 [INFO] openai._base_client: Retrying request to /chat/completions in 0.474187 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"event\": \"http_request\", \"url\": \"https://api.openalex.org/works?filter=title.search%3AMemory+neuron+networks+for+identification+and+control+of+dynamical+systems&per-page=5\", \"code\": 200}\n",
      "{\"event\": \"route_after_verify\", \"decision\": \"FormatReference\", \"ver_score\": 11}\n",
      "\n",
      "=== Result ===\n",
      "Resolved Type: journal article\n",
      "Formatted: P. Sastry, G. Santharam, and K. Unnikrishnan, \"Memory neuron networks for identification and control of dynamical systems\", *IEEE Tran. Neur. Netw.*, vol. 5, no. 2, pp. 306–319, Mar 1994, https://doi.org/10.1109/72.279193.\n",
      "Verification OK: True\n",
      "Report:\n",
      " Corrections (field: old → new):\n",
      "- title: 'Memory neuron networks for identification and control of dynamical systems,' → 'Memory neuron networks for identification and control of dynamical systems'\n",
      "- authors: '['P. S. Sastry', 'G. Santharam', 'and K. P. Unnikrishnan']' → '['P.S. Sastry', 'G. Santharam', 'K.P. Unnikrishnan']'\n",
      "- journal_name: 'vol. 5' → 'IEEE Transactions on Neural Networks'\n",
      "- journal_abbrev: 'None' → 'IEEE Trans. Neural Netw.'\n",
      "- doi: '10.1109/72.279193.' → '10.1109/72.279193'\n",
      "- month: 'None' → '3'\n",
      "- journal_abbrev: 'IEEE Trans. Neural Netw.' → 'IEEE Tran. Neur. Netw.'\n",
      "All verification checks passed after corrections.\n",
      "CSL-JSON: {\n",
      "  \"type\": \"article-journal\",\n",
      "  \"title\": \"Memory neuron networks for identification and control of dynamical systems\",\n",
      "  \"author\": [\n",
      "    {\n",
      "      \"family\": \"Sastry\",\n",
      "      \"given\": \"P.S.\"\n",
      "    },\n",
      "    {\n",
      "      \"family\": \"Santharam\",\n",
      "      \"given\": \"G.\"\n",
      "    },\n",
      "    {\n",
      "      \"family\": \"Unnikrishnan\",\n",
      "      \"given\": \"K.P.\"\n",
      "    }\n",
      "  ],\n",
      "  \"container-title\": \"IEEE Transactions on Neural Networks\",\n",
      "  \"container-title-short\": \"IEEE Tran. Neur. Netw.\",\n",
      "  \"volume\": \"5\",\n",
      "  \"issue\": \"2\",\n",
      "  \"page\": \"306-319\",\n",
      "  \"DOI\": \"10.1109/72.279193\",\n",
      "  \"URL\": \"https://doi.org/10.1109/72.279193\",\n",
      "  \"issued\": {\n",
      "    \"date-parts\": [\n",
      "      [\n",
      "        1994,\n",
      "        3\n",
      "      ]\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "BibTeX:\n",
      " @article{Sastry1994,\n",
      "  author = {P.S. Sastry and G. Santharam and K.P. Unnikrishnan},\n",
      "  title = {Memory neuron networks for identification and control of dynamical systems},\n",
      "  journal = {IEEE Transactions on Neural Networks},\n",
      "  volume = {5},\n",
      "  number = {2},\n",
      "  pages = {306-319},\n",
      "  year = {1994},\n",
      "  url = {https://doi.org/10.1109/72.279193}\n",
      "}\n",
      "{\"event\": \"llm_provider_selected\", \"provider\": \"azure\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-14 20:52:30,420 [INFO] openai._base_client: Retrying request to /chat/completions in 0.776735 seconds\n",
      "2025-08-14 20:52:31,203 [WARNING] ieee-ref-langgraph: LLM json() failed: Connection error.\n",
      "2025-08-14 20:52:31,207 [INFO] openai._base_client: Retrying request to /chat/completions in 0.476658 seconds\n",
      "2025-08-14 20:52:31,686 [INFO] openai._base_client: Retrying request to /chat/completions in 0.977314 seconds\n",
      "2025-08-14 20:52:32,671 [WARNING] ieee-ref-langgraph: LLM json() failed: Connection error.\n",
      "/var/folders/9g/tnv_kfvn5kx8s1x4rjvf1gmr0000gn/T/ipykernel_95299/3553259949.py:909: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  parsed = ExtractedModel(**parsed).dict(exclude_none=True)\n",
      "2025-08-14 20:52:32,805 [INFO] httpx: HTTP Request: GET https://api.crossref.org/works/10.1007/s11356-021-15225-2. \"HTTP/1.1 404 Not Found\"\n",
      "2025-08-14 20:52:32,882 [INFO] httpx: HTTP Request: GET https://api.openalex.org/works?filter=title.search%3AThe+non-linear+relationship+between+carbon+dioxide+emissions%2C+financial+development+and+energy+consumption+in+developing+European+and+Central+Asian+economies%2C&per-page=5 \"HTTP/1.1 403 FORBIDDEN\"\n",
      "2025-08-14 20:52:32,884 [INFO] httpx: HTTP Request: GET https://export.arxiv.org/api/query?search_query=ti%3A%22The+non-linear+relationship+between+carbon+dioxide+emissions%2C+financial+development+and+energy+consumption+in+developing+European+and+Central+Asian+economies%2C%22&start=0&max_results=1 \"HTTP/1.1 200 OK\"\n",
      "2025-08-14 20:52:32,904 [INFO] httpx: HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/DOI:10.1007/s11356-021-15225-2.?fields=title%2Cvenue%2Cyear%2Cauthors%2CexternalIds%2CpublicationVenue%2CpublicationTypes \"HTTP/1.1 404 Not Found\"\n",
      "2025-08-14 20:52:32,946 [INFO] httpx: HTTP Request: GET https://api.openalex.org/works?filter=doi%3A10.1007%2Fs11356-021-15225-2. \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"event\": \"http_request\", \"url\": \"https://api.crossref.org/works/10.1007/s11356-021-15225-2.\", \"code\": 404}\n",
      "{\"event\": \"http_request\", \"url\": \"https://api.openalex.org/works?filter=title.search%3AThe+non-linear+relationship+between+carbon+dioxide+emissions%2C+financial+development+and+energy+consumption+in+developing+European+and+Central+Asian+economies%2C&per-page=5\", \"code\": 403}\n",
      "{\"event\": \"http_request\", \"url\": \"https://export.arxiv.org/api/query?search_query=ti%3A%22The+non-linear+relationship+between+carbon+dioxide+emissions%2C+financial+development+and+energy+consumption+in+developing+European+and+Central+Asian+economies%2C%22&start=0&max_results=1\", \"code\": 200}\n",
      "{\"event\": \"http_request\", \"url\": \"https://api.semanticscholar.org/graph/v1/paper/DOI:10.1007/s11356-021-15225-2.?fields=title%2Cvenue%2Cyear%2Cauthors%2CexternalIds%2CpublicationVenue%2CpublicationTypes\", \"code\": 404}\n",
      "{\"event\": \"http_request\", \"url\": \"https://api.openalex.org/works?filter=doi%3A10.1007%2Fs11356-021-15225-2.\", \"code\": 200}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-14 20:52:33,117 [INFO] httpx: HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=The+non-linear+relationship+between+carbon+dioxide+emissions%2C+financial+development+and+energy+consumption+in+developing+European+and+Central+Asian+economies%2C&limit=5&fields=title%2Cvenue%2Cyear%2Cauthors%2CexternalIds%2CpublicationVenue%2CpublicationTypes \"HTTP/1.1 200 OK\"\n",
      "2025-08-14 20:52:33,144 [INFO] httpx: HTTP Request: GET https://api.crossref.org/works/10.1007/s11356-021-15225-2. \"HTTP/1.1 404 Not Found\"\n",
      "2025-08-14 20:52:33,287 [INFO] httpx: HTTP Request: GET https://api.openalex.org/works?filter=title.search%3AThe+non-linear+relationship+between+carbon+dioxide+emissions%2C+financial+development+and+energy+consumption+in+developing+European+and+Central+Asian+economies%2C&per-page=5 \"HTTP/1.1 403 FORBIDDEN\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"event\": \"http_request\", \"url\": \"https://api.semanticscholar.org/graph/v1/paper/search?query=The+non-linear+relationship+between+carbon+dioxide+emissions%2C+financial+development+and+energy+consumption+in+developing+European+and+Central+Asian+economies%2C&limit=5&fields=title%2Cvenue%2Cyear%2Cauthors%2CexternalIds%2CpublicationVenue%2CpublicationTypes\", \"code\": 200}\n",
      "{\"event\": \"http_request\", \"url\": \"https://api.crossref.org/works/10.1007/s11356-021-15225-2.\", \"code\": 404}\n",
      "{\"event\": \"http_request\", \"url\": \"https://api.openalex.org/works?filter=title.search%3AThe+non-linear+relationship+between+carbon+dioxide+emissions%2C+financial+development+and+energy+consumption+in+developing+European+and+Central+Asian+economies%2C&per-page=5\", \"code\": 403}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-14 20:52:33,320 [INFO] httpx: HTTP Request: GET https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term=The+non-linear+relationship+between+carbon+dioxide+emissions%2C+financial+development+and+energy+consumption+in+developing+European+and+Central+Asian+economies%2C&retmode=json&retmax=1&tool=ieee-ref-agent&email=you%40example.com \"HTTP/1.1 200 OK\"\n",
      "2025-08-14 20:52:33,340 [INFO] httpx: HTTP Request: GET https://api.crossref.org/works?query.title=The+non-linear+relationship+between+carbon+dioxide+emissions%2C+financial+development+and+energy+consumption+in+developing+European+and+Central+Asian+economies%2C&rows=5&select=title%2Cauthor%2Ccontainer-title%2Cshort-container-title%2Cissued%2CDOI%2Cpage%2Cvolume%2Cissue%2Cpublished-print%2Cpublished-online%2Ctype \"HTTP/1.1 200 OK\"\n",
      "2025-08-14 20:52:33,381 [INFO] httpx: HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/DOI:10.1007/s11356-021-15225-2.?fields=title%2Cvenue%2Cyear%2Cauthors%2CexternalIds%2CpublicationVenue%2CpublicationTypes \"HTTP/1.1 404 Not Found\"\n",
      "2025-08-14 20:52:33,407 [INFO] httpx: HTTP Request: GET https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi?db=pubmed&id=38468014&retmode=json&tool=ieee-ref-agent&email=you%40example.com \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"event\": \"http_request\", \"url\": \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term=The+non-linear+relationship+between+carbon+dioxide+emissions%2C+financial+development+and+energy+consumption+in+developing+European+and+Central+Asian+economies%2C&retmode=json&retmax=1&tool=ieee-ref-agent&email=you%40example.com\", \"code\": 200}\n",
      "{\"event\": \"http_request\", \"url\": \"https://api.crossref.org/works?query.title=The+non-linear+relationship+between+carbon+dioxide+emissions%2C+financial+development+and+energy+consumption+in+developing+European+and+Central+Asian+economies%2C&rows=5&select=title%2Cauthor%2Ccontainer-title%2Cshort-container-title%2Cissued%2CDOI%2Cpage%2Cvolume%2Cissue%2Cpublished-print%2Cpublished-online%2Ctype\", \"code\": 200}\n",
      "{\"event\": \"http_request\", \"url\": \"https://api.semanticscholar.org/graph/v1/paper/DOI:10.1007/s11356-021-15225-2.?fields=title%2Cvenue%2Cyear%2Cauthors%2CexternalIds%2CpublicationVenue%2CpublicationTypes\", \"code\": 404}\n",
      "{\"event\": \"http_request\", \"url\": \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi?db=pubmed&id=38468014&retmode=json&tool=ieee-ref-agent&email=you%40example.com\", \"code\": 200}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-14 20:52:33,796 [INFO] httpx: HTTP Request: GET https://api.crossref.org/works/10.1007/s11356-021-15225-2. \"HTTP/1.1 404 Not Found\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"event\": \"http_request\", \"url\": \"https://api.crossref.org/works/10.1007/s11356-021-15225-2.\", \"code\": 404}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-14 20:52:34,020 [INFO] httpx: HTTP Request: GET https://api.openalex.org/works?filter=title.search%3AThe+non-linear+relationship+between+carbon+dioxide+emissions%2C+financial+development+and+energy+consumption+in+developing+European+and+Central+Asian+economies%2C&per-page=5 \"HTTP/1.1 403 FORBIDDEN\"\n",
      "2025-08-14 20:52:34,165 [INFO] httpx: HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/DOI:10.1007/s11356-021-15225-2.?fields=title%2Cvenue%2Cyear%2Cauthors%2CexternalIds%2CpublicationVenue%2CpublicationTypes \"HTTP/1.1 404 Not Found\"\n",
      "2025-08-14 20:52:34,172 [INFO] openai._base_client: Retrying request to /chat/completions in 0.424884 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"event\": \"http_request\", \"url\": \"https://api.openalex.org/works?filter=title.search%3AThe+non-linear+relationship+between+carbon+dioxide+emissions%2C+financial+development+and+energy+consumption+in+developing+European+and+Central+Asian+economies%2C&per-page=5\", \"code\": 403}\n",
      "{\"event\": \"http_request\", \"url\": \"https://api.semanticscholar.org/graph/v1/paper/DOI:10.1007/s11356-021-15225-2.?fields=title%2Cvenue%2Cyear%2Cauthors%2CexternalIds%2CpublicationVenue%2CpublicationTypes\", \"code\": 404}\n",
      "{\"event\": \"reject_candidate\", \"reason\": \"not_trustworthy\", \"score\": 0.7872727272727272, \"title\": \"RETRACTED ARTICLE: The non-linear relationship between carbon dioxide emissions, financial development and energy consumption in developing European and Central Asian economies\"}\n",
      "{\"event\": \"route_after_verify\", \"decision\": \"ApplyCorrections\", \"ver_score\": 10}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-14 20:52:34,603 [INFO] openai._base_client: Retrying request to /chat/completions in 0.988522 seconds\n",
      "2025-08-14 20:52:35,596 [WARNING] ieee-ref-langgraph: LLM json() failed: Connection error.\n",
      "2025-08-14 20:52:35,640 [INFO] httpx: HTTP Request: GET https://api.crossref.org/works/10.1007/s11356-021-15225-2. \"HTTP/1.1 404 Not Found\"\n",
      "2025-08-14 20:52:35,715 [INFO] httpx: HTTP Request: GET https://api.openalex.org/works?filter=title.search%3AThe+non-linear+relationship+between+carbon+dioxide+emissions%2C+financial+development+and+energy+consumption+in+developing+european+and+central+asian+economies%2C&per-page=5 \"HTTP/1.1 403 FORBIDDEN\"\n",
      "2025-08-14 20:52:35,716 [INFO] httpx: HTTP Request: GET https://api.openalex.org/works?filter=doi%3A10.1007%2Fs11356-021-15225-2. \"HTTP/1.1 200 OK\"\n",
      "2025-08-14 20:52:35,717 [INFO] httpx: HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/DOI:10.1007/s11356-021-15225-2.?fields=title%2Cvenue%2Cyear%2Cauthors%2CexternalIds%2CpublicationVenue%2CpublicationTypes \"HTTP/1.1 404 Not Found\"\n",
      "2025-08-14 20:52:35,764 [INFO] httpx: HTTP Request: GET https://export.arxiv.org/api/query?search_query=ti%3A%22The+non-linear+relationship+between+carbon+dioxide+emissions%2C+financial+development+and+energy+consumption+in+developing+european+and+central+asian+economies%2C%22&start=0&max_results=1 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"event\": \"http_request\", \"url\": \"https://api.crossref.org/works/10.1007/s11356-021-15225-2.\", \"code\": 404}\n",
      "{\"event\": \"http_request\", \"url\": \"https://api.openalex.org/works?filter=title.search%3AThe+non-linear+relationship+between+carbon+dioxide+emissions%2C+financial+development+and+energy+consumption+in+developing+european+and+central+asian+economies%2C&per-page=5\", \"code\": 403}\n",
      "{\"event\": \"http_request\", \"url\": \"https://api.openalex.org/works?filter=doi%3A10.1007%2Fs11356-021-15225-2.\", \"code\": 200}\n",
      "{\"event\": \"http_request\", \"url\": \"https://api.semanticscholar.org/graph/v1/paper/DOI:10.1007/s11356-021-15225-2.?fields=title%2Cvenue%2Cyear%2Cauthors%2CexternalIds%2CpublicationVenue%2CpublicationTypes\", \"code\": 404}\n",
      "{\"event\": \"http_request\", \"url\": \"https://export.arxiv.org/api/query?search_query=ti%3A%22The+non-linear+relationship+between+carbon+dioxide+emissions%2C+financial+development+and+energy+consumption+in+developing+european+and+central+asian+economies%2C%22&start=0&max_results=1\", \"code\": 200}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-14 20:52:35,924 [INFO] httpx: HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=The+non-linear+relationship+between+carbon+dioxide+emissions%2C+financial+development+and+energy+consumption+in+developing+european+and+central+asian+economies%2C&limit=5&fields=title%2Cvenue%2Cyear%2Cauthors%2CexternalIds%2CpublicationVenue%2CpublicationTypes \"HTTP/1.1 200 OK\"\n",
      "2025-08-14 20:52:35,977 [INFO] httpx: HTTP Request: GET https://api.crossref.org/works/10.1007/s11356-021-15225-2. \"HTTP/1.1 404 Not Found\"\n",
      "2025-08-14 20:52:36,118 [INFO] httpx: HTTP Request: GET https://api.openalex.org/works?filter=title.search%3AThe+non-linear+relationship+between+carbon+dioxide+emissions%2C+financial+development+and+energy+consumption+in+developing+european+and+central+asian+economies%2C&per-page=5 \"HTTP/1.1 403 FORBIDDEN\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"event\": \"http_request\", \"url\": \"https://api.semanticscholar.org/graph/v1/paper/search?query=The+non-linear+relationship+between+carbon+dioxide+emissions%2C+financial+development+and+energy+consumption+in+developing+european+and+central+asian+economies%2C&limit=5&fields=title%2Cvenue%2Cyear%2Cauthors%2CexternalIds%2CpublicationVenue%2CpublicationTypes\", \"code\": 200}\n",
      "{\"event\": \"http_request\", \"url\": \"https://api.crossref.org/works/10.1007/s11356-021-15225-2.\", \"code\": 404}\n",
      "{\"event\": \"http_request\", \"url\": \"https://api.openalex.org/works?filter=title.search%3AThe+non-linear+relationship+between+carbon+dioxide+emissions%2C+financial+development+and+energy+consumption+in+developing+european+and+central+asian+economies%2C&per-page=5\", \"code\": 403}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-14 20:52:36,163 [INFO] httpx: HTTP Request: GET https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term=The+non-linear+relationship+between+carbon+dioxide+emissions%2C+financial+development+and+energy+consumption+in+developing+european+and+central+asian+economies%2C&retmode=json&retmax=1&tool=ieee-ref-agent&email=you%40example.com \"HTTP/1.1 200 OK\"\n",
      "2025-08-14 20:52:36,195 [INFO] httpx: HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/DOI:10.1007/s11356-021-15225-2.?fields=title%2Cvenue%2Cyear%2Cauthors%2CexternalIds%2CpublicationVenue%2CpublicationTypes \"HTTP/1.1 404 Not Found\"\n",
      "2025-08-14 20:52:36,261 [INFO] httpx: HTTP Request: GET https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi?db=pubmed&id=38468014&retmode=json&tool=ieee-ref-agent&email=you%40example.com \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"event\": \"http_request\", \"url\": \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term=The+non-linear+relationship+between+carbon+dioxide+emissions%2C+financial+development+and+energy+consumption+in+developing+european+and+central+asian+economies%2C&retmode=json&retmax=1&tool=ieee-ref-agent&email=you%40example.com\", \"code\": 200}\n",
      "{\"event\": \"http_request\", \"url\": \"https://api.semanticscholar.org/graph/v1/paper/DOI:10.1007/s11356-021-15225-2.?fields=title%2Cvenue%2Cyear%2Cauthors%2CexternalIds%2CpublicationVenue%2CpublicationTypes\", \"code\": 404}\n",
      "{\"event\": \"http_request\", \"url\": \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi?db=pubmed&id=38468014&retmode=json&tool=ieee-ref-agent&email=you%40example.com\", \"code\": 200}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-14 20:52:36,638 [INFO] httpx: HTTP Request: GET https://api.crossref.org/works/10.1007/s11356-021-15225-2. \"HTTP/1.1 404 Not Found\"\n",
      "2025-08-14 20:52:36,824 [INFO] httpx: HTTP Request: GET https://api.openalex.org/works?filter=title.search%3AThe+non-linear+relationship+between+carbon+dioxide+emissions%2C+financial+development+and+energy+consumption+in+developing+european+and+central+asian+economies%2C&per-page=5 \"HTTP/1.1 403 FORBIDDEN\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"event\": \"http_request\", \"url\": \"https://api.crossref.org/works/10.1007/s11356-021-15225-2.\", \"code\": 404}\n",
      "{\"event\": \"http_request\", \"url\": \"https://api.openalex.org/works?filter=title.search%3AThe+non-linear+relationship+between+carbon+dioxide+emissions%2C+financial+development+and+energy+consumption+in+developing+european+and+central+asian+economies%2C&per-page=5\", \"code\": 403}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-14 20:52:36,976 [INFO] httpx: HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/DOI:10.1007/s11356-021-15225-2.?fields=title%2Cvenue%2Cyear%2Cauthors%2CexternalIds%2CpublicationVenue%2CpublicationTypes \"HTTP/1.1 404 Not Found\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"event\": \"http_request\", \"url\": \"https://api.semanticscholar.org/graph/v1/paper/DOI:10.1007/s11356-021-15225-2.?fields=title%2Cvenue%2Cyear%2Cauthors%2CexternalIds%2CpublicationVenue%2CpublicationTypes\", \"code\": 404}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-14 20:52:37,818 [INFO] httpx: HTTP Request: GET https://api.crossref.org/works?query.title=The+non-linear+relationship+between+carbon+dioxide+emissions%2C+financial+development+and+energy+consumption+in+developing+european+and+central+asian+economies%2C&rows=5&select=title%2Cauthor%2Ccontainer-title%2Cshort-container-title%2Cissued%2CDOI%2Cpage%2Cvolume%2Cissue%2Cpublished-print%2Cpublished-online%2Ctype \"HTTP/1.1 200 OK\"\n",
      "2025-08-14 20:52:37,870 [INFO] openai._base_client: Retrying request to /chat/completions in 0.426325 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"event\": \"http_request\", \"url\": \"https://api.crossref.org/works?query.title=The+non-linear+relationship+between+carbon+dioxide+emissions%2C+financial+development+and+energy+consumption+in+developing+european+and+central+asian+economies%2C&rows=5&select=title%2Cauthor%2Ccontainer-title%2Cshort-container-title%2Cissued%2CDOI%2Cpage%2Cvolume%2Cissue%2Cpublished-print%2Cpublished-online%2Ctype\", \"code\": 200}\n",
      "{\"event\": \"reject_candidate\", \"reason\": \"not_trustworthy\", \"score\": 0.7872727272727272, \"title\": \"RETRACTED ARTICLE: The non-linear relationship between carbon dioxide emissions, financial development and energy consumption in developing European and Central Asian economies\"}\n",
      "{\"event\": \"route_after_verify\", \"decision\": \"FormatReference\", \"ver_score\": 11}\n",
      "\n",
      "=== Result ===\n",
      "Resolved Type: journal article\n",
      "Formatted: K. C. Apaza, and J. M. López, \"The non-linear relationship between carbon dioxide emissions, financial development and energy consumption in developing european and central asian economies,\", *vol. 28*, vol. 28, no. n, pp. 63330–63345, 2021, https://doi.org/10.1007/s11356-021-15225-2..\n",
      "Verification OK: True\n",
      "Report:\n",
      " Corrections (field: old → new):\n",
      "- title_sentence_case: 'The non-linear relationship between carbon dioxide emissions, financial development and energy consumption in developing European and Central Asian economies,' → 'The non-linear relationship between carbon dioxide emissions, financial development and energy consumption in developing european and central asian economies,'\n",
      "All verification checks passed after corrections.\n",
      "CSL-JSON: {\n",
      "  \"type\": \"article-journal\",\n",
      "  \"title\": \"The non-linear relationship between carbon dioxide emissions, financial development and energy consumption in developing european and central asian economies,\",\n",
      "  \"author\": [\n",
      "    {\n",
      "      \"family\": \"Apaza\",\n",
      "      \"given\": \"K. C.\"\n",
      "    },\n",
      "    {\n",
      "      \"family\": \"López\",\n",
      "      \"given\": \"J. M.\"\n",
      "    }\n",
      "  ],\n",
      "  \"container-title\": \"vol. 28\",\n",
      "  \"volume\": \"28\",\n",
      "  \"issue\": \"n\",\n",
      "  \"page\": \"63330-63345\",\n",
      "  \"DOI\": \"10.1007/s11356-021-15225-2.\",\n",
      "  \"URL\": \"https://doi.org/10.1007/s11356-021-15225-2.\",\n",
      "  \"issued\": {\n",
      "    \"date-parts\": [\n",
      "      [\n",
      "        2021\n",
      "      ]\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "BibTeX:\n",
      " @article{Apaza2021,\n",
      "  author = {K. C. Apaza and J. M. López},\n",
      "  title = {The non-linear relationship between carbon dioxide emissions, financial development and energy consumption in developing european and central asian economies,},\n",
      "  journal = {vol. 28},\n",
      "  volume = {28},\n",
      "  number = {n},\n",
      "  pages = {63330-63345},\n",
      "  year = {2021},\n",
      "  url = {https://doi.org/10.1007/s11356-021-15225-2.}\n",
      "}\n",
      "{\"event\": \"llm_provider_selected\", \"provider\": \"azure\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-14 20:52:38,302 [INFO] openai._base_client: Retrying request to /chat/completions in 0.811748 seconds\n",
      "2025-08-14 20:52:39,121 [WARNING] ieee-ref-langgraph: LLM json() failed: Connection error.\n",
      "2025-08-14 20:52:39,125 [INFO] openai._base_client: Retrying request to /chat/completions in 0.445009 seconds\n",
      "2025-08-14 20:52:39,574 [INFO] openai._base_client: Retrying request to /chat/completions in 0.962757 seconds\n",
      "2025-08-14 20:52:40,545 [WARNING] ieee-ref-langgraph: LLM json() failed: Connection error.\n",
      "/var/folders/9g/tnv_kfvn5kx8s1x4rjvf1gmr0000gn/T/ipykernel_95299/3553259949.py:909: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  parsed = ExtractedModel(**parsed).dict(exclude_none=True)\n",
      "2025-08-14 20:52:40,761 [INFO] httpx: HTTP Request: GET https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term=Attention+Is+All+You+Need&retmode=json&retmax=1&tool=ieee-ref-agent&email=you%40example.com \"HTTP/1.1 200 OK\"\n",
      "2025-08-14 20:52:40,765 [INFO] httpx: HTTP Request: GET https://export.arxiv.org/api/query?search_query=ti%3A%22Attention+Is+All+You+Need%22&start=0&max_results=1 \"HTTP/1.1 200 OK\"\n",
      "2025-08-14 20:52:40,859 [INFO] httpx: HTTP Request: GET https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi?db=pubmed&id=40725289&retmode=json&tool=ieee-ref-agent&email=you%40example.com \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"event\": \"http_request\", \"url\": \"https://export.arxiv.org/api/query?search_query=ti%3A%22Attention+Is+All+You+Need%22&start=0&max_results=1\", \"code\": 200}\n",
      "{\"event\": \"http_request\", \"url\": \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term=Attention+Is+All+You+Need&retmode=json&retmax=1&tool=ieee-ref-agent&email=you%40example.com\", \"code\": 200}\n",
      "{\"event\": \"http_request\", \"url\": \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi?db=pubmed&id=40725289&retmode=json&tool=ieee-ref-agent&email=you%40example.com\", \"code\": 200}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-14 20:52:41,297 [INFO] httpx: HTTP Request: GET https://api.crossref.org/works?query.title=Attention+Is+All+You+Need&rows=5&select=title%2Cauthor%2Ccontainer-title%2Cshort-container-title%2Cissued%2CDOI%2Cpage%2Cvolume%2Cissue%2Cpublished-print%2Cpublished-online%2Ctype \"HTTP/1.1 200 OK\"\n",
      "2025-08-14 20:52:41,301 [INFO] httpx: HTTP Request: GET https://api.openalex.org/works?filter=title.search%3AAttention+Is+All+You+Need&per-page=5 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"event\": \"http_request\", \"url\": \"https://api.crossref.org/works?query.title=Attention+Is+All+You+Need&rows=5&select=title%2Cauthor%2Ccontainer-title%2Cshort-container-title%2Cissued%2CDOI%2Cpage%2Cvolume%2Cissue%2Cpublished-print%2Cpublished-online%2Ctype\", \"code\": 200}\n",
      "{\"event\": \"http_request\", \"url\": \"https://api.openalex.org/works?filter=title.search%3AAttention+Is+All+You+Need&per-page=5\", \"code\": 200}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-14 20:52:41,513 [INFO] httpx: HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=Attention+Is+All+You+Need&limit=5&fields=title%2Cvenue%2Cyear%2Cauthors%2CexternalIds%2CpublicationVenue%2CpublicationTypes \"HTTP/1.1 200 OK\"\n",
      "2025-08-14 20:52:41,528 [INFO] openai._base_client: Retrying request to /chat/completions in 0.437013 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"event\": \"http_request\", \"url\": \"https://api.semanticscholar.org/graph/v1/paper/search?query=Attention+Is+All+You+Need&limit=5&fields=title%2Cvenue%2Cyear%2Cauthors%2CexternalIds%2CpublicationVenue%2CpublicationTypes\", \"code\": 200}\n",
      "{\"event\": \"reject_candidate\", \"reason\": \"not_trustworthy\", \"score\": 0.7799999999999999, \"title\": \"Attention Is All You Need\"}\n",
      "{\"event\": \"route_after_verify\", \"decision\": \"ApplyCorrections\", \"ver_score\": 10}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-14 20:52:41,972 [INFO] openai._base_client: Retrying request to /chat/completions in 0.896502 seconds\n",
      "2025-08-14 20:52:42,876 [WARNING] ieee-ref-langgraph: LLM json() failed: Connection error.\n",
      "2025-08-14 20:52:43,076 [INFO] httpx: HTTP Request: GET https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term=Attention+is+all+you+need&retmode=json&retmax=1&tool=ieee-ref-agent&email=you%40example.com \"HTTP/1.1 200 OK\"\n",
      "2025-08-14 20:52:43,080 [INFO] httpx: HTTP Request: GET https://api.crossref.org/works?query.title=Attention+is+all+you+need&rows=5&select=title%2Cauthor%2Ccontainer-title%2Cshort-container-title%2Cissued%2CDOI%2Cpage%2Cvolume%2Cissue%2Cpublished-print%2Cpublished-online%2Ctype \"HTTP/1.1 200 OK\"\n",
      "2025-08-14 20:52:43,081 [INFO] httpx: HTTP Request: GET https://export.arxiv.org/api/query?search_query=ti%3A%22Attention+is+all+you+need%22&start=0&max_results=1 \"HTTP/1.1 200 OK\"\n",
      "2025-08-14 20:52:43,165 [INFO] httpx: HTTP Request: GET https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi?db=pubmed&id=40725289&retmode=json&tool=ieee-ref-agent&email=you%40example.com \"HTTP/1.1 200 OK\"\n",
      "2025-08-14 20:52:43,189 [INFO] httpx: HTTP Request: GET https://api.openalex.org/works?filter=title.search%3AAttention+is+all+you+need&per-page=5 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"event\": \"http_request\", \"url\": \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term=Attention+is+all+you+need&retmode=json&retmax=1&tool=ieee-ref-agent&email=you%40example.com\", \"code\": 200}\n",
      "{\"event\": \"http_request\", \"url\": \"https://api.crossref.org/works?query.title=Attention+is+all+you+need&rows=5&select=title%2Cauthor%2Ccontainer-title%2Cshort-container-title%2Cissued%2CDOI%2Cpage%2Cvolume%2Cissue%2Cpublished-print%2Cpublished-online%2Ctype\", \"code\": 200}\n",
      "{\"event\": \"http_request\", \"url\": \"https://export.arxiv.org/api/query?search_query=ti%3A%22Attention+is+all+you+need%22&start=0&max_results=1\", \"code\": 200}\n",
      "{\"event\": \"http_request\", \"url\": \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi?db=pubmed&id=40725289&retmode=json&tool=ieee-ref-agent&email=you%40example.com\", \"code\": 200}\n",
      "{\"event\": \"http_request\", \"url\": \"https://api.openalex.org/works?filter=title.search%3AAttention+is+all+you+need&per-page=5\", \"code\": 200}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-14 20:52:43,958 [INFO] httpx: HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=Attention+is+all+you+need&limit=5&fields=title%2Cvenue%2Cyear%2Cauthors%2CexternalIds%2CpublicationVenue%2CpublicationTypes \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"event\": \"http_request\", \"url\": \"https://api.semanticscholar.org/graph/v1/paper/search?query=Attention+is+all+you+need&limit=5&fields=title%2Cvenue%2Cyear%2Cauthors%2CexternalIds%2CpublicationVenue%2CpublicationTypes\", \"code\": 200}\n",
      "{\"event\": \"reject_candidate\", \"reason\": \"not_trustworthy\", \"score\": 0.7799999999999999, \"title\": \"Attention Is All You Need\"}\n",
      "{\"event\": \"route_after_verify\", \"decision\": \"FormatReference\", \"ver_score\": 11}\n",
      "\n",
      "=== Result ===\n",
      "Resolved Type: journal article\n",
      "Formatted: A. V. E. al., \"Attention is all you need\", *in NeurIPS*, 2017.\n",
      "Verification OK: True\n",
      "Report:\n",
      " Corrections (field: old → new):\n",
      "- title_sentence_case: 'Attention Is All You Need' → 'Attention is all you need'\n",
      "All verification checks passed after corrections.\n",
      "CSL-JSON: {\n",
      "  \"type\": \"article-journal\",\n",
      "  \"title\": \"Attention is all you need\",\n",
      "  \"author\": [\n",
      "    {\n",
      "      \"family\": \"al.\",\n",
      "      \"given\": \"A. Vaswani et\"\n",
      "    }\n",
      "  ],\n",
      "  \"container-title\": \"in NeurIPS\",\n",
      "  \"issued\": {\n",
      "    \"date-parts\": [\n",
      "      [\n",
      "        2017\n",
      "      ]\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "BibTeX:\n",
      " @article{al2017,\n",
      "  author = {A. Vaswani et al.},\n",
      "  title = {Attention is all you need},\n",
      "  journal = {in NeurIPS},\n",
      "  year = {2017}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Agentic IEEE Reference Pipeline (LangGraph) — Single Jupyter Cell (Production Grade, Updated)\n",
    "# - Robust LLM adapter (OpenAI/Azure/Anthropic/Ollama), JSON-only w/ deterministic params\n",
    "# - Async concurrent lookups + retries/backoff: Crossref/OpenAlex/Semantic Scholar/PubMed/arXiv\n",
    "# - Better matching (top-N fetch + trust guard), author formatting, e-locator handling, DOI links\n",
    "# - Verification agents + repair loop with improved circuit breakers\n",
    "# - Mermaid graph rendering via inline JS (no extension) + Kroki SVG fallback (guarded)\n",
    "# - Exports: IEEE string, CSL-JSON (w/ abbrev, URL), BibTeX (w/ url)\n",
    "# - Structured logs (_jlog), polite User-Agent, optional API keys, pydantic validation for LLM JSON\n",
    "\n",
    "import os, re, json, asyncio, logging, textwrap, hashlib, time, sys\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "# ---------- Optional deps ----------\n",
    "try:\n",
    "    from dotenv import load_dotenv; load_dotenv()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    import httpx\n",
    "except Exception:\n",
    "    httpx = None\n",
    "\n",
    "try:\n",
    "    from cachetools import TTLCache\n",
    "    CACHE_AVAILABLE = True\n",
    "except Exception:\n",
    "    TTLCache = None\n",
    "    CACHE_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    from rapidfuzz import fuzz\n",
    "    RF_AVAILABLE = True\n",
    "except Exception:\n",
    "    fuzz = None\n",
    "    RF_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    from pydantic import BaseModel, ValidationError\n",
    "except Exception:\n",
    "    class BaseModel:  # fallback no-op\n",
    "        def __init__(self, **kw): pass\n",
    "        def dict(self, **kw): return {}\n",
    "    class ValidationError(Exception): ...\n",
    "    BaseModel = BaseModel\n",
    "    ValidationError = ValidationError\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from IPython.display import display, Markdown, Image, SVG, HTML\n",
    "\n",
    "# LangGraph\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# ============================ Configuration & Logging ============================\n",
    "\n",
    "@dataclass\n",
    "class PipelineConfig:\n",
    "    timeout_s: float = float(os.getenv(\"IEEE_REF_TIMEOUT\", \"12\"))\n",
    "    concurrency: int = int(os.getenv(\"IEEE_REF_CONCURRENCY\", \"8\"))\n",
    "    cache_ttl_s: int = int(os.getenv(\"IEEE_REF_CACHE_TTL\", \"3600\"))\n",
    "    max_correction_rounds: int = int(os.getenv(\"IEEE_REF_MAX_CORR\", \"3\"))\n",
    "    max_hops: int = int(os.getenv(\"IEEE_REF_MAX_HOPS\", \"12\"))\n",
    "    stagnation_patience: int = int(os.getenv(\"IEEE_REF_STAGNATION\", \"2\"))\n",
    "    llm_provider: str = os.getenv(\"IEEE_REF_LLM\", \"auto\")  # auto|openai|azure|anthropic|ollama|dummy\n",
    "    openai_model: str = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n",
    "    ollama_model: str = os.getenv(\"OLLAMA_MODEL\", \"llama3.2\")\n",
    "    ollama_base: str = os.getenv(\"OLLAMA_BASE_URL\", os.getenv(\"OLLAMA_HOST\", \"http://localhost:11434\"))\n",
    "    agent_threads: int = int(os.getenv(\"IEEE_REF_AGENT_THREADS\", \"6\"))\n",
    "    recursion_limit: int = int(os.getenv(\"IEEE_REF_RECURSION_LIMIT\", \"60\"))\n",
    "\n",
    "LOG_LEVEL = os.getenv(\"IEEE_REF_LOG_LEVEL\", \"INFO\").upper()\n",
    "logging.basicConfig(level=getattr(logging, LOG_LEVEL, logging.INFO), format=\"%(message)s\")\n",
    "logger = logging.getLogger(\"ieee-ref-langgraph\")\n",
    "import json as _json\n",
    "def _jlog(**kw):\n",
    "    try:\n",
    "        print(_json.dumps(kw, ensure_ascii=False))\n",
    "    except Exception:\n",
    "        print(str(kw))\n",
    "\n",
    "CFG = PipelineConfig()\n",
    "DEFAULT_UA = \"ieee-ref-agent/1.0 (mailto:you@example.com)\"\n",
    "\n",
    "# ============================ Utility Functions ============================\n",
    "\n",
    "def safe_json_load(s: Any) -> Optional[Dict[str, Any]]:\n",
    "    if s is None:\n",
    "        return None\n",
    "    if isinstance(s, dict):\n",
    "        return s\n",
    "    try:\n",
    "        sx = s.decode(\"utf-8\", \"ignore\") if isinstance(s, (bytes, bytearray)) else str(s)\n",
    "    except Exception:\n",
    "        sx = str(s)\n",
    "    sx = sx.strip()\n",
    "    try:\n",
    "        if sx.startswith(\"{\"):\n",
    "            return json.loads(sx)\n",
    "    except Exception:\n",
    "        pass\n",
    "    # brace-balanced extraction\n",
    "    i, n = 0, len(sx)\n",
    "    while i < n and sx[i] != \"{\": i += 1\n",
    "    if i >= n: return None\n",
    "    stack = 0; in_str = False; esc = False; start = None\n",
    "    for j in range(i, n):\n",
    "        ch = sx[j]\n",
    "        if in_str:\n",
    "            if esc: esc = False\n",
    "            elif ch == \"\\\\\": esc = True\n",
    "            elif ch == '\"': in_str = False\n",
    "        else:\n",
    "            if ch == '\"': in_str = True\n",
    "            elif ch == \"{\":\n",
    "                if stack == 0: start = j\n",
    "                stack += 1\n",
    "            elif ch == \"}\":\n",
    "                stack -= 1\n",
    "                if stack == 0 and start is not None:\n",
    "                    candidate = sx[start:j+1]\n",
    "                    try: return json.loads(candidate)\n",
    "                    except Exception: start = None\n",
    "    return None\n",
    "\n",
    "def normalize_text(x: Any) -> str:\n",
    "    if x is None: return \"\"\n",
    "    s = str(x).strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def norm_for_compare(x: Any) -> str:\n",
    "    s = normalize_text(x).lower()\n",
    "    s = re.sub(r\"[^\\w\\s]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def token_similarity(a: str, b: str) -> float:\n",
    "    a = norm_for_compare(a); b = norm_for_compare(b)\n",
    "    if not a or not b: return 0.0\n",
    "    if RF_AVAILABLE and fuzz is not None:\n",
    "        return fuzz.token_sort_ratio(a, b) / 100.0\n",
    "    sa, sb = set(a.split()), set(b.split())\n",
    "    inter = sa & sb\n",
    "    union = sa | sb\n",
    "    return len(inter) / max(1, len(union))\n",
    "\n",
    "def authors_to_list(a: Any) -> List[str]:\n",
    "    if not a: return []\n",
    "    if isinstance(a, list):\n",
    "        return [normalize_text(x) for x in a if normalize_text(x)]\n",
    "    parts = re.split(r\",\\s*|\\s+&\\s+| and \", str(a))\n",
    "    return [normalize_text(p) for p in parts if normalize_text(p)]\n",
    "\n",
    "SUFFIXES = {\"jr\", \"jr.\", \"sr\", \"sr.\", \"ii\", \"iii\", \"iv\", \"v\"}\n",
    "\n",
    "def _initials(given: str) -> List[str]:\n",
    "    parts = re.split(r\"\\s+\", given.strip())\n",
    "    out=[]\n",
    "    for p in parts:\n",
    "        if not p: continue\n",
    "        hy = p.split(\"-\")\n",
    "        if len(hy)>1:\n",
    "            out.append(\"-\".join([h[0].upper()+\".\" for h in hy if h]))\n",
    "        elif re.match(r\"^[A-Za-z]\\.$\", p):\n",
    "            out.append(p.upper())\n",
    "        elif p.lower().rstrip(\".\") in SUFFIXES:\n",
    "            out.append(p.capitalize().rstrip(\".\")+\".\")\n",
    "        else:\n",
    "            out.append(p[0].upper()+\".\")\n",
    "    return out\n",
    "\n",
    "def format_author_ieee(name: str) -> str:\n",
    "    n = normalize_text(name)\n",
    "    if not n: return \"\"\n",
    "    if \",\" in n:\n",
    "        last, given = [p.strip() for p in n.split(\",\", 1)]\n",
    "    else:\n",
    "        toks = n.split()\n",
    "        if len(toks) == 1: return toks[0]\n",
    "        last = toks[-1]; given = \" \".join(toks[:-1])\n",
    "    init = \" \".join(_initials(given))\n",
    "    last_tokens = last.split()\n",
    "    if last_tokens and last_tokens[-1].lower().rstrip(\".\") in SUFFIXES:\n",
    "        suf = last_tokens[-1].capitalize().rstrip(\".\")+\".\"\n",
    "        last = \" \".join(last_tokens[:-1])\n",
    "        return f\"{init} {last}, {suf}\".strip(\", \")\n",
    "    return f\"{init} {last}\".strip()\n",
    "\n",
    "def format_authors_ieee_list(auths: List[str]) -> str:\n",
    "    items = [format_author_ieee(a) for a in auths if a]\n",
    "    if not items: return \"\"\n",
    "    if len(items) <= 6:\n",
    "        return \", \".join(items[:-1]) + (\", and \" if len(items) > 1 else \"\") + items[-1] if len(items) > 1 else items[0]\n",
    "    return \", \".join(items[:6]) + \", et al.\"\n",
    "\n",
    "def sentence_case(title: str) -> str:\n",
    "    t = normalize_text(title)\n",
    "    if not t: return \"\"\n",
    "    if t.isupper(): t = t.lower()\n",
    "    tokens = t.split(); out = []\n",
    "    for i, tok in enumerate(tokens):\n",
    "        if tok.isupper() and len(tok) > 1:\n",
    "            out.append(tok)\n",
    "        else:\n",
    "            out.append(tok[:1].upper() + tok[1:].lower() if i == 0 else tok.lower())\n",
    "    res = \" \".join(out)\n",
    "    res = re.sub(r\"\\bieee\\b\", \"IEEE\", res, flags=re.I)\n",
    "    return res\n",
    "\n",
    "def heuristic_abbrev(fullname: str) -> str:\n",
    "    fullname = normalize_text(fullname)\n",
    "    if not fullname: return \"\"\n",
    "    tokens = [t for t in re.split(r\"[\\s,]+\", fullname) if t.lower() not in {\"on\",\"of\",\"and\",\"the\",\"in\",\"for\",\"to\"}]\n",
    "    out = []\n",
    "    for t in tokens[:8]:\n",
    "        if len(t) <= 4 and t.isupper(): out.append(t)\n",
    "        elif len(t) <= 3: out.append(t.capitalize() + \".\")\n",
    "        else: out.append(t[:4].capitalize() + \".\")\n",
    "    return \" \".join(out)\n",
    "\n",
    "def ensure_doi_prefix(doi: str) -> str:\n",
    "    # kept for compatibility; not used anymore\n",
    "    d = normalize_text(doi)\n",
    "    if not d: return \"\"\n",
    "    return d if d.lower().startswith(\"doi:\") else f\"doi:{d}\"\n",
    "\n",
    "def format_doi_link(doi: str) -> str:\n",
    "    d = normalize_text(doi).lower().replace(\"doi:\",\"\").strip()\n",
    "    return f\"https://doi.org/{d}\" if d else \"\"\n",
    "\n",
    "def normalize_pages(p: str) -> Tuple[str, bool]:\n",
    "    p = normalize_text(p).replace(\"—\",\"-\").replace(\"–\",\"-\")\n",
    "    if not p: return \"\", False\n",
    "    # Treat single token or alphanumeric token without dash as eLocator/article number\n",
    "    if (\"-\" not in p) and re.fullmatch(r\"[A-Za-z]?\\d+[A-Za-z]?\", p):\n",
    "        return p, True\n",
    "    return p, False\n",
    "\n",
    "MONTHS_NAME = {\n",
    "    \"1\":\"Jan\",\"2\":\"Feb\",\"3\":\"Mar\",\"4\":\"Apr\",\"5\":\"May\",\"6\":\"Jun\",\n",
    "    \"7\":\"Jul\",\"8\":\"Aug\",\"9\":\"Sep\",\"10\":\"Oct\",\"11\":\"Nov\",\"12\":\"Dec\",\n",
    "}\n",
    "def normalize_month_field(m: Any) -> str:\n",
    "    s = normalize_text(m)\n",
    "    if not s: return \"\"\n",
    "    m_map = {\"jan\":\"1\",\"feb\":\"2\",\"mar\":\"3\",\"apr\":\"4\",\"may\":\"5\",\"jun\":\"6\",\"jul\":\"7\",\"aug\":\"8\",\"sep\":\"9\",\"sept\":\"9\",\"oct\":\"10\",\"nov\":\"11\",\"dec\":\"12\"}\n",
    "    sl = s.strip(\". \").lower()\n",
    "    if sl in m_map: return m_map[sl]\n",
    "    if re.fullmatch(r\"0?[1-9]|1[0-2]\", sl): return str(int(sl))\n",
    "    return s\n",
    "\n",
    "def fingerprint_state(ex: Dict[str, Any], best: Dict[str, Any], sugg: Dict[str, Any]) -> str:\n",
    "    payload = json.dumps({\"ex\": ex, \"best\": best, \"sugg\": sugg}, sort_keys=True, ensure_ascii=False)\n",
    "    return hashlib.sha256(payload.encode(\"utf-8\", \"ignore\")).hexdigest()\n",
    "\n",
    "def safe_str(v: Any) -> str:\n",
    "    try:\n",
    "        if v is None:\n",
    "            return \"\"\n",
    "        return str(v).strip()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "# ============================ Validation Model ============================\n",
    "\n",
    "class ExtractedModel(BaseModel):\n",
    "    title: Optional[str] = None\n",
    "    authors: Optional[List[str]] = None\n",
    "    journal_name: Optional[str] = None\n",
    "    journal_abbrev: Optional[str] = None\n",
    "    conference_name: Optional[str] = None\n",
    "    volume: Optional[str] = None\n",
    "    issue: Optional[str] = None\n",
    "    pages: Optional[str] = None\n",
    "    year: Optional[str] = None\n",
    "    month: Optional[str] = None\n",
    "    doi: Optional[str] = None\n",
    "    publisher: Optional[str] = None\n",
    "    location: Optional[str] = None\n",
    "    edition: Optional[str] = None\n",
    "    isbn: Optional[str] = None\n",
    "    url: Optional[str] = None\n",
    "    arxiv_id: Optional[str] = None\n",
    "\n",
    "# ============================ LLM Adapter ============================\n",
    "\n",
    "class LLMAdapter:\n",
    "    \"\"\"LLM JSON-mode adapter supporting OpenAI, Azure OpenAI, Anthropic, Ollama; falls back to dummy.\"\"\"\n",
    "    def __init__(self, cfg: PipelineConfig):\n",
    "        self.cfg = cfg\n",
    "        self.provider = self._auto_provider(cfg.llm_provider)\n",
    "        self._client = None\n",
    "        self._init_client()\n",
    "        _jlog(event=\"llm_provider_selected\", provider=self.provider)\n",
    "\n",
    "    def _auto_provider(self, p: str) -> str:\n",
    "        if p != \"auto\":\n",
    "            return p\n",
    "        if os.getenv(\"OPENAI_API_KEY\"): return \"openai\"\n",
    "        if os.getenv(\"AZURE_OPENAI_API_KEY\"): return \"azure\"\n",
    "        if os.getenv(\"ANTHROPIC_API_KEY\"): return \"anthropic\"\n",
    "        if os.getenv(\"OLLAMA_BASE_URL\") or os.getenv(\"OLLAMA_HOST\"): return \"ollama\"\n",
    "        return \"dummy\"\n",
    "\n",
    "    def _init_client(self):\n",
    "        prov = self.provider\n",
    "        try:\n",
    "            if prov == \"openai\":\n",
    "                from openai import OpenAI\n",
    "                base = os.getenv(\"OPENAI_API_BASE\")\n",
    "                self._client = OpenAI(base_url=base) if base else OpenAI()\n",
    "            elif prov == \"azure\":\n",
    "                from openai import AzureOpenAI\n",
    "                ep = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "                ver = os.getenv(\"OPENAI_API_VERSION\", \"2024-06-01\")\n",
    "                if not ep:\n",
    "                    raise RuntimeError(\"AZURE_OPENAI_ENDPOINT is not set\")\n",
    "                self._client = AzureOpenAI(azure_endpoint=ep, api_version=ver)\n",
    "            elif prov == \"anthropic\":\n",
    "                import anthropic\n",
    "                self._client = anthropic.AsyncAnthropic()\n",
    "            elif prov == \"ollama\" and httpx is not None:\n",
    "                base = os.getenv(\"OLLAMA_BASE_URL\") or os.getenv(\"OLLAMA_HOST\") or self.cfg.ollama_base\n",
    "                self._client = httpx.AsyncClient(base_url=base, timeout=self.cfg.timeout_s)\n",
    "            else:\n",
    "                self._client = None\n",
    "        except Exception as e:\n",
    "            logger.warning(\"LLM init failed: %s\", e)\n",
    "            self._client = None\n",
    "            self.provider = \"dummy\"\n",
    "\n",
    "    async def _openai_json(self, prompt: str) -> str:\n",
    "        model = self.cfg.openai_model\n",
    "        resp = self._client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\":\"system\",\"content\":\"Return STRICT JSON only. No prose.\"},\n",
    "                      {\"role\":\"user\",\"content\":prompt}],\n",
    "            temperature=0.1,\n",
    "            top_p=0.1,\n",
    "            response_format={\"type\":\"json_object\"},\n",
    "        )\n",
    "        return resp.choices[0].message.content\n",
    "\n",
    "    async def _azure_json(self, prompt: str) -> str:\n",
    "        deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\") or self.cfg.openai_model\n",
    "        resp = self._client.chat.completions.create(\n",
    "            model=deployment,\n",
    "            messages=[{\"role\":\"system\",\"content\":\"Return STRICT JSON only. No prose.\"},\n",
    "                      {\"role\":\"user\",\"content\":prompt}],\n",
    "            temperature=0.1,\n",
    "            top_p=0.1,\n",
    "            response_format={\"type\":\"json_object\"},\n",
    "        )\n",
    "        return resp.choices[0].message.content\n",
    "\n",
    "    async def _anthropic_json(self, prompt: str) -> str:\n",
    "        msg = await self._client.messages.create(\n",
    "            model=os.getenv(\"ANTHROPIC_MODEL\",\"claude-3-5-sonnet-20240620\"),\n",
    "            system=\"Return STRICT JSON only. No prose.\",\n",
    "            max_tokens=1024, temperature=0.1,\n",
    "            messages=[{\"role\":\"user\",\"content\":prompt}],\n",
    "        )\n",
    "        texts = []\n",
    "        for c in msg.content:\n",
    "            if getattr(c, \"type\", None) == \"text\":\n",
    "                texts.append(c.text)\n",
    "        return \"\".join(texts)\n",
    "\n",
    "    async def _ollama_json(self, prompt: str) -> str:\n",
    "        data = {\"model\": self.cfg.ollama_model, \"prompt\": \"Return STRICT JSON only.\\n\\n\" + prompt, \"stream\": False}\n",
    "        r = await self._client.post(\"/api/generate\", json=data)\n",
    "        r.raise_for_status()\n",
    "        return r.json().get(\"response\",\"\")\n",
    "\n",
    "    async def json(self, prompt: str) -> Dict[str, Any]:\n",
    "        try:\n",
    "            if self.provider == \"openai\":\n",
    "                raw = await self._openai_json(prompt)\n",
    "            elif self.provider == \"azure\":\n",
    "                raw = await self._azure_json(prompt)\n",
    "            elif self.provider == \"anthropic\":\n",
    "                raw = await self._anthropic_json(prompt)\n",
    "            elif self.provider == \"ollama\":\n",
    "                raw = await self._ollama_json(prompt)\n",
    "            else:\n",
    "                return {}\n",
    "            return safe_json_load(raw) or {}\n",
    "        except Exception as e:\n",
    "            logger.warning(\"LLM json() failed: %s\", e)\n",
    "            return {}\n",
    "\n",
    "# ============================ Async Source Clients ============================\n",
    "\n",
    "ARXIV_RE = re.compile(r'(arxiv:)?\\s*(\\d{4}\\.\\d{4,5})(v\\d+)?', re.I)\n",
    "DOI_RE = re.compile(r'(10\\.\\d{4,9}/[^\\s,;]+)', re.I)\n",
    "\n",
    "class SourceClient:\n",
    "    NAME: str = \"base\"\n",
    "    def __init__(self, cfg: PipelineConfig, client=None, limiter=None, cache=None):\n",
    "        self.cfg = cfg\n",
    "        self.client = client or (httpx.AsyncClient(timeout=httpx.Timeout(connect=cfg.timeout_s, read=cfg.timeout_s, write=cfg.timeout_s, pool=cfg.timeout_s)) if httpx is not None else None)\n",
    "        self.limiter = limiter or asyncio.Semaphore(cfg.concurrency)\n",
    "        self.cache = cache\n",
    "\n",
    "    def _cache_get(self, key: str):\n",
    "        if not self.cache: return None\n",
    "        return self.cache.get((self.NAME, key))\n",
    "\n",
    "    def _cache_set(self, key: str, val: Dict[str, Any]):\n",
    "        if not self.cache: return\n",
    "        self.cache[(self.NAME, key)] = val\n",
    "\n",
    "    async def _get_json(self, url: str, params: Optional[Dict[str, Any]] = None, headers: Optional[Dict[str, str]] = None) -> Dict[str, Any]:\n",
    "        if self.client is None:\n",
    "            raise RuntimeError(\"HTTP client unavailable.\")\n",
    "        hdrs = {\"User-Agent\": DEFAULT_UA}\n",
    "        if headers: hdrs.update(headers)\n",
    "\n",
    "        attempt = 0\n",
    "        while True:\n",
    "            attempt += 1\n",
    "            try:\n",
    "                async with self.limiter:\n",
    "                    r = await self.client.get(url, params=params, headers=hdrs)\n",
    "                _jlog(event=\"http_request\", url=str(r.request.url), code=r.status_code)\n",
    "                if r.status_code in (429, 500, 502, 503, 504) and attempt <= 4:\n",
    "                    await asyncio.sleep(min(2**attempt, 8) + (0.1 * attempt))\n",
    "                    continue\n",
    "                r.raise_for_status()\n",
    "                ct = r.headers.get(\"content-type\",\"\")\n",
    "                if \"json\" in ct:\n",
    "                    return r.json()\n",
    "                return {\"_raw\": r.text}\n",
    "            except Exception as e:\n",
    "                if attempt <= 2:\n",
    "                    await asyncio.sleep(0.3 * attempt)\n",
    "                    continue\n",
    "                raise\n",
    "\n",
    "    async def by_doi(self, doi: str) -> Optional[Dict[str, Any]]: raise NotImplementedError\n",
    "    async def by_title(self, title: str) -> Optional[Any]: raise NotImplementedError\n",
    "\n",
    "class CrossrefClient(SourceClient):\n",
    "    NAME = \"crossref\"; BASE_URL = \"https://api.crossref.org/works\"\n",
    "    async def by_doi(self, doi: str) -> Optional[Dict[str, Any]]:\n",
    "        key = f\"doi:{doi.lower().strip()}\"\n",
    "        if (c := self._cache_get(key)): return c\n",
    "        try:\n",
    "            data = await self._get_json(f\"{self.BASE_URL}/{doi}\")\n",
    "            msg = data.get(\"message\")\n",
    "            if msg: self._cache_set(key, msg)\n",
    "            return msg\n",
    "        except Exception: return None\n",
    "    async def by_title(self, title: str) -> Optional[List[Dict[str, Any]]]:\n",
    "        key = f\"title:{norm_for_compare(title)}\"\n",
    "        c = self._cache_get(key)\n",
    "        # We'll still fetch up to 5; but keep first item in cache for speed\n",
    "        params = {\n",
    "            \"query.title\": title,\n",
    "            \"rows\": 5,\n",
    "            \"select\":\"title,author,container-title,short-container-title,issued,DOI,page,volume,issue,published-print,published-online,type\"\n",
    "        }\n",
    "        try:\n",
    "            data = await self._get_json(self.BASE_URL, params=params)\n",
    "            items = data.get(\"message\", {}).get(\"items\", [])[:5]\n",
    "            if items and not c:\n",
    "                self._cache_set(key, items[0])\n",
    "            return items\n",
    "        except Exception: return None\n",
    "\n",
    "class OpenAlexClient(SourceClient):\n",
    "    NAME = \"openalex\"; BASE_URL = \"https://api.openalex.org/works\"\n",
    "    async def by_doi(self, doi: str) -> Optional[Dict[str, Any]]:\n",
    "        key = f\"doi:{doi.lower().strip()}\"\n",
    "        if (c := self._cache_get(key)): return c\n",
    "        try:\n",
    "            headers = {\"User-Agent\": DEFAULT_UA}\n",
    "            data = await self._get_json(self.BASE_URL, params={\"filter\": f\"doi:{doi}\"}, headers=headers)\n",
    "            items = data.get(\"results\", [])\n",
    "            it = items[0] if items else None\n",
    "            if it: self._cache_set(key, it)\n",
    "            return it\n",
    "        except Exception: return None\n",
    "    async def by_title(self, title: str) -> Optional[List[Dict[str, Any]]]:\n",
    "        key = f\"title:{norm_for_compare(title)}\"\n",
    "        try:\n",
    "            headers = {\"User-Agent\": DEFAULT_UA}\n",
    "            data = await self._get_json(self.BASE_URL, params={\"filter\": f\"title.search:{title}\", \"per-page\":5}, headers=headers)\n",
    "            items = data.get(\"results\", [])[:5]\n",
    "            return items\n",
    "        except Exception: return None\n",
    "\n",
    "class SemanticScholarClient(SourceClient):\n",
    "    NAME = \"semanticscholar\"; BASE_URL = \"https://api.semanticscholar.org/graph/v1/paper\"\n",
    "    S2_KEY = os.getenv(\"SEMANTIC_SCHOLAR_API_KEY\")\n",
    "    def _headers(self):\n",
    "        h = {\"User-Agent\": DEFAULT_UA}\n",
    "        if self.S2_KEY: h[\"x-api-key\"] = self.S2_KEY\n",
    "        return h\n",
    "    async def by_doi(self, doi: str) -> Optional[Dict[str, Any]]:\n",
    "        key = f\"doi:{doi.lower().strip()}\"\n",
    "        if (c := self._cache_get(key)): return c\n",
    "        try:\n",
    "            data = await self._get_json(f\"{self.BASE_URL}/DOI:{doi}\", params={\"fields\":\"title,venue,year,authors,externalIds,publicationVenue,publicationTypes\"}, headers=self._headers())\n",
    "            if data and not data.get(\"error\"): self._cache_set(key, data)\n",
    "            return data if data and not data.get(\"error\") else None\n",
    "        except Exception: return None\n",
    "    async def by_title(self, title: str) -> Optional[List[Dict[str, Any]]]:\n",
    "        try:\n",
    "            data = await self._get_json(\n",
    "                f\"{self.BASE_URL}/search\",\n",
    "                params={\"query\": title, \"limit\":5, \"fields\":\"title,venue,year,authors,externalIds,publicationVenue,publicationTypes\"},\n",
    "                headers=self._headers()\n",
    "            )\n",
    "            items = data.get(\"data\") or []\n",
    "            return items[:5]\n",
    "        except Exception: return None\n",
    "\n",
    "class PubMedClient(SourceClient):\n",
    "    NAME = \"pubmed\"\n",
    "    ESEARCH = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n",
    "    ESUMMARY = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi\"\n",
    "    async def by_doi(self, doi: str) -> Optional[Dict[str, Any]]:\n",
    "        return None\n",
    "    async def by_title(self, title: str) -> Optional[Dict[str, Any]]:\n",
    "        key = f\"title:{norm_for_compare(title)}\"\n",
    "        if (c := self._cache_get(key)): return c\n",
    "        try:\n",
    "            d = await self._get_json(self.ESEARCH, params={\"db\":\"pubmed\",\"term\":title,\"retmode\":\"json\",\"retmax\":\"1\",\"tool\":\"ieee-ref-agent\",\"email\":\"you@example.com\"})\n",
    "            ids = d.get(\"esearchresult\", {}).get(\"idlist\", [])\n",
    "            if not ids: return None\n",
    "            pmid = ids[0]\n",
    "            d2 = await self._get_json(self.ESUMMARY, params={\"db\":\"pubmed\",\"id\":pmid,\"retmode\":\"json\",\"tool\":\"ieee-ref-agent\",\"email\":\"you@example.com\"})\n",
    "            res = d2.get(\"result\", {}).get(pmid)\n",
    "            if res: self._cache_set(key, res)\n",
    "            return res\n",
    "        except Exception: return None\n",
    "\n",
    "class ArxivClient(SourceClient):\n",
    "    NAME = \"arxiv\"; BASE_URL = \"https://export.arXiv.org/api/query\"\n",
    "    async def by_doi(self, doi: str) -> Optional[Dict[str, Any]]:\n",
    "        return None\n",
    "    async def by_title(self, title: str) -> Optional[Dict[str, Any]]:\n",
    "        # keep title search for fallback\n",
    "        try:\n",
    "            if self.client is None:\n",
    "                return None\n",
    "            async with self.limiter:\n",
    "                r = await self.client.get(self.BASE_URL, params={\"search_query\": f\"ti:\\\"{title}\\\"\", \"start\":0, \"max_results\":1}, headers={\"Accept\":\"application/atom+xml\",\"User-Agent\":DEFAULT_UA})\n",
    "                _jlog(event=\"http_request\", url=str(r.request.url), code=r.status_code)\n",
    "                r.raise_for_status()\n",
    "                xml = r.text\n",
    "                tmatch = re.search(r\"<title>(.*?)</title>\", xml, flags=re.DOTALL)\n",
    "                if not tmatch: return None\n",
    "                title0 = normalize_text(re.sub(r\"\\s+\", \" \", tmatch.group(1)))\n",
    "                auths = [normalize_text(a) for a in re.findall(r\"<name>(.*?)</name>\", xml)]\n",
    "                ymatch = re.search(r\"<published>(\\d{4})-\", xml)\n",
    "                year0 = ymatch.group(1) if ymatch else \"\"\n",
    "                return {\"title\": title0, \"authors\": auths, \"journal_name\":\"arXiv\", \"year\": year0, \"doi\":\"\"}\n",
    "        except Exception:\n",
    "            return None\n",
    "    async def by_id(self, arx: str) -> Optional[Dict[str, Any]]:\n",
    "        try:\n",
    "            if self.client is None:\n",
    "                return None\n",
    "            async with self.limiter:\n",
    "                r = await self.client.get(self.BASE_URL, params={\"id_list\": arx}, headers={\"Accept\":\"application/atom+xml\",\"User-Agent\":DEFAULT_UA})\n",
    "                _jlog(event=\"http_request\", url=str(r.request.url), code=r.status_code)\n",
    "                r.raise_for_status()\n",
    "                xml = r.text\n",
    "                tmatch = re.search(r\"<title>(.*?)</title>\", xml, flags=re.DOTALL)\n",
    "                if not tmatch: return None\n",
    "                title0 = normalize_text(re.sub(r\"\\s+\", \" \", tmatch.group(1)))\n",
    "                auths = [normalize_text(a) for a in re.findall(r\"<name>(.*?)</name>\", xml)]\n",
    "                ymatch = re.search(r\"<published>(\\d{4})-\", xml)\n",
    "                year0 = ymatch.group(1) if ymatch else \"\"\n",
    "                return {\"title\": title0, \"authors\": auths, \"journal_name\":\"arXiv\", \"year\": year0, \"doi\":\"\"}\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "# ============================ Candidate normalization & scoring ============================\n",
    "\n",
    "def normalize_candidate(source: str, rec: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    out: Dict[str, Any] = {\"source\": source, \"raw\": rec}\n",
    "    if source == \"crossref\":\n",
    "        out[\"title\"] = normalize_text((rec.get(\"title\") or [\"\"])[0]) if rec.get(\"title\") else \"\"\n",
    "        out[\"authors\"] = [normalize_text(f\"{a.get('given','')} {a.get('family','')}\".strip()) for a in rec.get(\"author\", [])] if rec.get(\"author\") else []\n",
    "        out[\"journal_name\"] = normalize_text((rec.get(\"container-title\") or [\"\"])[0]) if rec.get(\"container-title\") else \"\"\n",
    "        out[\"journal_abbrev\"] = normalize_text((rec.get(\"short-container-title\") or [\"\"])[0]) if rec.get(\"short-container-title\") else \"\"\n",
    "        out[\"volume\"] = normalize_text(rec.get(\"volume\") or \"\")\n",
    "        out[\"issue\"]  = normalize_text(rec.get(\"issue\") or \"\")\n",
    "        out[\"pages\"]  = normalize_text(rec.get(\"page\") or \"\")\n",
    "        out[\"doi\"]    = normalize_text(rec.get(\"DOI\") or \"\")\n",
    "        out[\"cr_type\"]= normalize_text(rec.get(\"type\") or \"\")\n",
    "        y, m = \"\", \"\"\n",
    "        for src in (\"issued\",\"published-print\",\"published-online\"):\n",
    "            dp = (rec.get(src) or {}).get(\"date-parts\")\n",
    "            if dp:\n",
    "                y = str(dp[0][0])\n",
    "                if len(dp[0])>1: m = str(dp[0][1])\n",
    "                break\n",
    "        out[\"year\"], out[\"month\"] = y, normalize_month_field(m)\n",
    "    elif source == \"openalex\":\n",
    "        out[\"title\"] = normalize_text(rec.get(\"display_name\") or rec.get(\"title\") or \"\")\n",
    "        out[\"authors\"] = [normalize_text(a.get(\"author\", {}).get(\"display_name\") or \"\") for a in rec.get(\"authorships\", [])] if rec.get(\"authorships\") else []\n",
    "        hv = rec.get(\"host_venue\", {}) if isinstance(rec.get(\"host_venue\"), dict) else {}\n",
    "        out[\"journal_name\"] = normalize_text(hv.get(\"display_name\") or \"\")\n",
    "        out[\"journal_abbrev\"] = normalize_text(hv.get(\"abbrev\") or \"\")\n",
    "        out[\"doi\"] = normalize_text(rec.get(\"doi\") or \"\")\n",
    "        out[\"volume\"] = normalize_text(rec.get(\"biblio\", {}).get(\"volume\") or \"\")\n",
    "        out[\"issue\"]  = normalize_text(rec.get(\"biblio\", {}).get(\"issue\") or \"\")\n",
    "        fp = rec.get(\"biblio\", {}).get(\"first_page\") or \"\"\n",
    "        lp = rec.get(\"biblio\", {}).get(\"last_page\") or \"\"\n",
    "        out[\"pages\"] = f\"{fp}-{lp}\" if fp and lp else normalize_text(fp or \"\")\n",
    "        out[\"year\"]   = str(rec.get(\"publication_year\") or (rec.get(\"from_publication_date\") or \"\")[:4] or \"\")\n",
    "        out[\"month\"]  = \"\"\n",
    "        out[\"oa_is_proceedings\"] = \"proceedings\" in norm_for_compare(hv.get(\"display_name\") or \"\")\n",
    "    elif source == \"semanticscholar\":\n",
    "        out[\"title\"] = normalize_text(rec.get(\"title\") or \"\")\n",
    "        out[\"authors\"] = [normalize_text(a.get(\"name\") or \"\") for a in rec.get(\"authors\", [])] if rec.get(\"authors\") else []\n",
    "        out[\"journal_name\"] = normalize_text(rec.get(\"venue\") or (rec.get(\"publicationVenue\") or {}).get(\"name\") or \"\")\n",
    "        out[\"journal_abbrev\"] = \"\"\n",
    "        eid = rec.get(\"externalIds\") or {}\n",
    "        out[\"doi\"] = normalize_text(eid.get(\"DOI\") or rec.get(\"doi\") or \"\")\n",
    "        out[\"year\"]   = normalize_text(rec.get(\"year\") or \"\")\n",
    "        out[\"month\"]  = \"\"\n",
    "        out[\"s2_types\"] = [normalize_text(t) for t in (rec.get(\"publicationTypes\") or [])]\n",
    "    elif source == \"pubmed\":\n",
    "        out[\"title\"] = normalize_text(rec.get(\"title\") or rec.get(\"sorttitle\") or \"\")\n",
    "        out[\"authors\"] = [normalize_text(a.get(\"name\")) for a in rec.get(\"authors\", []) if a.get(\"name\")] if rec.get(\"authors\") else []\n",
    "        out[\"journal_name\"] = normalize_text((rec.get(\"fulljournalname\") or rec.get(\"source\") or \"\"))\n",
    "        out[\"journal_abbrev\"] = normalize_text(rec.get(\"source\") or \"\")\n",
    "        out[\"doi\"] = normalize_text((rec.get(\"elocationid\") or \"\").replace(\"doi:\",\"\").strip())\n",
    "        out[\"volume\"] = normalize_text(rec.get(\"volume\") or \"\")\n",
    "        out[\"issue\"]  = normalize_text(rec.get(\"issue\") or \"\")\n",
    "        out[\"pages\"]  = normalize_text(rec.get(\"pages\") or \"\")\n",
    "        out[\"year\"]   = normalize_text((rec.get(\"pubdate\") or \"\").split(\" \")[0])\n",
    "        out[\"month\"]  = \"\"\n",
    "    elif source == \"arxiv\":\n",
    "        out[\"title\"] = normalize_text(rec.get(\"title\") or \"\")\n",
    "        out[\"authors\"] = [normalize_text(a) for a in rec.get(\"authors\", [])]\n",
    "        out[\"journal_name\"] = \"arXiv\"\n",
    "        out[\"journal_abbrev\"] = \"arXiv\"\n",
    "        out[\"doi\"] = normalize_text(rec.get(\"doi\") or \"\")\n",
    "        out[\"year\"] = normalize_text(rec.get(\"year\") or \"\")\n",
    "        out[\"month\"] = \"\"\n",
    "        out[\"volume\"] = \"\"\n",
    "        out[\"issue\"] = \"\"\n",
    "        out[\"pages\"] = \"\"\n",
    "    else:\n",
    "        out.update({k:\"\" for k in (\"title\",\"authors\",\"journal_name\",\"journal_abbrev\",\"doi\",\"volume\",\"issue\",\"pages\",\"year\",\"month\")})\n",
    "    return out\n",
    "\n",
    "def score_candidate(extracted: Dict[str, Any], cand: Dict[str, Any]) -> float:\n",
    "    score = 0.0\n",
    "    ex_doi = normalize_text(extracted.get(\"doi\") or \"\").lower().replace(\"doi:\",\"\")\n",
    "    ca_doi = normalize_text(cand.get(\"doi\") or \"\").lower().replace(\"doi:\",\"\")\n",
    "    if ex_doi and ca_doi and ex_doi == ca_doi: score += 1.0\n",
    "    score += 0.6 * token_similarity(extracted.get(\"title\") or \"\", cand.get(\"title\") or \"\")\n",
    "    ex_auth = [a.split()[-1].lower() for a in authors_to_list(extracted.get(\"authors\")) if a.split()]\n",
    "    ca_auth = [a.split()[-1].lower() for a in authors_to_list(cand.get(\"authors\")) if a.split()]\n",
    "    if ex_auth and ca_auth:\n",
    "        inter = len(set(ex_auth) & set(ca_auth))\n",
    "        score += 0.2 * (inter / max(1, len(set(ex_auth) | set(ca_auth))))\n",
    "    ey = str(extracted.get(\"year\") or \"\").strip()\n",
    "    cy = str(cand.get(\"year\") or \"\").strip()\n",
    "    if ey and cy and ey == cy: score += 0.1\n",
    "    src_weight = {\"crossref\": 0.12, \"openalex\": 0.08, \"semanticscholar\": 0.06, \"pubmed\": 0.05, \"arxiv\": 0.03}\n",
    "    score += src_weight.get(cand.get(\"source\",\"\"), 0.0)\n",
    "    return score\n",
    "\n",
    "# ============================ Type reconciliation ============================\n",
    "\n",
    "TYPE_CANON = {\n",
    "    \"journal-article\": \"journal article\",\n",
    "    \"paper-conference\": \"conference paper\",\n",
    "    \"proceedings-article\": \"conference paper\",\n",
    "    \"book-chapter\": \"book chapter\",\n",
    "    \"book\": \"book\",\n",
    "    \"dataset\": \"dataset\",\n",
    "    \"standard\": \"standard\",\n",
    "    \"report\": \"technical report\",\n",
    "    \"thesis\": \"thesis\",\n",
    "}\n",
    "\n",
    "def reconcile_type(initial_type: str, candidates: List[Dict[str, Any]], llm_vote: Optional[str]) -> str:\n",
    "    votes = []\n",
    "    if initial_type: votes.append(initial_type)\n",
    "    if llm_vote: votes.append(llm_vote.lower())\n",
    "    for c in candidates or []:\n",
    "        if c[\"source\"] == \"crossref\":\n",
    "            t = c.get(\"cr_type\",\"\")\n",
    "            if t: votes.append(TYPE_CANON.get(t, t))\n",
    "        elif c[\"source\"] == \"openalex\":\n",
    "            if c.get(\"oa_is_proceedings\"): votes.append(\"conference paper\")\n",
    "        elif c[\"source\"] == \"semanticscholar\":\n",
    "            types = c.get(\"s2_types\") or []\n",
    "            if any(\"conference\" in t for t in types): votes.append(\"conference paper\")\n",
    "            if any(\"journal\" in t for t in types): votes.append(\"journal article\")\n",
    "            if any(\"book\" in t for t in types): votes.append(\"book\")\n",
    "        elif c[\"source\"] == \"arxiv\":\n",
    "            votes.append(\"preprint\")\n",
    "    from collections import Counter\n",
    "    cnt = Counter([v.lower() for v in votes if v])\n",
    "    if not cnt: return initial_type or \"other\"\n",
    "    return cnt.most_common(1)[0][0]\n",
    "\n",
    "# ============================ Verification Agents (threaded) ============================\n",
    "\n",
    "def _prefer_abbrev(a: str, b: str) -> str:\n",
    "    cand = [x for x in [a, b] if x]\n",
    "    if not cand: return \"\"\n",
    "    def score(x):\n",
    "        s = x.strip()\n",
    "        return (sum(1 for c in s if c.isupper()), -len(s))\n",
    "    return sorted(cand, key=score, reverse=True)[0]\n",
    "\n",
    "def agent_journal(extracted: Dict[str, Any], best: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    ex_j = normalize_text(extracted.get(\"journal_name\") or \"\")\n",
    "    ex_ab = normalize_text(extracted.get(\"journal_abbrev\") or \"\")\n",
    "    be_j = normalize_text(best.get(\"journal_name\") or \"\")\n",
    "    be_ab = normalize_text(best.get(\"journal_abbrev\") or \"\")\n",
    "    sim_full = token_similarity(ex_j, be_j) if ex_j and be_j else 0.0\n",
    "    sim_ab   = token_similarity(ex_ab, be_ab) if ex_ab and be_ab else 0.0\n",
    "    ok = (sim_full >= 0.6) or (sim_ab >= 0.6) or (bool(ex_j) and not be_j)\n",
    "    corr = {}\n",
    "    if be_j and be_j != ex_j: corr[\"journal_name\"] = be_j\n",
    "    if (be_ab and be_ab != ex_ab) or (not ex_ab and (be_ab or be_j)):\n",
    "        chosen = _prefer_abbrev(be_ab, heuristic_abbrev(be_j or ex_j))\n",
    "        corr[\"journal_abbrev\"] = chosen\n",
    "    return {\"ok\": ok, \"correction\": corr or None}\n",
    "\n",
    "def agent_authors(extracted: Dict[str, Any], best: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    ex = authors_to_list(extracted.get(\"authors\"))\n",
    "    be = authors_to_list(best.get(\"authors\"))\n",
    "    if be:\n",
    "        matches = 0\n",
    "        for ea in ex:\n",
    "            last = ea.split()[-1].lower() if ea.split() else \"\"\n",
    "            if any((ba.split()[-1].lower() if ba.split() else \"\") == last for ba in be):\n",
    "                matches += 1\n",
    "        required = max(1, int(0.5 * len(ex))) if ex else 1\n",
    "        if matches >= required:\n",
    "            if any(re.match(r\"^[A-Z]\\.\", p.split()[0]) if p.split() else False for p in ex[:3]):\n",
    "                return {\"ok\": True, \"correction\": {\"authors\": be}}\n",
    "            return {\"ok\": True}\n",
    "        return {\"ok\": False, \"correction\": {\"authors\": be}}\n",
    "    return {\"ok\": bool(ex)}\n",
    "\n",
    "def agent_title(extracted: Dict[str, Any], best: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    ex_t = normalize_text(extracted.get(\"title\") or \"\")\n",
    "    be_t = normalize_text(best.get(\"title\") or \"\")\n",
    "    desired = sentence_case(ex_t) if ex_t else \"\"\n",
    "    if be_t:\n",
    "        sim = token_similarity(ex_t, be_t)\n",
    "        if sim >= 0.7:\n",
    "            if ex_t != desired:\n",
    "                return {\"ok\": True, \"correction\": {\"title\": desired}}\n",
    "            return {\"ok\": True}\n",
    "        return {\"ok\": False, \"correction\": {\"title\": be_t}}\n",
    "    else:\n",
    "        if ex_t and ex_t != desired:\n",
    "            return {\"ok\": False, \"correction\": {\"title\": desired}}\n",
    "        return {\"ok\": bool(ex_t)}\n",
    "\n",
    "def agent_year_month(extracted: Dict[str, Any], best: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    ex_y = str(extracted.get(\"year\") or \"\")\n",
    "    ex_m = normalize_month_field(extracted.get(\"month\") or \"\")\n",
    "    be_y = str(best.get(\"year\") or \"\")\n",
    "    be_m = normalize_month_field(best.get(\"month\") or \"\")\n",
    "    ok = True; corr = {}\n",
    "    if be_y and be_y != ex_y: corr[\"year\"] = be_y; ok = False\n",
    "    if be_m and be_m != ex_m: corr[\"month\"] = be_m; ok = False\n",
    "    return {\"ok\": ok, \"correction\": corr or None}\n",
    "\n",
    "def agent_vipd(extracted: Dict[str, Any], best: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    exv, exi, exp, exd = [normalize_text(extracted.get(k) or \"\") for k in (\"volume\",\"issue\",\"pages\",\"doi\")]\n",
    "    bev, bei, bep, bed = [normalize_text(best.get(k) or \"\") for k in (\"volume\",\"issue\",\"pages\",\"doi\")]\n",
    "    ok = True; corr = {}\n",
    "    if bev and bev != exv: corr[\"volume\"] = bev; ok = False\n",
    "    if bei and bei != exi: corr[\"issue\"]  = bei; ok = False\n",
    "    if bep and bep != exp: corr[\"pages\"]  = bep; ok = False\n",
    "    if bed and bed.lower().replace(\"doi:\",\"\") != exd.lower().replace(\"doi:\",\"\"):\n",
    "        corr[\"doi\"] = bed; ok = False\n",
    "    return {\"ok\": ok, \"correction\": corr or None}\n",
    "\n",
    "def agent_presence(extracted: Dict[str, Any], best: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    return {\"ok\": bool(extracted.get(\"title\")) and bool(extracted.get(\"authors\"))}\n",
    "\n",
    "# ============================ LangGraph State & Nodes ============================\n",
    "\n",
    "class PipelineState(TypedDict, total=False):\n",
    "    reference: str\n",
    "    type: str\n",
    "    extracted: Dict[str, Any]\n",
    "    candidates: List[Dict[str, Any]]\n",
    "    best: Dict[str, Any]\n",
    "    verification: Dict[str, bool]\n",
    "    suggestions: Dict[str, Any]\n",
    "    corrections: List[Tuple[str, Any, Any]]\n",
    "    formatted: str\n",
    "    report: str\n",
    "    attempts: int\n",
    "    hops: int\n",
    "    _made_changes_last_cycle: bool\n",
    "    _cfg: PipelineConfig\n",
    "    _llm: Any\n",
    "    _http: Any\n",
    "    _cache: Any\n",
    "    _limiter: Any\n",
    "    _sources: Any\n",
    "    _llm_type_vote: Optional[str]\n",
    "    csl_json: Dict[str, Any]\n",
    "    bibtex: str\n",
    "    _ver_score: int\n",
    "    _stagnation: int\n",
    "    _fp: str\n",
    "    _fp_history: set\n",
    "    _loop_detected: bool\n",
    "\n",
    "async def _init_runtime(state: PipelineState) -> PipelineState:\n",
    "    cfg = state.get(\"_cfg\") or CFG\n",
    "    llm = LLMAdapter(cfg)\n",
    "    http = httpx.AsyncClient(timeout=httpx.Timeout(connect=cfg.timeout_s, read=cfg.timeout_s, write=cfg.timeout_s, pool=cfg.timeout_s)) if httpx is not None else None\n",
    "    cache = TTLCache(maxsize=1000, ttl=cfg.cache_ttl_s) if CACHE_AVAILABLE and TTLCache is not None else None\n",
    "    limiter = asyncio.Semaphore(cfg.concurrency)\n",
    "    sources = [\n",
    "        CrossrefClient(cfg, client=http, limiter=limiter, cache=cache),\n",
    "        OpenAlexClient(cfg, client=http, limiter=limiter, cache=cache),\n",
    "        SemanticScholarClient(cfg, client=http, limiter=limiter, cache=cache),\n",
    "        PubMedClient(cfg, client=http, limiter=limiter, cache=cache),\n",
    "        ArxivClient(cfg, client=http, limiter=limiter, cache=cache),\n",
    "    ]\n",
    "    state.update({\n",
    "        \"_cfg\": cfg, \"_llm\": llm, \"_http\": http, \"_cache\": cache,\n",
    "        \"_limiter\": limiter, \"_sources\": sources,\n",
    "    })\n",
    "    state.setdefault(\"hops\", 0)\n",
    "    state.setdefault(\"attempts\", 0)\n",
    "    state.setdefault(\"_ver_score\", -1)\n",
    "    state.setdefault(\"_stagnation\", 0)\n",
    "    state.setdefault(\"_fp\", \"\")\n",
    "    state.setdefault(\"_fp_history\", set())\n",
    "    state.setdefault(\"_loop_detected\", False)\n",
    "    state.setdefault(\"_made_changes_last_cycle\", False)\n",
    "    return state\n",
    "\n",
    "# --- Detect type using heuristics + LLM vote (once) ---\n",
    "async def node_detect_type_async(state: PipelineState) -> PipelineState:\n",
    "    ref = state[\"reference\"]\n",
    "    rtype = \"other\"\n",
    "    if re.search(r\"\\bvol\\.|no\\.|pp\\.\", ref, flags=re.I):\n",
    "        rtype = \"journal article\"\n",
    "    if re.search(r\"\\bin\\b.+(proc|conference|symposium|workshop)\", ref, flags=re.I):\n",
    "        rtype = \"conference paper\"\n",
    "    if re.search(r\"\\bISBN\\b\", ref, flags=re.I):\n",
    "        rtype = \"book\"\n",
    "    llm: LLMAdapter = state[\"_llm\"]\n",
    "    vote = await llm.json(\n",
    "        \"Classify this reference into one of: journal article, conference paper, book, book chapter, thesis, technical report, dataset, standard, software, other. \"\n",
    "        \"Return JSON {\\\"type\\\": \\\"...\\\"}. Ref:\\n\" + ref\n",
    "    )\n",
    "    state[\"_llm_type_vote\"] = (vote or {}).get(\"type\")\n",
    "    state[\"type\"] = reconcile_type(rtype, [], state[\"_llm_type_vote\"])\n",
    "    return state\n",
    "\n",
    "# --- Parse/extract using LLM-first with regex/arXiv/DOI fallback ---\n",
    "async def node_parse_extract_async(state: PipelineState) -> PipelineState:\n",
    "    ref, rtype = state[\"reference\"], state[\"type\"]\n",
    "    llm: LLMAdapter = state[\"_llm\"]\n",
    "    prompt = (\n",
    "        \"Parse the IEEE-style reference. Return STRICT JSON. Keys among:\\n\"\n",
    "        \"title, authors (list or string), journal_name, journal_abbrev, conference_name,\\n\"\n",
    "        \"volume, issue, pages, year, month, doi, publisher, location, edition, isbn, url.\\n\"\n",
    "        \"Omit unknown keys. JSON ONLY.\\n\\n\"\n",
    "        f\"Type hint: {rtype}\\nReference: {ref}\"\n",
    "    )\n",
    "    parsed = await llm.json(prompt)\n",
    "    if not parsed:\n",
    "        parsed = {}\n",
    "        m = re.search(r\"“([^”]{3,})”|\\\"([^\\\"]{3,})\\\"\", ref)\n",
    "        if m:\n",
    "            parsed[\"title\"] = normalize_text(m.group(1) or m.group(2))\n",
    "            prefix = ref[:m.start()]\n",
    "            parsed[\"authors\"] = authors_to_list(prefix)\n",
    "        dm = DOI_RE.search(ref)\n",
    "        if dm: parsed[\"doi\"] = dm.group(1)\n",
    "        am = ARXIV_RE.search(ref)\n",
    "        if am: parsed[\"arxiv_id\"] = am.group(2)\n",
    "        pm = re.search(r\"pp\\.?\\s*([\\d\\u2013\\u2014\\-]+)\", ref, flags=re.I)\n",
    "        if pm: parsed[\"pages\"] = pm.group(1).replace(\"\\u2013\",\"-\").replace(\"\\u2014\",\"-\")\n",
    "        vm = re.search(r\"vol\\.?\\s*([0-9A-Za-z]+)\", ref, flags=re.I)\n",
    "        if vm: parsed[\"volume\"] = vm.group(1)\n",
    "        im = re.search(r\"no\\.?\\s*([0-9A-Za-z]+)\", ref, flags=re.I)\n",
    "        if im: parsed[\"issue\"] = im.group(1)\n",
    "        y = re.search(r\"\\b(19|20)\\d{2}\\b\", ref)\n",
    "        if y: parsed[\"year\"] = y.group(0)\n",
    "        if m:\n",
    "            after = ref[m.end():]\n",
    "            jm = re.search(r\",\\s*([^,]+?),\", after)\n",
    "            if jm: parsed[\"journal_name\"] = normalize_text(jm.group(1))\n",
    "    if isinstance(parsed.get(\"authors\"), str):\n",
    "        parsed[\"authors\"] = authors_to_list(parsed[\"authors\"])\n",
    "    if parsed.get(\"month\"):\n",
    "        parsed[\"month\"] = normalize_month_field(parsed[\"month\"])\n",
    "    try:\n",
    "        parsed = ExtractedModel(**parsed).dict(exclude_none=True)\n",
    "    except ValidationError as ve:\n",
    "        _jlog(event=\"llm_parse_validation_error\", errors=str(ve))\n",
    "    state[\"extracted\"] = parsed\n",
    "    return state\n",
    "\n",
    "# --- Multi-source lookup (concurrent, top-N, arXiv by id) ---\n",
    "async def node_multisource_lookup_async(state: PipelineState) -> PipelineState:\n",
    "    ex, sources = state[\"extracted\"], state[\"_sources\"]\n",
    "    doi = normalize_text(ex.get(\"doi\") or \"\").lower().replace(\"doi:\",\"\")\n",
    "    title = normalize_text(ex.get(\"title\") or \"\")\n",
    "    arxiv_id = normalize_text(ex.get(\"arxiv_id\") or \"\")\n",
    "\n",
    "    tasks = []\n",
    "    for s in sources:\n",
    "        if arxiv_id and isinstance(s, ArxivClient):\n",
    "            tasks.append(s.by_id(arxiv_id))\n",
    "        if doi:\n",
    "            tasks.append(s.by_doi(doi))\n",
    "        if title:\n",
    "            tasks.append(s.by_title(title))\n",
    "\n",
    "    results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "    out_norm: List[Dict[str, Any]] = []\n",
    "\n",
    "    def _add(source_name, rec):\n",
    "        if isinstance(rec, list):\n",
    "            for r in rec:\n",
    "                if r: out_norm.append(normalize_candidate(source_name, r))\n",
    "        elif isinstance(rec, dict) and rec:\n",
    "            out_norm.append(normalize_candidate(source_name, rec))\n",
    "\n",
    "    idx = 0\n",
    "    for s in sources:\n",
    "        if arxiv_id and isinstance(s, ArxivClient):\n",
    "            _add(s.NAME, results[idx]); idx+=1\n",
    "        if doi:\n",
    "            _add(s.NAME, results[idx]); idx+=1\n",
    "        if title:\n",
    "            _add(s.NAME, results[idx]); idx+=1\n",
    "\n",
    "    dedup: Dict[Tuple[str, str], Dict[str, Any]] = {}\n",
    "    for c in out_norm:\n",
    "        key = (c[\"source\"], c.get(\"doi\") or c.get(\"title\") or \"\")\n",
    "        dedup[key] = c\n",
    "    state[\"candidates\"] = list(dedup.values())\n",
    "    return state\n",
    "\n",
    "def is_trustworthy_match(ex, cand) -> bool:\n",
    "    ex_doi = normalize_text(ex.get(\"doi\")).lower().replace(\"doi:\",\"\")\n",
    "    ca_doi = normalize_text(cand.get(\"doi\")).lower().replace(\"doi:\",\"\")\n",
    "    if ex_doi and ca_doi and ex_doi == ca_doi:\n",
    "        return True\n",
    "    t_sim = token_similarity(ex.get(\"title\",\"\"), cand.get(\"title\",\"\"))\n",
    "    ex_last = {a.split()[-1].lower() for a in authors_to_list(ex.get(\"authors\")) if a.split()}\n",
    "    ca_last = {a.split()[-1].lower() for a in authors_to_list(cand.get(\"authors\")) if a.split()}\n",
    "    return (t_sim >= 0.8) and bool(ex_last & ca_last)\n",
    "\n",
    "def node_select_best(state: PipelineState) -> PipelineState:\n",
    "    ex = state[\"extracted\"]; candidates = state.get(\"candidates\") or []\n",
    "    if not candidates:\n",
    "        state[\"best\"] = {}\n",
    "        return state\n",
    "    best, best_score = None, -1.0\n",
    "    for c in candidates:\n",
    "        sc = score_candidate(ex, c)\n",
    "        if sc > best_score:\n",
    "            best, best_score = c, sc\n",
    "    if best and not is_trustworthy_match(ex, best):\n",
    "        _jlog(event=\"reject_candidate\", reason=\"not_trustworthy\", score=best_score, title=best.get(\"title\"))\n",
    "        best = {}\n",
    "    state[\"best\"] = best or {}\n",
    "    return state\n",
    "\n",
    "def node_reconcile_type(state: PipelineState) -> PipelineState:\n",
    "    state[\"type\"] = reconcile_type(state.get(\"type\",\"other\"), state.get(\"candidates\") or [], state.get(\"_llm_type_vote\"))\n",
    "    return state\n",
    "\n",
    "def node_verify_agents(state: PipelineState) -> PipelineState:\n",
    "    ex = state[\"extracted\"]\n",
    "    be = state.get(\"best\") or {}\n",
    "\n",
    "    agents = [agent_journal, agent_authors, agent_title, agent_year_month, agent_vipd, agent_presence]\n",
    "    results = {}\n",
    "    with ThreadPoolExecutor(max_workers=CFG.agent_threads) as pool:\n",
    "        fut_map = {pool.submit(a, ex, be): a.__name__ for a in agents}\n",
    "        for fut in as_completed(fut_map):\n",
    "            name = fut_map[fut]\n",
    "            try:\n",
    "                results[name] = fut.result()\n",
    "            except Exception as e:\n",
    "                logger.exception(\"Agent %s failed: %s\", name, e)\n",
    "                results[name] = {\"ok\": False}\n",
    "\n",
    "    suggestions = {}\n",
    "    for out in results.values():\n",
    "        if out.get(\"correction\"):\n",
    "            suggestions.update(out[\"correction\"])\n",
    "\n",
    "    vipd_ok = results.get(\"agent_vipd\", {}).get(\"ok\", False)\n",
    "    ym_ok = results.get(\"agent_year_month\", {}).get(\"ok\", False)\n",
    "\n",
    "    verification = {\n",
    "        \"title\":          results.get(\"agent_title\", {}).get(\"ok\", False),\n",
    "        \"authors\":        results.get(\"agent_authors\", {}).get(\"ok\", False),\n",
    "        \"journal_name\":   results.get(\"agent_journal\", {}).get(\"ok\", False),\n",
    "        \"journal_abbrev\": results.get(\"agent_journal\", {}).get(\"ok\", False),\n",
    "        \"year\":           ym_ok,\n",
    "        \"month\":          ym_ok,\n",
    "        \"volume\":         vipd_ok,\n",
    "        \"issue\":          vipd_ok,\n",
    "        \"pages\":          vipd_ok,\n",
    "        \"doi\":            vipd_ok,\n",
    "        \"presence\":       results.get(\"agent_presence\", {}).get(\"ok\", False),\n",
    "    }\n",
    "\n",
    "    ver_score = sum(1 for v in verification.values() if v)\n",
    "    last_score = state.get(\"_ver_score\", -1)\n",
    "    stagnation = state.get(\"_stagnation\", 0)\n",
    "    stagnation = 0 if ver_score > last_score else (stagnation + 1)\n",
    "\n",
    "    state[\"_ver_score\"] = ver_score\n",
    "    state[\"_stagnation\"] = stagnation\n",
    "    state[\"verification\"] = verification\n",
    "    state[\"suggestions\"] = suggestions\n",
    "    state[\"hops\"] = (state.get(\"hops\") or 0) + 1\n",
    "\n",
    "    fp = fingerprint_state(ex, be, suggestions)\n",
    "    hist = state.get(\"_fp_history\") or set()\n",
    "    if fp in hist:\n",
    "        state[\"_loop_detected\"] = True\n",
    "    else:\n",
    "        hist.add(fp)\n",
    "        state[\"_fp_history\"] = hist\n",
    "        state[\"_loop_detected\"] = False\n",
    "\n",
    "    state[\"_fp\"] = fp\n",
    "    return state\n",
    "\n",
    "def node_apply_corrections(state: PipelineState) -> PipelineState:\n",
    "    ex = dict(state[\"extracted\"])\n",
    "    best = state.get(\"best\") or {}\n",
    "    suggestions = state.get(\"suggestions\") or {}\n",
    "    changes: List[Tuple[str, Any, Any]] = []\n",
    "\n",
    "    # authoritative merge\n",
    "    for k in (\"title\",\"authors\",\"journal_name\",\"journal_abbrev\",\"volume\",\"issue\",\"pages\",\"doi\",\"year\",\"month\",\"conference_name\",\"publisher\",\"location\",\"edition\",\"isbn\",\"url\"):\n",
    "        bv = best.get(k)\n",
    "        if bv and normalize_text(ex.get(k)) != normalize_text(bv):\n",
    "            changes.append((k, ex.get(k), bv)); ex[k] = bv\n",
    "\n",
    "    # sentence case & authors list\n",
    "    if ex.get(\"title\"):\n",
    "        sc = sentence_case(ex[\"title\"])\n",
    "        if sc != ex[\"title\"]:\n",
    "            changes.append((\"title_sentence_case\", ex[\"title\"], sc)); ex[\"title\"] = sc\n",
    "    if isinstance(ex.get(\"authors\"), str):\n",
    "        al = authors_to_list(ex[\"authors\"])\n",
    "        if al != ex[\"authors\"]:\n",
    "            changes.append((\"authors_list\", ex[\"authors\"], al)); ex[\"authors\"] = al\n",
    "\n",
    "    # agent suggestions\n",
    "    for k, v in suggestions.items():\n",
    "        if normalize_text(ex.get(k)) != normalize_text(v):\n",
    "            changes.append((k, ex.get(k), v)); ex[k] = v\n",
    "\n",
    "    # normalize month\n",
    "    if ex.get(\"month\"):\n",
    "        newm = normalize_month_field(ex[\"month\"])\n",
    "        if newm != ex[\"month\"]:\n",
    "            changes.append((\"month_normalized\", ex[\"month\"], newm)); ex[\"month\"] = newm\n",
    "\n",
    "    state[\"extracted\"] = ex\n",
    "    state[\"corrections\"] = (state.get(\"corrections\") or []) + changes\n",
    "    state[\"attempts\"] = (state.get(\"attempts\") or 0) + 1\n",
    "    state[\"_made_changes_last_cycle\"] = bool(changes)\n",
    "\n",
    "    # loop/stagnation update (fingerprint on new ex)\n",
    "    sugg = state.get(\"suggestions\") or {}\n",
    "    best_now = state.get(\"best\") or {}\n",
    "    new_fp = fingerprint_state(ex, best_now, sugg)\n",
    "    hist = state.get(\"_fp_history\") or set()\n",
    "    if new_fp in hist:\n",
    "        state[\"_loop_detected\"] = True\n",
    "    else:\n",
    "        hist.add(new_fp)\n",
    "        state[\"_fp_history\"] = hist\n",
    "        state[\"_loop_detected\"] = False\n",
    "    state[\"_fp\"] = new_fp\n",
    "    return state\n",
    "\n",
    "async def node_llm_correct_async(state: PipelineState) -> PipelineState:\n",
    "    ref = state[\"reference\"]; ex = state[\"extracted\"]; ver = state.get(\"verification\") or {}\n",
    "    llm: LLMAdapter = state[\"_llm\"]\n",
    "    prompt = (\n",
    "        \"You are an IEEE reference corrector. Given raw reference, current JSON, and verification booleans, \"\n",
    "        \"return STRICT JSON correcting the fields. Keys among: title, authors (list), journal_name, journal_abbrev, \"\n",
    "        \"conference_name, volume, issue, pages, year, month, doi, publisher, location, edition, isbn, url. \"\n",
    "        \"Omit unknown keys. JSON ONLY.\\n\\n\"\n",
    "        f\"Raw: {ref}\\n\\nCurrent: {json.dumps(ex, ensure_ascii=False)}\\n\\nVerification: {json.dumps(ver)}\"\n",
    "    )\n",
    "    patch = await llm.json(prompt)\n",
    "    if patch:\n",
    "        if isinstance(patch.get(\"authors\"), str):\n",
    "            patch[\"authors\"] = authors_to_list(patch[\"authors\"])\n",
    "        if patch.get(\"month\"):\n",
    "            patch[\"month\"] = normalize_month_field(patch[\"month\"])\n",
    "        try:\n",
    "            patch = ExtractedModel(**patch).dict(exclude_none=True)\n",
    "        except ValidationError as ve:\n",
    "            _jlog(event=\"llm_patch_validation_error\", errors=str(ve))\n",
    "            patch = {}\n",
    "        ex2 = dict(ex); changes = []\n",
    "        for k, v in patch.items():\n",
    "            if normalize_text(ex2.get(k)) != normalize_text(v):\n",
    "                changes.append((k, ex2.get(k), v)); ex2[k] = v\n",
    "        state[\"extracted\"] = ex2\n",
    "        state[\"corrections\"] = (state.get(\"corrections\") or []) + changes\n",
    "        state[\"_made_changes_last_cycle\"] = state.get(\"_made_changes_last_cycle\", False) or bool(changes)\n",
    "        best = state.get(\"best\") or {}\n",
    "        sugg = state.get(\"suggestions\") or {}\n",
    "        state[\"_fp\"] = fingerprint_state(ex2, best, sugg)\n",
    "    return state\n",
    "\n",
    "def node_enrich_from_best(state: PipelineState) -> PipelineState:\n",
    "    ex = dict(state[\"extracted\"]); be = state.get(\"best\") or {}\n",
    "    for k in (\"journal_abbrev\",\"journal_name\",\"volume\",\"issue\",\"pages\",\"year\",\"month\",\"doi\",\"conference_name\",\"publisher\",\"location\",\"edition\",\"isbn\",\"url\",\"title\",\"authors\"):\n",
    "        if not ex.get(k) and be.get(k):\n",
    "            ex[k] = be.get(k)\n",
    "    if ex.get(\"month\"):\n",
    "        ex[\"month\"] = normalize_month_field(ex[\"month\"])\n",
    "    state[\"extracted\"] = ex\n",
    "    return state\n",
    "\n",
    "def node_format_reference(state: PipelineState) -> PipelineState:\n",
    "    ex = state[\"extracted\"]; rtype = (state[\"type\"] or \"other\").lower()\n",
    "    A = authors_to_list(ex.get(\"authors\") or [])\n",
    "    A_fmt = format_authors_ieee_list(A)\n",
    "    title_raw = ex.get(\"title\") or \"\"\n",
    "    title = sentence_case(title_raw)\n",
    "    journal = ex.get(\"journal_abbrev\") or ex.get(\"journal_name\") or \"\"\n",
    "    vol = normalize_text(ex.get(\"volume\") or \"\")\n",
    "    issue = normalize_text(ex.get(\"issue\") or \"\")\n",
    "    pages_raw = normalize_text(ex.get(\"pages\") or \"\")\n",
    "    pages_norm, is_eloc = normalize_pages(pages_raw)\n",
    "    if \"-\" in pages_norm: pages_norm = pages_norm.replace(\"-\", \"–\")\n",
    "    year = normalize_text(ex.get(\"year\") or \"\")\n",
    "    month = normalize_month_field(ex.get(\"month\") or \"\")\n",
    "    month_disp = MONTHS_NAME.get(month, month) if month else \"\"\n",
    "    doi_link = format_doi_link(ex.get(\"doi\") or \"\")\n",
    "    conf = normalize_text(ex.get(\"conference_name\") or \"\")\n",
    "    loc = normalize_text(ex.get(\"location\") or \"\")\n",
    "    pub = normalize_text(ex.get(\"publisher\") or \"\")\n",
    "    edition = normalize_text(ex.get(\"edition\") or \"\")\n",
    "    isbn = normalize_text(ex.get(\"isbn\") or \"\")\n",
    "\n",
    "    parts: List[str] = []\n",
    "    if A_fmt: parts.append(A_fmt)\n",
    "\n",
    "    include_quoted_title = rtype not in (\"book\",)\n",
    "    if include_quoted_title and title:\n",
    "        parts.append(f\"\\\"{title}\\\"\")\n",
    "\n",
    "    if rtype in (\"journal article\",\"journal\"):\n",
    "        if journal: parts.append(f\"*{journal}*\")\n",
    "        if vol: parts.append(f\"vol. {vol}\")\n",
    "        if issue: parts.append(f\"no. {issue}\")\n",
    "        if pages_norm:\n",
    "            parts.append(f\"Art. no. {pages_norm}\" if is_eloc else f\"pp. {pages_norm}\")\n",
    "        date = \" \".join([m for m in [month_disp, year] if m]).strip()\n",
    "        if date: parts.append(date)\n",
    "        if doi_link: parts.append(doi_link)\n",
    "\n",
    "    elif rtype == \"conference paper\":\n",
    "        venue = conf or journal or \"Proceedings\"\n",
    "        if venue: parts.append(f\"in *{venue}*\")\n",
    "        if loc: parts.append(loc)\n",
    "        if pages_norm:\n",
    "            parts.append(f\"pp. {pages_norm}\")\n",
    "        date = \" \".join([m for m in [month_disp, year] if m]).strip()\n",
    "        if date: parts.append(date)\n",
    "        if doi_link: parts.append(doi_link)\n",
    "\n",
    "    elif rtype == \"preprint\":\n",
    "        parts.append(\"preprint\")\n",
    "        if journal and \"arxiv\" in journal.lower():\n",
    "            parts.append(journal)\n",
    "        date = \" \".join([m for m in [month_disp, year] if m]).strip()\n",
    "        if date: parts.append(date)\n",
    "        if doi_link: parts.append(doi_link)\n",
    "\n",
    "    elif rtype == \"book\":\n",
    "        if title: parts.append(f\"*{title}*\")\n",
    "        if edition: parts.append(f\"{edition} ed.\")\n",
    "        imprint = f\"{loc}: {pub}\" if (loc and pub) else (loc or pub)\n",
    "        if imprint: parts.append(imprint)\n",
    "        if year: parts.append(year)\n",
    "        if isbn: parts.append(f\"ISBN: {isbn}\")\n",
    "        if doi_link: parts.append(doi_link)\n",
    "\n",
    "    elif rtype in (\"book chapter\",\"chapter\"):\n",
    "        book_title = normalize_text(ex.get(\"book_title\") or conf or journal)\n",
    "        if book_title: parts.append(f\"in *{book_title}*\")\n",
    "        if pages_norm: parts.append(f\"pp. {pages_norm}\")\n",
    "        if pub: parts.append(pub)\n",
    "        date = \" \".join([m for m in [month_disp, year] if m]).strip()\n",
    "        if date: parts.append(date)\n",
    "        if doi_link: parts.append(doi_link)\n",
    "\n",
    "    else:\n",
    "        venue = journal or conf or pub\n",
    "        if venue: parts.append(venue)\n",
    "        date = \" \".join([m for m in [month_disp, year] if m]).strip()\n",
    "        if date: parts.append(date)\n",
    "        if vol: parts.append(f\"vol. {vol}\")\n",
    "        if issue: parts.append(f\"no. {issue}\")\n",
    "        if pages_norm: parts.append(f\"pp. {pages_norm}\")\n",
    "        if doi_link: parts.append(doi_link)\n",
    "\n",
    "    state[\"formatted\"] = (\", \".join([p for p in parts if p]) + \".\").replace(\" ,\", \",\")\n",
    "    return state\n",
    "\n",
    "def to_csl_json(ex: Dict[str, Any], rtype: str) -> Dict[str, Any]:\n",
    "    typemap = {\n",
    "        \"journal article\": \"article-journal\",\n",
    "        \"conference paper\": \"paper-conference\",\n",
    "        \"book\": \"book\",\n",
    "        \"book chapter\": \"chapter\",\n",
    "        \"thesis\": \"thesis\",\n",
    "        \"technical report\": \"report\",\n",
    "        \"dataset\": \"dataset\",\n",
    "        \"standard\": \"standard\",\n",
    "        \"software\": \"software\",\n",
    "        \"preprint\": \"article\",\n",
    "    }\n",
    "    t = typemap.get(rtype, \"article\")\n",
    "\n",
    "    authors_list = authors_to_list(ex.get(\"authors\"))\n",
    "    authors = []\n",
    "    for a in authors_list:\n",
    "        parts = a.split()\n",
    "        family = parts[-1] if parts else a\n",
    "        given = \" \".join(parts[:-1]) if len(parts) > 1 else \"\"\n",
    "        authors.append({\"family\": safe_str(family), \"given\": safe_str(given)})\n",
    "\n",
    "    year_raw = ex.get(\"year\")\n",
    "    month_raw = normalize_month_field(ex.get(\"month\") or \"\")\n",
    "    issued = None\n",
    "    try:\n",
    "        y = int(year_raw) if safe_str(year_raw).isdigit() else None\n",
    "        if y is not None:\n",
    "            if month_raw and month_raw.isdigit():\n",
    "                issued = {\"date-parts\": [[y, int(month_raw)]]}\n",
    "            else:\n",
    "                issued = {\"date-parts\": [[y]]}\n",
    "    except Exception:\n",
    "        issued = None\n",
    "\n",
    "    doi_link = format_doi_link(ex.get(\"doi\") or \"\")\n",
    "    csl = {\n",
    "        \"type\": t,\n",
    "        \"title\": safe_str(ex.get(\"title\")),\n",
    "        \"author\": authors if authors else None,\n",
    "        \"container-title\": safe_str(ex.get(\"journal_name\") or ex.get(\"conference_name\")),\n",
    "        \"container-title-short\": safe_str(ex.get(\"journal_abbrev\")) or None,\n",
    "        \"volume\": safe_str(ex.get(\"volume\")),\n",
    "        \"issue\": safe_str(ex.get(\"issue\")),\n",
    "        \"page\": safe_str(ex.get(\"pages\")),\n",
    "        \"DOI\": safe_str(ex.get(\"doi\")),\n",
    "        \"URL\": doi_link or safe_str(ex.get(\"url\")),\n",
    "        \"publisher\": safe_str(ex.get(\"publisher\")),\n",
    "        \"issued\": issued,\n",
    "    }\n",
    "    return {k: v for k, v in csl.items() if v}\n",
    "\n",
    "def to_bibtex(ex: Dict[str, Any], rtype: str) -> str:\n",
    "    def bibtex_escape(s: str) -> str:\n",
    "        return (\n",
    "            s.replace(\"\\\\\", \"\\\\textbackslash{}\")\n",
    "             .replace(\"{\", \"\\\\{\").replace(\"}\", \"\\\\}\")\n",
    "             .replace(\"&\", \"\\\\&\").replace(\"%\", \"\\\\%\")\n",
    "             .replace(\"$\", \"\\\\$\").replace(\"#\", \"\\\\#\").replace(\"_\", \"\\\\_\")\n",
    "        )\n",
    "\n",
    "    authors_list = authors_to_list(ex.get(\"authors\"))\n",
    "    first_author_last = \"\"\n",
    "    if authors_list:\n",
    "        parts = authors_list[0].split()\n",
    "        first_author_last = parts[-1] if parts else authors_list[0]\n",
    "\n",
    "    year_str = safe_str(ex.get(\"year\"))\n",
    "    fa_key = re.sub(r\"[^A-Za-z0-9]+\", \"\", safe_str(first_author_last)) or \"ref\"\n",
    "    yr_key = re.sub(r\"[^0-9]+\", \"\", year_str)\n",
    "    if not yr_key:\n",
    "        basis = safe_str(ex.get(\"doi\")) or safe_str(ex.get(\"title\"))\n",
    "        h = hashlib.sha1(basis.encode(\"utf-8\", \"ignore\")).hexdigest()[:6] if basis else \"000000\"\n",
    "        yr_key = h\n",
    "    key = f\"{fa_key}{yr_key}\"\n",
    "\n",
    "    entry_type = {\n",
    "        \"journal article\": \"article\",\n",
    "        \"conference paper\": \"inproceedings\",\n",
    "        \"book\": \"book\",\n",
    "        \"book chapter\": \"incollection\",\n",
    "        \"thesis\": \"phdthesis\",\n",
    "        \"technical report\": \"techreport\",\n",
    "        \"dataset\": \"misc\",\n",
    "        \"standard\": \"misc\",\n",
    "        \"software\": \"misc\",\n",
    "        \"preprint\": \"misc\",\n",
    "    }.get(rtype, \"misc\")\n",
    "\n",
    "    A = \" and \".join(authors_list)\n",
    "    title = safe_str(ex.get(\"title\"))\n",
    "    journal = safe_str(ex.get(\"journal_name\"))\n",
    "    conf = safe_str(ex.get(\"conference_name\"))\n",
    "    volume = safe_str(ex.get(\"volume\"))\n",
    "    number = safe_str(ex.get(\"issue\"))\n",
    "    pages = safe_str(ex.get(\"pages\"))\n",
    "    year = safe_str(ex.get(\"year\"))\n",
    "    doi = safe_str(ex.get(\"doi\"))\n",
    "    publisher = safe_str(ex.get(\"publisher\"))\n",
    "    isbn = safe_str(ex.get(\"isbn\"))\n",
    "    url_or_doi = format_doi_link(doi) if doi else safe_str(ex.get(\"url\"))\n",
    "\n",
    "    fields: List[Tuple[str, str]] = []\n",
    "    if entry_type == \"article\":\n",
    "        fields += [(\"author\", A), (\"title\", title), (\"journal\", journal),\n",
    "                   (\"volume\", volume), (\"number\", number), (\"pages\", pages),\n",
    "                   (\"year\", year)]\n",
    "        if url_or_doi: fields.append((\"url\", url_or_doi))\n",
    "        if doi and not url_or_doi: fields.append((\"doi\", doi))\n",
    "    elif entry_type == \"inproceedings\":\n",
    "        fields += [(\"author\", A), (\"title\", title), (\"booktitle\", conf or journal),\n",
    "                   (\"pages\", pages), (\"year\", year)]\n",
    "        if url_or_doi: fields.append((\"url\", url_or_doi))\n",
    "    elif entry_type == \"book\":\n",
    "        fields += [(\"author\", A), (\"title\", title), (\"publisher\", publisher),\n",
    "                   (\"year\", year), (\"isbn\", isbn)]\n",
    "        if url_or_doi: fields.append((\"url\", url_or_doi))\n",
    "    elif entry_type == \"incollection\":\n",
    "        fields += [(\"author\", A), (\"title\", title), (\"booktitle\", conf or journal),\n",
    "                   (\"pages\", pages), (\"publisher\", publisher), (\"year\", year)]\n",
    "        if url_or_doi: fields.append((\"url\", url_or_doi))\n",
    "    elif entry_type == \"phdthesis\":\n",
    "        fields += [(\"author\", A), (\"title\", title), (\"school\", publisher or conf or journal),\n",
    "                   (\"year\", year)]\n",
    "        if url_or_doi: fields.append((\"url\", url_or_doi))\n",
    "    elif entry_type == \"techreport\":\n",
    "        fields += [(\"author\", A), (\"title\", title), (\"institution\", publisher or conf or journal),\n",
    "                   (\"year\", year)]\n",
    "        if url_or_doi: fields.append((\"url\", url_or_doi))\n",
    "    else:  # misc\n",
    "        fields += [(\"author\", A), (\"title\", title), (\"howpublished\", conf or journal or publisher),\n",
    "                   (\"year\", year)]\n",
    "        if url_or_doi: fields.append((\"url\", url_or_doi))\n",
    "\n",
    "    fields = [(k, bibtex_escape(v)) for k, v in fields if v]\n",
    "    body = \",\\n  \".join([f\"{k} = {{{v}}}\" for k, v in fields])\n",
    "    return f\"@{entry_type}{{{key},\\n  {body}\\n}}\"\n",
    "\n",
    "def node_build_exports(state: PipelineState) -> PipelineState:\n",
    "    ex = state[\"extracted\"]; rtype = (state[\"type\"] or \"other\").lower()\n",
    "    state[\"csl_json\"] = to_csl_json(ex, rtype)\n",
    "    state[\"bibtex\"] = to_bibtex(ex, rtype)\n",
    "    return state\n",
    "\n",
    "def node_build_report(state: PipelineState) -> PipelineState:\n",
    "    changes = state.get(\"corrections\") or []\n",
    "    ver = state.get(\"verification\") or {}\n",
    "    lines = []\n",
    "    if not changes:\n",
    "        lines.append(\"No corrections were necessary; reference matched authoritative sources.\")\n",
    "    else:\n",
    "        lines.append(\"Corrections (field: old → new):\")\n",
    "        for f, old, new in changes:\n",
    "            lines.append(f\"- {f}: '{old}' → '{new}'\")\n",
    "    failed = [k for k, v in ver.items() if not v]\n",
    "    if failed:\n",
    "        lines.append(\"Fields still needing attention: \" + \", \".join(sorted(failed)))\n",
    "    else:\n",
    "        lines.append(\"All verification checks passed after corrections.\")\n",
    "    state[\"report\"] = \"\\n\".join(lines)\n",
    "    return state\n",
    "\n",
    "# --- Cleanup node to close clients\n",
    "async def node_cleanup_async(state: PipelineState) -> PipelineState:\n",
    "    try:\n",
    "        if state.get(\"_http\") is not None:\n",
    "            await state[\"_http\"].aclose()\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        llm = state.get(\"_llm\")\n",
    "        if llm and llm.provider == \"ollama\" and getattr(llm, \"_client\", None) is not None:\n",
    "            await llm._client.aclose()\n",
    "    except Exception:\n",
    "        pass\n",
    "    return state\n",
    "\n",
    "# ============================ Graph Build & Routing ============================\n",
    "\n",
    "def should_exit(state: PipelineState) -> bool:\n",
    "    cfg = state.get(\"_cfg\") or CFG\n",
    "    if state.get(\"_loop_detected\"): return True\n",
    "    if (state.get(\"hops\") or 0) >= cfg.max_hops: return True\n",
    "    if (state.get(\"attempts\") or 0) >= cfg.max_correction_rounds: return True\n",
    "    if (state.get(\"_stagnation\") or 0) >= cfg.stagnation_patience: return True\n",
    "    if not state.get(\"_made_changes_last_cycle\") and (state.get(\"_stagnation\",0) >= 1):\n",
    "        return True\n",
    "    ver = state.get(\"verification\") or {}\n",
    "    return bool(ver) and all(ver.values())\n",
    "\n",
    "def build_graph(cfg: PipelineConfig = CFG) -> StateGraph:\n",
    "    g = StateGraph(PipelineState)\n",
    "\n",
    "    # nodes\n",
    "    g.add_node(\"InitRuntime\", _init_runtime)\n",
    "    g.add_node(\"DetectType\", node_detect_type_async)\n",
    "    g.add_node(\"ParseExtract\", node_parse_extract_async)\n",
    "    g.add_node(\"MultiSourceLookup\", node_multisource_lookup_async)\n",
    "    g.add_node(\"SelectBest\", node_select_best)\n",
    "    g.add_node(\"ReconcileType\", node_reconcile_type)\n",
    "    g.add_node(\"VerifyAgents\", node_verify_agents)\n",
    "\n",
    "    g.add_node(\"ApplyCorrections\", node_apply_corrections)\n",
    "    g.add_node(\"LLMCorrect\", node_llm_correct_async)\n",
    "    g.add_node(\"EnrichFromBest\", node_enrich_from_best)\n",
    "\n",
    "    g.add_node(\"FormatReference\", node_format_reference)\n",
    "    g.add_node(\"BuildExports\", node_build_exports)\n",
    "    g.add_node(\"BuildReport\", node_build_report)\n",
    "    g.add_node(\"Cleanup\", node_cleanup_async)\n",
    "\n",
    "    # linear backbone\n",
    "    g.add_edge(START, \"InitRuntime\")\n",
    "    g.add_edge(\"InitRuntime\", \"DetectType\")\n",
    "    g.add_edge(\"DetectType\", \"ParseExtract\")\n",
    "    g.add_edge(\"ParseExtract\", \"MultiSourceLookup\")\n",
    "    g.add_edge(\"MultiSourceLookup\", \"SelectBest\")\n",
    "    g.add_edge(\"SelectBest\", \"ReconcileType\")\n",
    "    g.add_edge(\"ReconcileType\", \"VerifyAgents\")\n",
    "\n",
    "    # conditional: if done -> format; else repair loop\n",
    "    def route_after_verify(state: PipelineState) -> str:\n",
    "        decision = \"FormatReference\" if should_exit(state) else \"ApplyCorrections\"\n",
    "        _jlog(event=\"route_after_verify\", decision=decision, ver_score=state.get(\"_ver_score\"))\n",
    "        return decision\n",
    "    g.add_conditional_edges(\"VerifyAgents\", route_after_verify, {\n",
    "        \"FormatReference\": \"FormatReference\",\n",
    "        \"ApplyCorrections\": \"ApplyCorrections\",\n",
    "    })\n",
    "\n",
    "    # repair path (bounded by guards)\n",
    "    g.add_edge(\"ApplyCorrections\", \"LLMCorrect\")\n",
    "    g.add_edge(\"LLMCorrect\", \"EnrichFromBest\")\n",
    "    g.add_edge(\"EnrichFromBest\", \"MultiSourceLookup\")\n",
    "\n",
    "    # terminal leg\n",
    "    g.add_edge(\"FormatReference\", \"BuildExports\")\n",
    "    g.add_edge(\"BuildExports\", \"BuildReport\")\n",
    "    g.add_edge(\"BuildReport\", \"Cleanup\")\n",
    "    g.add_edge(\"Cleanup\", END)\n",
    "\n",
    "    return g\n",
    "\n",
    "# ============================ Mermaid Rendering ============================\n",
    "\n",
    "MERMAID_DAG = r'''flowchart TD\n",
    "A[Init Runtime] --> B[Detect Type (Heuristics + LLM)]\n",
    "B --> C[Parse & Extract (LLM-first)]\n",
    "C --> D[Fetch Candidates (Crossref/OpenAlex/S2/PubMed/arXiv)]\n",
    "D --> E[Select Best (Consensus Scoring)]\n",
    "E --> T[Reconcile Type]\n",
    "T --> F[Verification Agents (Threaded) + Progress Metrics]\n",
    "F -->|repair| G[Apply Corrections (Authority + Agents)]\n",
    "G --> I[LLM Correction (JSON-only)]\n",
    "I --> X[Enrich From Best]\n",
    "X --> D2[Re-Fetch Candidates]\n",
    "D2 --> E2[Re-Select Best]\n",
    "E2 --> T2[Reconcile Type]\n",
    "T2 --> F2[Re-Verify + loop/stagnation guards]\n",
    "F -->|exit| H[Format IEEE]\n",
    "H --> J[Build CSL-JSON & BibTeX]\n",
    "J --> R[Human Report]\n",
    "style H fill:#e0f7fa,stroke:#006064,stroke-width:1px\n",
    "style R fill:#f1f8e9,stroke:#33691e,stroke-width:1px\n",
    "style D fill:#fff3e0,stroke:#e65100,stroke-width:1px\n",
    "style F fill:#ede7f6,stroke:#4527a0,stroke-width:1px\n",
    "style T fill:#e8f5e9,stroke:#2e7d32,stroke-width:1px\n",
    "'''\n",
    "\n",
    "def show_mermaid_inline(mermaid_code: str) -> None:\n",
    "    html = f\"\"\"\n",
    "    <div class=\"mermaid\">\n",
    "    {mermaid_code}\n",
    "    </div>\n",
    "    <script>\n",
    "      (function() {{\n",
    "        function init() {{\n",
    "          mermaid.initialize({{startOnLoad:true}});\n",
    "        }}\n",
    "        if (!window.mermaid) {{\n",
    "          var s = document.createElement('script');\n",
    "          s.src = 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js';\n",
    "          s.onload = init;\n",
    "          document.head.appendChild(s);\n",
    "        }} else {{\n",
    "          init();\n",
    "        }}\n",
    "      }})();\n",
    "    </script>\n",
    "    \"\"\"\n",
    "    display(HTML(html))\n",
    "\n",
    "def show_mermaid_kroki(mermaid_code: str) -> None:\n",
    "    display(Markdown(f\"```mermaid\\n{mermaid_code}\\n```\"))\n",
    "    if httpx is None:\n",
    "        return\n",
    "    try:\n",
    "        r = httpx.post(\"https://kroki.io/mermaid/svg\", content=mermaid_code.encode(\"utf-8\"), timeout=10.0,\n",
    "                       headers={\"Content-Type\": \"text/plain\",\"User-Agent\":DEFAULT_UA})\n",
    "        _jlog(event=\"http_request\", url=\"https://kroki.io/mermaid/svg\", code=r.status_code)\n",
    "        if r.status_code == 200:\n",
    "            display(SVG(r.content))\n",
    "        else:\n",
    "            if r.status_code == 400:\n",
    "                print(\"Kroki 400: Mermaid syntax or styling may be invalid. Try removing `style` lines or simplify labels.\")\n",
    "            else:\n",
    "                print(f\"Kroki error: {r.status_code}: {r.text[:200]}\")\n",
    "    except Exception as e:\n",
    "        _jlog(event=\"kroki_render_failed\", error=str(e))\n",
    "\n",
    "# ============================ Build/compile ============================\n",
    "\n",
    "graph = build_graph(CFG)\n",
    "compiled = graph.compile()\n",
    "\n",
    "# Show diagram (respect env)\n",
    "if os.environ.get(\"NO_EXTERNAL_JS\",\"0\") == \"1\":\n",
    "    show_mermaid_kroki(MERMAID_DAG)\n",
    "else:\n",
    "    show_mermaid_inline(MERMAID_DAG)\n",
    "    show_mermaid_kroki(MERMAID_DAG)\n",
    "\n",
    "# ============================ Runner & Examples ============================\n",
    "\n",
    "examples = [\n",
    "    'F.-J. Lin, P.-H. Shen, S.-L. Yang, and P. H. Chou, “Recurrent radial basis function network-based fuzzy neural network control for permanent-magnet linear synchronous motor servo drive,” IEEE Trans. on Magnetics, vol. 42, no. 11, Nov. 2006.',\n",
    "    'P. S. Sastry, G. Santharam, and K. P. Unnikrishnan, “Memory neuron networks for identification and control of dynamical systems,” IEEE Trans. Neural Netw., vol. 5, no. 2, pp. 306–319, May 1994, doi: 10.1109/72.279193.',\n",
    "    'K. C. Apaza and J. M. López, “The non-linear relationship between carbon dioxide emissions, financial development and energy consumption in developing European and Central Asian economies,” Environ. Sci. Pollut. Res. Int., vol. 28, no. 44, pp. 63330–63345, Jul. 2021, doi: 10.1007/s11356-021-15225-2.',\n",
    "    'A. Vaswani et al., \"Attention Is All You Need\", in NeurIPS, 2017.'\n",
    "]\n",
    "\n",
    "async def run_one(reference: str, recursion_limit: Optional[int] = None) -> Dict[str, Any]:\n",
    "    state: PipelineState = {\"reference\": reference}\n",
    "    out = await compiled.ainvoke(state, config={\"recursion_limit\": recursion_limit or CFG.recursion_limit})\n",
    "    return out\n",
    "\n",
    "async def run_examples():\n",
    "    for ref in examples:\n",
    "        result = await run_one(ref)\n",
    "        print(\"\\n=== Result ===\")\n",
    "        print(\"Resolved Type:\", result.get(\"type\"))\n",
    "        print(\"Formatted:\", result.get(\"formatted\"))\n",
    "        print(\"Verification OK:\", all((result.get(\"verification\") or {}).values()) if result.get(\"verification\") else False)\n",
    "        print(\"Report:\\n\", result.get(\"report\"))\n",
    "        print(\"CSL-JSON:\", json.dumps(result.get(\"csl_json\"), indent=2, ensure_ascii=False))\n",
    "        print(\"BibTeX:\\n\", result.get(\"bibtex\"))\n",
    "\n",
    "ENV_SAMPLE = \"\"\"\n",
    "# One of these providers is enough:\n",
    "OPENAI_API_KEY=sk-...\n",
    "OPENAI_MODEL=gpt-4o-mini\n",
    "\n",
    "# OR Azure OpenAI:\n",
    "AZURE_OPENAI_API_KEY=...\n",
    "AZURE_OPENAI_ENDPOINT=https://<your-resource>.openai.azure.com\n",
    "AZURE_OPENAI_DEPLOYMENT=gpt-4o-base\n",
    "OPENAI_API_VERSION=2024-06-01\n",
    "\n",
    "# OR Anthropic:\n",
    "ANTHROPIC_API_KEY=...\n",
    "ANTHROPIC_MODEL=claude-3-5-sonnet-20240620\n",
    "# Agentic IEEE Reference Pipeline (LangGraph) — Single Jupyter Cell (Production Grade, Updated)\n",
    "# - Robust LLM adapter (OpenAI/Azure/Anthropic/Ollama), JSON-only w/ deterministic params\n",
    "# - Async concurrent lookups + retries/backoff: Crossref/OpenAlex/Semantic Scholar/PubMed/arXiv\n",
    "# - Better matching (top-N fetch + trust guard), author formatting, e-locator handling, DOI links\n",
    "# - Verification agents + repair loop with improved circuit breakers\n",
    "# - Mermaid graph rendering via inline JS (no extension) + Kroki SVG fallback (guarded)\n",
    "# - Exports: IEEE string, CSL-JSON (w/ abbrev, URL), BibTeX (w/ url)\n",
    "# - Structured logs (_jlog), polite User-Agent, optional API keys, pydantic validation for LLM JSON\n",
    "\n",
    "import os, re, json, asyncio, logging, textwrap, hashlib, time, sys\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "# ---------- Optional deps ----------\n",
    "try:\n",
    "    from dotenv import load_dotenv; load_dotenv()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    import httpx\n",
    "except Exception:\n",
    "    httpx = None\n",
    "\n",
    "try:\n",
    "    from cachetools import TTLCache\n",
    "    CACHE_AVAILABLE = True\n",
    "except Exception:\n",
    "    TTLCache = None\n",
    "    CACHE_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    from rapidfuzz import fuzz\n",
    "    RF_AVAILABLE = True\n",
    "except Exception:\n",
    "    fuzz = None\n",
    "    RF_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    from pydantic import BaseModel, ValidationError\n",
    "except Exception:\n",
    "    class BaseModel:  # fallback no-op\n",
    "        def __init__(self, **kw): pass\n",
    "        def dict(self, **kw): return {}\n",
    "    class ValidationError(Exception): ...\n",
    "    BaseModel = BaseModel\n",
    "    ValidationError = ValidationError\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from IPython.display import display, Markdown, Image, SVG, HTML\n",
    "\n",
    "# LangGraph\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# ============================ Configuration & Logging ============================\n",
    "\n",
    "@dataclass\n",
    "class PipelineConfig:\n",
    "    timeout_s: float = float(os.getenv(\"IEEE_REF_TIMEOUT\", \"12\"))\n",
    "    concurrency: int = int(os.getenv(\"IEEE_REF_CONCURRENCY\", \"8\"))\n",
    "    cache_ttl_s: int = int(os.getenv(\"IEEE_REF_CACHE_TTL\", \"3600\"))\n",
    "    max_correction_rounds: int = int(os.getenv(\"IEEE_REF_MAX_CORR\", \"3\"))\n",
    "    max_hops: int = int(os.getenv(\"IEEE_REF_MAX_HOPS\", \"12\"))\n",
    "    stagnation_patience: int = int(os.getenv(\"IEEE_REF_STAGNATION\", \"2\"))\n",
    "    llm_provider: str = os.getenv(\"IEEE_REF_LLM\", \"auto\")  # auto|openai|azure|anthropic|ollama|dummy\n",
    "    openai_model: str = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n",
    "    ollama_model: str = os.getenv(\"OLLAMA_MODEL\", \"llama3.2\")\n",
    "    ollama_base: str = os.getenv(\"OLLAMA_BASE_URL\", os.getenv(\"OLLAMA_HOST\", \"http://localhost:11434\"))\n",
    "    agent_threads: int = int(os.getenv(\"IEEE_REF_AGENT_THREADS\", \"6\"))\n",
    "    recursion_limit: int = int(os.getenv(\"IEEE_REF_RECURSION_LIMIT\", \"60\"))\n",
    "\n",
    "LOG_LEVEL = os.getenv(\"IEEE_REF_LOG_LEVEL\", \"INFO\").upper()\n",
    "logging.basicConfig(level=getattr(logging, LOG_LEVEL, logging.INFO), format=\"%(message)s\")\n",
    "logger = logging.getLogger(\"ieee-ref-langgraph\")\n",
    "import json as _json\n",
    "def _jlog(**kw):\n",
    "    try:\n",
    "        print(_json.dumps(kw, ensure_ascii=False))\n",
    "    except Exception:\n",
    "        print(str(kw))\n",
    "\n",
    "CFG = PipelineConfig()\n",
    "DEFAULT_UA = \"ieee-ref-agent/1.0 (mailto:you@example.com)\"\n",
    "\n",
    "# ============================ Utility Functions ============================\n",
    "\n",
    "def safe_json_load(s: Any) -> Optional[Dict[str, Any]]:\n",
    "    if s is None:\n",
    "        return None\n",
    "    if isinstance(s, dict):\n",
    "        return s\n",
    "    try:\n",
    "        sx = s.decode(\"utf-8\", \"ignore\") if isinstance(s, (bytes, bytearray)) else str(s)\n",
    "    except Exception:\n",
    "        sx = str(s)\n",
    "    sx = sx.strip()\n",
    "    try:\n",
    "        if sx.startswith(\"{\"):\n",
    "            return json.loads(sx)\n",
    "    except Exception:\n",
    "        pass\n",
    "    # brace-balanced extraction\n",
    "    i, n = 0, len(sx)\n",
    "    while i < n and sx[i] != \"{\": i += 1\n",
    "    if i >= n: return None\n",
    "    stack = 0; in_str = False; esc = False; start = None\n",
    "    for j in range(i, n):\n",
    "        ch = sx[j]\n",
    "        if in_str:\n",
    "            if esc: esc = False\n",
    "            elif ch == \"\\\\\": esc = True\n",
    "            elif ch == '\"': in_str = False\n",
    "        else:\n",
    "            if ch == '\"': in_str = True\n",
    "            elif ch == \"{\":\n",
    "                if stack == 0: start = j\n",
    "                stack += 1\n",
    "            elif ch == \"}\":\n",
    "                stack -= 1\n",
    "                if stack == 0 and start is not None:\n",
    "                    candidate = sx[start:j+1]\n",
    "                    try: return json.loads(candidate)\n",
    "                    except Exception: start = None\n",
    "    return None\n",
    "\n",
    "def normalize_text(x: Any) -> str:\n",
    "    if x is None: return \"\"\n",
    "    s = str(x).strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def norm_for_compare(x: Any) -> str:\n",
    "    s = normalize_text(x).lower()\n",
    "    s = re.sub(r\"[^\\w\\s]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def token_similarity(a: str, b: str) -> float:\n",
    "    a = norm_for_compare(a); b = norm_for_compare(b)\n",
    "    if not a or not b: return 0.0\n",
    "    if RF_AVAILABLE and fuzz is not None:\n",
    "        return fuzz.token_sort_ratio(a, b) / 100.0\n",
    "    sa, sb = set(a.split()), set(b.split())\n",
    "    inter = sa & sb\n",
    "    union = sa | sb\n",
    "    return len(inter) / max(1, len(union))\n",
    "\n",
    "def authors_to_list(a: Any) -> List[str]:\n",
    "    if not a: return []\n",
    "    if isinstance(a, list):\n",
    "        return [normalize_text(x) for x in a if normalize_text(x)]\n",
    "    parts = re.split(r\",\\s*|\\s+&\\s+| and \", str(a))\n",
    "    return [normalize_text(p) for p in parts if normalize_text(p)]\n",
    "\n",
    "SUFFIXES = {\"jr\", \"jr.\", \"sr\", \"sr.\", \"ii\", \"iii\", \"iv\", \"v\"}\n",
    "\n",
    "def _initials(given: str) -> List[str]:\n",
    "    parts = re.split(r\"\\s+\", given.strip())\n",
    "    out=[]\n",
    "    for p in parts:\n",
    "        if not p: continue\n",
    "        hy = p.split(\"-\")\n",
    "        if len(hy)>1:\n",
    "            out.append(\"-\".join([h[0].upper()+\".\" for h in hy if h]))\n",
    "        elif re.match(r\"^[A-Za-z]\\.$\", p):\n",
    "            out.append(p.upper())\n",
    "        elif p.lower().rstrip(\".\") in SUFFIXES:\n",
    "            out.append(p.capitalize().rstrip(\".\")+\".\")\n",
    "        else:\n",
    "            out.append(p[0].upper()+\".\")\n",
    "    return out\n",
    "\n",
    "def format_author_ieee(name: str) -> str:\n",
    "    n = normalize_text(name)\n",
    "    if not n: return \"\"\n",
    "    if \",\" in n:\n",
    "        last, given = [p.strip() for p in n.split(\",\", 1)]\n",
    "    else:\n",
    "        toks = n.split()\n",
    "        if len(toks) == 1: return toks[0]\n",
    "        last = toks[-1]; given = \" \".join(toks[:-1])\n",
    "    init = \" \".join(_initials(given))\n",
    "    last_tokens = last.split()\n",
    "    if last_tokens and last_tokens[-1].lower().rstrip(\".\") in SUFFIXES:\n",
    "        suf = last_tokens[-1].capitalize().rstrip(\".\")+\".\"\n",
    "        last = \" \".join(last_tokens[:-1])\n",
    "        return f\"{init} {last}, {suf}\".strip(\", \")\n",
    "    return f\"{init} {last}\".strip()\n",
    "\n",
    "def format_authors_ieee_list(auths: List[str]) -> str:\n",
    "    items = [format_author_ieee(a) for a in auths if a]\n",
    "    if not items: return \"\"\n",
    "    if len(items) <= 6:\n",
    "        return \", \".join(items[:-1]) + (\", and \" if len(items) > 1 else \"\") + items[-1] if len(items) > 1 else items[0]\n",
    "    return \", \".join(items[:6]) + \", et al.\"\n",
    "\n",
    "def sentence_case(title: str) -> str:\n",
    "    t = normalize_text(title)\n",
    "    if not t: return \"\"\n",
    "    if t.isupper(): t = t.lower()\n",
    "    tokens = t.split(); out = []\n",
    "    for i, tok in enumerate(tokens):\n",
    "        if tok.isupper() and len(tok) > 1:\n",
    "            out.append(tok)\n",
    "        else:\n",
    "            out.append(tok[:1].upper() + tok[1:].lower() if i == 0 else tok.lower())\n",
    "    res = \" \".join(out)\n",
    "    res = re.sub(r\"\\bieee\\b\", \"IEEE\", res, flags=re.I)\n",
    "    return res\n",
    "\n",
    "def heuristic_abbrev(fullname: str) -> str:\n",
    "    fullname = normalize_text(fullname)\n",
    "    if not fullname: return \"\"\n",
    "    tokens = [t for t in re.split(r\"[\\s,]+\", fullname) if t.lower() not in {\"on\",\"of\",\"and\",\"the\",\"in\",\"for\",\"to\"}]\n",
    "    out = []\n",
    "    for t in tokens[:8]:\n",
    "        if len(t) <= 4 and t.isupper(): out.append(t)\n",
    "        elif len(t) <= 3: out.append(t.capitalize() + \".\")\n",
    "        else: out.append(t[:4].capitalize() + \".\")\n",
    "    return \" \".join(out)\n",
    "\n",
    "def ensure_doi_prefix(doi: str) -> str:\n",
    "    # kept for compatibility; not used anymore\n",
    "    d = normalize_text(doi)\n",
    "    if not d: return \"\"\n",
    "    return d if d.lower().startswith(\"doi:\") else f\"doi:{d}\"\n",
    "\n",
    "def format_doi_link(doi: str) -> str:\n",
    "    d = normalize_text(doi).lower().replace(\"doi:\",\"\").strip()\n",
    "    return f\"https://doi.org/{d}\" if d else \"\"\n",
    "\n",
    "def normalize_pages(p: str) -> Tuple[str, bool]:\n",
    "    p = normalize_text(p).replace(\"—\",\"-\").replace(\"–\",\"-\")\n",
    "    if not p: return \"\", False\n",
    "    # Treat single token or alphanumeric token without dash as eLocator/article number\n",
    "    if (\"-\" not in p) and re.fullmatch(r\"[A-Za-z]?\\d+[A-Za-z]?\", p):\n",
    "        return p, True\n",
    "    return p, False\n",
    "\n",
    "MONTHS_NAME = {\n",
    "    \"1\":\"Jan\",\"2\":\"Feb\",\"3\":\"Mar\",\"4\":\"Apr\",\"5\":\"May\",\"6\":\"Jun\",\n",
    "    \"7\":\"Jul\",\"8\":\"Aug\",\"9\":\"Sep\",\"10\":\"Oct\",\"11\":\"Nov\",\"12\":\"Dec\",\n",
    "}\n",
    "def normalize_month_field(m: Any) -> str:\n",
    "    s = normalize_text(m)\n",
    "    if not s: return \"\"\n",
    "    m_map = {\"jan\":\"1\",\"feb\":\"2\",\"mar\":\"3\",\"apr\":\"4\",\"may\":\"5\",\"jun\":\"6\",\"jul\":\"7\",\"aug\":\"8\",\"sep\":\"9\",\"sept\":\"9\",\"oct\":\"10\",\"nov\":\"11\",\"dec\":\"12\"}\n",
    "    sl = s.strip(\". \").lower()\n",
    "    if sl in m_map: return m_map[sl]\n",
    "    if re.fullmatch(r\"0?[1-9]|1[0-2]\", sl): return str(int(sl))\n",
    "    return s\n",
    "\n",
    "def fingerprint_state(ex: Dict[str, Any], best: Dict[str, Any], sugg: Dict[str, Any]) -> str:\n",
    "    payload = json.dumps({\"ex\": ex, \"best\": best, \"sugg\": sugg}, sort_keys=True, ensure_ascii=False)\n",
    "    return hashlib.sha256(payload.encode(\"utf-8\", \"ignore\")).hexdigest()\n",
    "\n",
    "def safe_str(v: Any) -> str:\n",
    "    try:\n",
    "        if v is None:\n",
    "            return \"\"\n",
    "        return str(v).strip()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "# ============================ Validation Model ============================\n",
    "\n",
    "class ExtractedModel(BaseModel):\n",
    "    title: Optional[str] = None\n",
    "    authors: Optional[List[str]] = None\n",
    "    journal_name: Optional[str] = None\n",
    "    journal_abbrev: Optional[str] = None\n",
    "    conference_name: Optional[str] = None\n",
    "    volume: Optional[str] = None\n",
    "    issue: Optional[str] = None\n",
    "    pages: Optional[str] = None\n",
    "    year: Optional[str] = None\n",
    "    month: Optional[str] = None\n",
    "    doi: Optional[str] = None\n",
    "    publisher: Optional[str] = None\n",
    "    location: Optional[str] = None\n",
    "    edition: Optional[str] = None\n",
    "    isbn: Optional[str] = None\n",
    "    url: Optional[str] = None\n",
    "    arxiv_id: Optional[str] = None\n",
    "\n",
    "# ============================ LLM Adapter ============================\n",
    "\n",
    "class LLMAdapter:\n",
    "    \"\"\"LLM JSON-mode adapter supporting OpenAI, Azure OpenAI, Anthropic, Ollama; falls back to dummy.\"\"\"\n",
    "    def __init__(self, cfg: PipelineConfig):\n",
    "        self.cfg = cfg\n",
    "        self.provider = self._auto_provider(cfg.llm_provider)\n",
    "        self._client = None\n",
    "        self._init_client()\n",
    "        _jlog(event=\"llm_provider_selected\", provider=self.provider)\n",
    "\n",
    "    def _auto_provider(self, p: str) -> str:\n",
    "        if p != \"auto\":\n",
    "            return p\n",
    "        if os.getenv(\"OPENAI_API_KEY\"): return \"openai\"\n",
    "        if os.getenv(\"AZURE_OPENAI_API_KEY\"): return \"azure\"\n",
    "        if os.getenv(\"ANTHROPIC_API_KEY\"): return \"anthropic\"\n",
    "        if os.getenv(\"OLLAMA_BASE_URL\") or os.getenv(\"OLLAMA_HOST\"): return \"ollama\"\n",
    "        return \"dummy\"\n",
    "\n",
    "    def _init_client(self):\n",
    "        prov = self.provider\n",
    "        try:\n",
    "            if prov == \"openai\":\n",
    "                from openai import OpenAI\n",
    "                base = os.getenv(\"OPENAI_API_BASE\")\n",
    "                self._client = OpenAI(base_url=base) if base else OpenAI()\n",
    "            elif prov == \"azure\":\n",
    "                from openai import AzureOpenAI\n",
    "                ep = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "                ver = os.getenv(\"OPENAI_API_VERSION\", \"2024-06-01\")\n",
    "                if not ep:\n",
    "                    raise RuntimeError(\"AZURE_OPENAI_ENDPOINT is not set\")\n",
    "                self._client = AzureOpenAI(azure_endpoint=ep, api_version=ver)\n",
    "            elif prov == \"anthropic\":\n",
    "                import anthropic\n",
    "                self._client = anthropic.AsyncAnthropic()\n",
    "            elif prov == \"ollama\" and httpx is not None:\n",
    "                base = os.getenv(\"OLLAMA_BASE_URL\") or os.getenv(\"OLLAMA_HOST\") or self.cfg.ollama_base\n",
    "                self._client = httpx.AsyncClient(base_url=base, timeout=self.cfg.timeout_s)\n",
    "            else:\n",
    "                self._client = None\n",
    "        except Exception as e:\n",
    "            logger.warning(\"LLM init failed: %s\", e)\n",
    "            self._client = None\n",
    "            self.provider = \"dummy\"\n",
    "\n",
    "    async def _openai_json(self, prompt: str) -> str:\n",
    "        model = self.cfg.openai_model\n",
    "        resp = self._client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\":\"system\",\"content\":\"Return STRICT JSON only. No prose.\"},\n",
    "                      {\"role\":\"user\",\"content\":prompt}],\n",
    "            temperature=0.1,\n",
    "            top_p=0.1,\n",
    "            response_format={\"type\":\"json_object\"},\n",
    "        )\n",
    "        return resp.choices[0].message.content\n",
    "\n",
    "    async def _azure_json(self, prompt: str) -> str:\n",
    "        deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\") or self.cfg.openai_model\n",
    "        resp = self._client.chat.completions.create(\n",
    "            model=deployment,\n",
    "            messages=[{\"role\":\"system\",\"content\":\"Return STRICT JSON only. No prose.\"},\n",
    "                      {\"role\":\"user\",\"content\":prompt}],\n",
    "            temperature=0.1,\n",
    "            top_p=0.1,\n",
    "            response_format={\"type\":\"json_object\"},\n",
    "        )\n",
    "        return resp.choices[0].message.content\n",
    "\n",
    "    async def _anthropic_json(self, prompt: str) -> str:\n",
    "        msg = await self._client.messages.create(\n",
    "            model=os.getenv(\"ANTHROPIC_MODEL\",\"claude-3-5-sonnet-20240620\"),\n",
    "            system=\"Return STRICT JSON only. No prose.\",\n",
    "            max_tokens=1024, temperature=0.1,\n",
    "            messages=[{\"role\":\"user\",\"content\":prompt}],\n",
    "        )\n",
    "        texts = []\n",
    "        for c in msg.content:\n",
    "            if getattr(c, \"type\", None) == \"text\":\n",
    "                texts.append(c.text)\n",
    "        return \"\".join(texts)\n",
    "\n",
    "    async def _ollama_json(self, prompt: str) -> str:\n",
    "        data = {\"model\": self.cfg.ollama_model, \"prompt\": \"Return STRICT JSON only.\\n\\n\" + prompt, \"stream\": False}\n",
    "        r = await self._client.post(\"/api/generate\", json=data)\n",
    "        r.raise_for_status()\n",
    "        return r.json().get(\"response\",\"\")\n",
    "\n",
    "    async def json(self, prompt: str) -> Dict[str, Any]:\n",
    "        try:\n",
    "            if self.provider == \"openai\":\n",
    "                raw = await self._openai_json(prompt)\n",
    "            elif self.provider == \"azure\":\n",
    "                raw = await self._azure_json(prompt)\n",
    "            elif self.provider == \"anthropic\":\n",
    "                raw = await self._anthropic_json(prompt)\n",
    "            elif self.provider == \"ollama\":\n",
    "                raw = await self._ollama_json(prompt)\n",
    "            else:\n",
    "                return {}\n",
    "            return safe_json_load(raw) or {}\n",
    "        except Exception as e:\n",
    "            logger.warning(\"LLM json() failed: %s\", e)\n",
    "            return {}\n",
    "\n",
    "# ============================ Async Source Clients ============================\n",
    "\n",
    "ARXIV_RE = re.compile(r'(arxiv:)?\\s*(\\d{4}\\.\\d{4,5})(v\\d+)?', re.I)\n",
    "DOI_RE = re.compile(r'(10\\.\\d{4,9}/[^\\s,;]+)', re.I)\n",
    "\n",
    "class SourceClient:\n",
    "    NAME: str = \"base\"\n",
    "    def __init__(self, cfg: PipelineConfig, client=None, limiter=None, cache=None):\n",
    "        self.cfg = cfg\n",
    "        self.client = client or (httpx.AsyncClient(timeout=httpx.Timeout(connect=cfg.timeout_s, read=cfg.timeout_s, write=cfg.timeout_s, pool=cfg.timeout_s)) if httpx is not None else None)\n",
    "        self.limiter = limiter or asyncio.Semaphore(cfg.concurrency)\n",
    "        self.cache = cache\n",
    "\n",
    "    def _cache_get(self, key: str):\n",
    "        if not self.cache: return None\n",
    "        return self.cache.get((self.NAME, key))\n",
    "\n",
    "    def _cache_set(self, key: str, val: Dict[str, Any]):\n",
    "        if not self.cache: return\n",
    "        self.cache[(self.NAME, key)] = val\n",
    "\n",
    "    async def _get_json(self, url: str, params: Optional[Dict[str, Any]] = None, headers: Optional[Dict[str, str]] = None) -> Dict[str, Any]:\n",
    "        if self.client is None:\n",
    "            raise RuntimeError(\"HTTP client unavailable.\")\n",
    "        hdrs = {\"User-Agent\": DEFAULT_UA}\n",
    "        if headers: hdrs.update(headers)\n",
    "\n",
    "        attempt = 0\n",
    "        while True:\n",
    "            attempt += 1\n",
    "            try:\n",
    "                async with self.limiter:\n",
    "                    r = await self.client.get(url, params=params, headers=hdrs)\n",
    "                _jlog(event=\"http_request\", url=str(r.request.url), code=r.status_code)\n",
    "                if r.status_code in (429, 500, 502, 503, 504) and attempt <= 4:\n",
    "                    await asyncio.sleep(min(2**attempt, 8) + (0.1 * attempt))\n",
    "                    continue\n",
    "                r.raise_for_status()\n",
    "                ct = r.headers.get(\"content-type\",\"\")\n",
    "                if \"json\" in ct:\n",
    "                    return r.json()\n",
    "                return {\"_raw\": r.text}\n",
    "            except Exception as e:\n",
    "                if attempt <= 2:\n",
    "                    await asyncio.sleep(0.3 * attempt)\n",
    "                    continue\n",
    "                raise\n",
    "\n",
    "    async def by_doi(self, doi: str) -> Optional[Dict[str, Any]]: raise NotImplementedError\n",
    "    async def by_title(self, title: str) -> Optional[Any]: raise NotImplementedError\n",
    "\n",
    "class CrossrefClient(SourceClient):\n",
    "    NAME = \"crossref\"; BASE_URL = \"https://api.crossref.org/works\"\n",
    "    async def by_doi(self, doi: str) -> Optional[Dict[str, Any]]:\n",
    "        key = f\"doi:{doi.lower().strip()}\"\n",
    "        if (c := self._cache_get(key)): return c\n",
    "        try:\n",
    "            data = await self._get_json(f\"{self.BASE_URL}/{doi}\")\n",
    "            msg = data.get(\"message\")\n",
    "            if msg: self._cache_set(key, msg)\n",
    "            return msg\n",
    "        except Exception: return None\n",
    "    async def by_title(self, title: str) -> Optional[List[Dict[str, Any]]]:\n",
    "        key = f\"title:{norm_for_compare(title)}\"\n",
    "        c = self._cache_get(key)\n",
    "        # We'll still fetch up to 5; but keep first item in cache for speed\n",
    "        params = {\n",
    "            \"query.title\": title,\n",
    "            \"rows\": 5,\n",
    "            \"select\":\"title,author,container-title,short-container-title,issued,DOI,page,volume,issue,published-print,published-online,type\"\n",
    "        }\n",
    "        try:\n",
    "            data = await self._get_json(self.BASE_URL, params=params)\n",
    "            items = data.get(\"message\", {}).get(\"items\", [])[:5]\n",
    "            if items and not c:\n",
    "                self._cache_set(key, items[0])\n",
    "            return items\n",
    "        except Exception: return None\n",
    "\n",
    "class OpenAlexClient(SourceClient):\n",
    "    NAME = \"openalex\"; BASE_URL = \"https://api.openalex.org/works\"\n",
    "    async def by_doi(self, doi: str) -> Optional[Dict[str, Any]]:\n",
    "        key = f\"doi:{doi.lower().strip()}\"\n",
    "        if (c := self._cache_get(key)): return c\n",
    "        try:\n",
    "            headers = {\"User-Agent\": DEFAULT_UA}\n",
    "            data = await self._get_json(self.BASE_URL, params={\"filter\": f\"doi:{doi}\"}, headers=headers)\n",
    "            items = data.get(\"results\", [])\n",
    "            it = items[0] if items else None\n",
    "            if it: self._cache_set(key, it)\n",
    "            return it\n",
    "        except Exception: return None\n",
    "    async def by_title(self, title: str) -> Optional[List[Dict[str, Any]]]:\n",
    "        key = f\"title:{norm_for_compare(title)}\"\n",
    "        try:\n",
    "            headers = {\"User-Agent\": DEFAULT_UA}\n",
    "            data = await self._get_json(self.BASE_URL, params={\"filter\": f\"title.search:{title}\", \"per-page\":5}, headers=headers)\n",
    "            items = data.get(\"results\", [])[:5]\n",
    "            return items\n",
    "        except Exception: return None\n",
    "\n",
    "class SemanticScholarClient(SourceClient):\n",
    "    NAME = \"semanticscholar\"; BASE_URL = \"https://api.semanticscholar.org/graph/v1/paper\"\n",
    "    S2_KEY = os.getenv(\"SEMANTIC_SCHOLAR_API_KEY\")\n",
    "    def _headers(self):\n",
    "        h = {\"User-Agent\": DEFAULT_UA}\n",
    "        if self.S2_KEY: h[\"x-api-key\"] = self.S2_KEY\n",
    "        return h\n",
    "    async def by_doi(self, doi: str) -> Optional[Dict[str, Any]]:\n",
    "        key = f\"doi:{doi.lower().strip()}\"\n",
    "        if (c := self._cache_get(key)): return c\n",
    "        try:\n",
    "            data = await self._get_json(f\"{self.BASE_URL}/DOI:{doi}\", params={\"fields\":\"title,venue,year,authors,externalIds,publicationVenue,publicationTypes\"}, headers=self._headers())\n",
    "            if data and not data.get(\"error\"): self._cache_set(key, data)\n",
    "            return data if data and not data.get(\"error\") else None\n",
    "        except Exception: return None\n",
    "    async def by_title(self, title: str) -> Optional[List[Dict[str, Any]]]:\n",
    "        try:\n",
    "            data = await self._get_json(\n",
    "                f\"{self.BASE_URL}/search\",\n",
    "                params={\"query\": title, \"limit\":5, \"fields\":\"title,venue,year,authors,externalIds,publicationVenue,publicationTypes\"},\n",
    "                headers=self._headers()\n",
    "            )\n",
    "            items = data.get(\"data\") or []\n",
    "            return items[:5]\n",
    "        except Exception: return None\n",
    "\n",
    "class PubMedClient(SourceClient):\n",
    "    NAME = \"pubmed\"\n",
    "    ESEARCH = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n",
    "    ESUMMARY = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi\"\n",
    "    async def by_doi(self, doi: str) -> Optional[Dict[str, Any]]:\n",
    "        return None\n",
    "    async def by_title(self, title: str) -> Optional[Dict[str, Any]]:\n",
    "        key = f\"title:{norm_for_compare(title)}\"\n",
    "        if (c := self._cache_get(key)): return c\n",
    "        try:\n",
    "            d = await self._get_json(self.ESEARCH, params={\"db\":\"pubmed\",\"term\":title,\"retmode\":\"json\",\"retmax\":\"1\",\"tool\":\"ieee-ref-agent\",\"email\":\"you@example.com\"})\n",
    "            ids = d.get(\"esearchresult\", {}).get(\"idlist\", [])\n",
    "            if not ids: return None\n",
    "            pmid = ids[0]\n",
    "            d2 = await self._get_json(self.ESUMMARY, params={\"db\":\"pubmed\",\"id\":pmid,\"retmode\":\"json\",\"tool\":\"ieee-ref-agent\",\"email\":\"you@example.com\"})\n",
    "            res = d2.get(\"result\", {}).get(pmid)\n",
    "            if res: self._cache_set(key, res)\n",
    "            return res\n",
    "        except Exception: return None\n",
    "\n",
    "class ArxivClient(SourceClient):\n",
    "    NAME = \"arxiv\"; BASE_URL = \"https://export.arXiv.org/api/query\"\n",
    "    async def by_doi(self, doi: str) -> Optional[Dict[str, Any]]:\n",
    "        return None\n",
    "    async def by_title(self, title: str) -> Optional[Dict[str, Any]]:\n",
    "        # keep title search for fallback\n",
    "        try:\n",
    "            if self.client is None:\n",
    "                return None\n",
    "            async with self.limiter:\n",
    "                r = await self.client.get(self.BASE_URL, params={\"search_query\": f\"ti:\\\"{title}\\\"\", \"start\":0, \"max_results\":1}, headers={\"Accept\":\"application/atom+xml\",\"User-Agent\":DEFAULT_UA})\n",
    "                _jlog(event=\"http_request\", url=str(r.request.url), code=r.status_code)\n",
    "                r.raise_for_status()\n",
    "                xml = r.text\n",
    "                tmatch = re.search(r\"<title>(.*?)</title>\", xml, flags=re.DOTALL)\n",
    "                if not tmatch: return None\n",
    "                title0 = normalize_text(re.sub(r\"\\s+\", \" \", tmatch.group(1)))\n",
    "                auths = [normalize_text(a) for a in re.findall(r\"<name>(.*?)</name>\", xml)]\n",
    "                ymatch = re.search(r\"<published>(\\d{4})-\", xml)\n",
    "                year0 = ymatch.group(1) if ymatch else \"\"\n",
    "                return {\"title\": title0, \"authors\": auths, \"journal_name\":\"arXiv\", \"year\": year0, \"doi\":\"\"}\n",
    "        except Exception:\n",
    "            return None\n",
    "    async def by_id(self, arx: str) -> Optional[Dict[str, Any]]:\n",
    "        try:\n",
    "            if self.client is None:\n",
    "                return None\n",
    "            async with self.limiter:\n",
    "                r = await self.client.get(self.BASE_URL, params={\"id_list\": arx}, headers={\"Accept\":\"application/atom+xml\",\"User-Agent\":DEFAULT_UA})\n",
    "                _jlog(event=\"http_request\", url=str(r.request.url), code=r.status_code)\n",
    "                r.raise_for_status()\n",
    "                xml = r.text\n",
    "                tmatch = re.search(r\"<title>(.*?)</title>\", xml, flags=re.DOTALL)\n",
    "                if not tmatch: return None\n",
    "                title0 = normalize_text(re.sub(r\"\\s+\", \" \", tmatch.group(1)))\n",
    "                auths = [normalize_text(a) for a in re.findall(r\"<name>(.*?)</name>\", xml)]\n",
    "                ymatch = re.search(r\"<published>(\\d{4})-\", xml)\n",
    "                year0 = ymatch.group(1) if ymatch else \"\"\n",
    "                return {\"title\": title0, \"authors\": auths, \"journal_name\":\"arXiv\", \"year\": year0, \"doi\":\"\"}\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "# ============================ Candidate normalization & scoring ============================\n",
    "\n",
    "def normalize_candidate(source: str, rec: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    out: Dict[str, Any] = {\"source\": source, \"raw\": rec}\n",
    "    if source == \"crossref\":\n",
    "        out[\"title\"] = normalize_text((rec.get(\"title\") or [\"\"])[0]) if rec.get(\"title\") else \"\"\n",
    "        out[\"authors\"] = [normalize_text(f\"{a.get('given','')} {a.get('family','')}\".strip()) for a in rec.get(\"author\", [])] if rec.get(\"author\") else []\n",
    "        out[\"journal_name\"] = normalize_text((rec.get(\"container-title\") or [\"\"])[0]) if rec.get(\"container-title\") else \"\"\n",
    "        out[\"journal_abbrev\"] = normalize_text((rec.get(\"short-container-title\") or [\"\"])[0]) if rec.get(\"short-container-title\") else \"\"\n",
    "        out[\"volume\"] = normalize_text(rec.get(\"volume\") or \"\")\n",
    "        out[\"issue\"]  = normalize_text(rec.get(\"issue\") or \"\")\n",
    "        out[\"pages\"]  = normalize_text(rec.get(\"page\") or \"\")\n",
    "        out[\"doi\"]    = normalize_text(rec.get(\"DOI\") or \"\")\n",
    "        out[\"cr_type\"]= normalize_text(rec.get(\"type\") or \"\")\n",
    "        y, m = \"\", \"\"\n",
    "        for src in (\"issued\",\"published-print\",\"published-online\"):\n",
    "            dp = (rec.get(src) or {}).get(\"date-parts\")\n",
    "            if dp:\n",
    "                y = str(dp[0][0])\n",
    "                if len(dp[0])>1: m = str(dp[0][1])\n",
    "                break\n",
    "        out[\"year\"], out[\"month\"] = y, normalize_month_field(m)\n",
    "    elif source == \"openalex\":\n",
    "        out[\"title\"] = normalize_text(rec.get(\"display_name\") or rec.get(\"title\") or \"\")\n",
    "        out[\"authors\"] = [normalize_text(a.get(\"author\", {}).get(\"display_name\") or \"\") for a in rec.get(\"authorships\", [])] if rec.get(\"authorships\") else []\n",
    "        hv = rec.get(\"host_venue\", {}) if isinstance(rec.get(\"host_venue\"), dict) else {}\n",
    "        out[\"journal_name\"] = normalize_text(hv.get(\"display_name\") or \"\")\n",
    "        out[\"journal_abbrev\"] = normalize_text(hv.get(\"abbrev\") or \"\")\n",
    "        out[\"doi\"] = normalize_text(rec.get(\"doi\") or \"\")\n",
    "        out[\"volume\"] = normalize_text(rec.get(\"biblio\", {}).get(\"volume\") or \"\")\n",
    "        out[\"issue\"]  = normalize_text(rec.get(\"biblio\", {}).get(\"issue\") or \"\")\n",
    "        fp = rec.get(\"biblio\", {}).get(\"first_page\") or \"\"\n",
    "        lp = rec.get(\"biblio\", {}).get(\"last_page\") or \"\"\n",
    "        out[\"pages\"] = f\"{fp}-{lp}\" if fp and lp else normalize_text(fp or \"\")\n",
    "        out[\"year\"]   = str(rec.get(\"publication_year\") or (rec.get(\"from_publication_date\") or \"\")[:4] or \"\")\n",
    "        out[\"month\"]  = \"\"\n",
    "        out[\"oa_is_proceedings\"] = \"proceedings\" in norm_for_compare(hv.get(\"display_name\") or \"\")\n",
    "    elif source == \"semanticscholar\":\n",
    "        out[\"title\"] = normalize_text(rec.get(\"title\") or \"\")\n",
    "        out[\"authors\"] = [normalize_text(a.get(\"name\") or \"\") for a in rec.get(\"authors\", [])] if rec.get(\"authors\") else []\n",
    "        out[\"journal_name\"] = normalize_text(rec.get(\"venue\") or (rec.get(\"publicationVenue\") or {}).get(\"name\") or \"\")\n",
    "        out[\"journal_abbrev\"] = \"\"\n",
    "        eid = rec.get(\"externalIds\") or {}\n",
    "        out[\"doi\"] = normalize_text(eid.get(\"DOI\") or rec.get(\"doi\") or \"\")\n",
    "        out[\"year\"]   = normalize_text(rec.get(\"year\") or \"\")\n",
    "        out[\"month\"]  = \"\"\n",
    "        out[\"s2_types\"] = [normalize_text(t) for t in (rec.get(\"publicationTypes\") or [])]\n",
    "    elif source == \"pubmed\":\n",
    "        out[\"title\"] = normalize_text(rec.get(\"title\") or rec.get(\"sorttitle\") or \"\")\n",
    "        out[\"authors\"] = [normalize_text(a.get(\"name\")) for a in rec.get(\"authors\", []) if a.get(\"name\")] if rec.get(\"authors\") else []\n",
    "        out[\"journal_name\"] = normalize_text((rec.get(\"fulljournalname\") or rec.get(\"source\") or \"\"))\n",
    "        out[\"journal_abbrev\"] = normalize_text(rec.get(\"source\") or \"\")\n",
    "        out[\"doi\"] = normalize_text((rec.get(\"elocationid\") or \"\").replace(\"doi:\",\"\").strip())\n",
    "        out[\"volume\"] = normalize_text(rec.get(\"volume\") or \"\")\n",
    "        out[\"issue\"]  = normalize_text(rec.get(\"issue\") or \"\")\n",
    "        out[\"pages\"]  = normalize_text(rec.get(\"pages\") or \"\")\n",
    "        out[\"year\"]   = normalize_text((rec.get(\"pubdate\") or \"\").split(\" \")[0])\n",
    "        out[\"month\"]  = \"\"\n",
    "    elif source == \"arxiv\":\n",
    "        out[\"title\"] = normalize_text(rec.get(\"title\") or \"\")\n",
    "        out[\"authors\"] = [normalize_text(a) for a in rec.get(\"authors\", [])]\n",
    "        out[\"journal_name\"] = \"arXiv\"\n",
    "        out[\"journal_abbrev\"] = \"arXiv\"\n",
    "        out[\"doi\"] = normalize_text(rec.get(\"doi\") or \"\")\n",
    "        out[\"year\"] = normalize_text(rec.get(\"year\") or \"\")\n",
    "        out[\"month\"] = \"\"\n",
    "        out[\"volume\"] = \"\"\n",
    "        out[\"issue\"] = \"\"\n",
    "        out[\"pages\"] = \"\"\n",
    "    else:\n",
    "        out.update({k:\"\" for k in (\"title\",\"authors\",\"journal_name\",\"journal_abbrev\",\"doi\",\"volume\",\"issue\",\"pages\",\"year\",\"month\")})\n",
    "    return out\n",
    "\n",
    "def score_candidate(extracted: Dict[str, Any], cand: Dict[str, Any]) -> float:\n",
    "    score = 0.0\n",
    "    ex_doi = normalize_text(extracted.get(\"doi\") or \"\").lower().replace(\"doi:\",\"\")\n",
    "    ca_doi = normalize_text(cand.get(\"doi\") or \"\").lower().replace(\"doi:\",\"\")\n",
    "    if ex_doi and ca_doi and ex_doi == ca_doi: score += 1.0\n",
    "    score += 0.6 * token_similarity(extracted.get(\"title\") or \"\", cand.get(\"title\") or \"\")\n",
    "    ex_auth = [a.split()[-1].lower() for a in authors_to_list(extracted.get(\"authors\")) if a.split()]\n",
    "    ca_auth = [a.split()[-1].lower() for a in authors_to_list(cand.get(\"authors\")) if a.split()]\n",
    "    if ex_auth and ca_auth:\n",
    "        inter = len(set(ex_auth) & set(ca_auth))\n",
    "        score += 0.2 * (inter / max(1, len(set(ex_auth) | set(ca_auth))))\n",
    "    ey = str(extracted.get(\"year\") or \"\").strip()\n",
    "    cy = str(cand.get(\"year\") or \"\").strip()\n",
    "    if ey and cy and ey == cy: score += 0.1\n",
    "    src_weight = {\"crossref\": 0.12, \"openalex\": 0.08, \"semanticscholar\": 0.06, \"pubmed\": 0.05, \"arxiv\": 0.03}\n",
    "    score += src_weight.get(cand.get(\"source\",\"\"), 0.0)\n",
    "    return score\n",
    "\n",
    "# ============================ Type reconciliation ============================\n",
    "\n",
    "TYPE_CANON = {\n",
    "    \"journal-article\": \"journal article\",\n",
    "    \"paper-conference\": \"conference paper\",\n",
    "    \"proceedings-article\": \"conference paper\",\n",
    "    \"book-chapter\": \"book chapter\",\n",
    "    \"book\": \"book\",\n",
    "    \"dataset\": \"dataset\",\n",
    "    \"standard\": \"standard\",\n",
    "    \"report\": \"technical report\",\n",
    "    \"thesis\": \"thesis\",\n",
    "}\n",
    "\n",
    "def reconcile_type(initial_type: str, candidates: List[Dict[str, Any]], llm_vote: Optional[str]) -> str:\n",
    "    votes = []\n",
    "    if initial_type: votes.append(initial_type)\n",
    "    if llm_vote: votes.append(llm_vote.lower())\n",
    "    for c in candidates or []:\n",
    "        if c[\"source\"] == \"crossref\":\n",
    "            t = c.get(\"cr_type\",\"\")\n",
    "            if t: votes.append(TYPE_CANON.get(t, t))\n",
    "        elif c[\"source\"] == \"openalex\":\n",
    "            if c.get(\"oa_is_proceedings\"): votes.append(\"conference paper\")\n",
    "        elif c[\"source\"] == \"semanticscholar\":\n",
    "            types = c.get(\"s2_types\") or []\n",
    "            if any(\"conference\" in t for t in types): votes.append(\"conference paper\")\n",
    "            if any(\"journal\" in t for t in types): votes.append(\"journal article\")\n",
    "            if any(\"book\" in t for t in types): votes.append(\"book\")\n",
    "        elif c[\"source\"] == \"arxiv\":\n",
    "            votes.append(\"preprint\")\n",
    "    from collections import Counter\n",
    "    cnt = Counter([v.lower() for v in votes if v])\n",
    "    if not cnt: return initial_type or \"other\"\n",
    "    return cnt.most_common(1)[0][0]\n",
    "\n",
    "# ============================ Verification Agents (threaded) ============================\n",
    "\n",
    "def _prefer_abbrev(a: str, b: str) -> str:\n",
    "    cand = [x for x in [a, b] if x]\n",
    "    if not cand: return \"\"\n",
    "    def score(x):\n",
    "        s = x.strip()\n",
    "        return (sum(1 for c in s if c.isupper()), -len(s))\n",
    "    return sorted(cand, key=score, reverse=True)[0]\n",
    "\n",
    "def agent_journal(extracted: Dict[str, Any], best: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    ex_j = normalize_text(extracted.get(\"journal_name\") or \"\")\n",
    "    ex_ab = normalize_text(extracted.get(\"journal_abbrev\") or \"\")\n",
    "    be_j = normalize_text(best.get(\"journal_name\") or \"\")\n",
    "    be_ab = normalize_text(best.get(\"journal_abbrev\") or \"\")\n",
    "    sim_full = token_similarity(ex_j, be_j) if ex_j and be_j else 0.0\n",
    "    sim_ab   = token_similarity(ex_ab, be_ab) if ex_ab and be_ab else 0.0\n",
    "    ok = (sim_full >= 0.6) or (sim_ab >= 0.6) or (bool(ex_j) and not be_j)\n",
    "    corr = {}\n",
    "    if be_j and be_j != ex_j: corr[\"journal_name\"] = be_j\n",
    "    if (be_ab and be_ab != ex_ab) or (not ex_ab and (be_ab or be_j)):\n",
    "        chosen = _prefer_abbrev(be_ab, heuristic_abbrev(be_j or ex_j))\n",
    "        corr[\"journal_abbrev\"] = chosen\n",
    "    return {\"ok\": ok, \"correction\": corr or None}\n",
    "\n",
    "def agent_authors(extracted: Dict[str, Any], best: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    ex = authors_to_list(extracted.get(\"authors\"))\n",
    "    be = authors_to_list(best.get(\"authors\"))\n",
    "    if be:\n",
    "        matches = 0\n",
    "        for ea in ex:\n",
    "            last = ea.split()[-1].lower() if ea.split() else \"\"\n",
    "            if any((ba.split()[-1].lower() if ba.split() else \"\") == last for ba in be):\n",
    "                matches += 1\n",
    "        required = max(1, int(0.5 * len(ex))) if ex else 1\n",
    "        if matches >= required:\n",
    "            if any(re.match(r\"^[A-Z]\\.\", p.split()[0]) if p.split() else False for p in ex[:3]):\n",
    "                return {\"ok\": True, \"correction\": {\"authors\": be}}\n",
    "            return {\"ok\": True}\n",
    "        return {\"ok\": False, \"correction\": {\"authors\": be}}\n",
    "    return {\"ok\": bool(ex)}\n",
    "\n",
    "def agent_title(extracted: Dict[str, Any], best: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    ex_t = normalize_text(extracted.get(\"title\") or \"\")\n",
    "    be_t = normalize_text(best.get(\"title\") or \"\")\n",
    "    desired = sentence_case(ex_t) if ex_t else \"\"\n",
    "    if be_t:\n",
    "        sim = token_similarity(ex_t, be_t)\n",
    "        if sim >= 0.7:\n",
    "            if ex_t != desired:\n",
    "                return {\"ok\": True, \"correction\": {\"title\": desired}}\n",
    "            return {\"ok\": True}\n",
    "        return {\"ok\": False, \"correction\": {\"title\": be_t}}\n",
    "    else:\n",
    "        if ex_t and ex_t != desired:\n",
    "            return {\"ok\": False, \"correction\": {\"title\": desired}}\n",
    "        return {\"ok\": bool(ex_t)}\n",
    "\n",
    "def agent_year_month(extracted: Dict[str, Any], best: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    ex_y = str(extracted.get(\"year\") or \"\")\n",
    "    ex_m = normalize_month_field(extracted.get(\"month\") or \"\")\n",
    "    be_y = str(best.get(\"year\") or \"\")\n",
    "    be_m = normalize_month_field(best.get(\"month\") or \"\")\n",
    "    ok = True; corr = {}\n",
    "    if be_y and be_y != ex_y: corr[\"year\"] = be_y; ok = False\n",
    "    if be_m and be_m != ex_m: corr[\"month\"] = be_m; ok = False\n",
    "    return {\"ok\": ok, \"correction\": corr or None}\n",
    "\n",
    "def agent_vipd(extracted: Dict[str, Any], best: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    exv, exi, exp, exd = [normalize_text(extracted.get(k) or \"\") for k in (\"volume\",\"issue\",\"pages\",\"doi\")]\n",
    "    bev, bei, bep, bed = [normalize_text(best.get(k) or \"\") for k in (\"volume\",\"issue\",\"pages\",\"doi\")]\n",
    "    ok = True; corr = {}\n",
    "    if bev and bev != exv: corr[\"volume\"] = bev; ok = False\n",
    "    if bei and bei != exi: corr[\"issue\"]  = bei; ok = False\n",
    "    if bep and bep != exp: corr[\"pages\"]  = bep; ok = False\n",
    "    if bed and bed.lower().replace(\"doi:\",\"\") != exd.lower().replace(\"doi:\",\"\"):\n",
    "        corr[\"doi\"] = bed; ok = False\n",
    "    return {\"ok\": ok, \"correction\": corr or None}\n",
    "\n",
    "def agent_presence(extracted: Dict[str, Any], best: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    return {\"ok\": bool(extracted.get(\"title\")) and bool(extracted.get(\"authors\"))}\n",
    "\n",
    "# ============================ LangGraph State & Nodes ============================\n",
    "\n",
    "class PipelineState(TypedDict, total=False):\n",
    "    reference: str\n",
    "    type: str\n",
    "    extracted: Dict[str, Any]\n",
    "    candidates: List[Dict[str, Any]]\n",
    "    best: Dict[str, Any]\n",
    "    verification: Dict[str, bool]\n",
    "    suggestions: Dict[str, Any]\n",
    "    corrections: List[Tuple[str, Any, Any]]\n",
    "    formatted: str\n",
    "    report: str\n",
    "    attempts: int\n",
    "    hops: int\n",
    "    _made_changes_last_cycle: bool\n",
    "    _cfg: PipelineConfig\n",
    "    _llm: Any\n",
    "    _http: Any\n",
    "    _cache: Any\n",
    "    _limiter: Any\n",
    "    _sources: Any\n",
    "    _llm_type_vote: Optional[str]\n",
    "    csl_json: Dict[str, Any]\n",
    "    bibtex: str\n",
    "    _ver_score: int\n",
    "    _stagnation: int\n",
    "    _fp: str\n",
    "    _fp_history: set\n",
    "    _loop_detected: bool\n",
    "\n",
    "async def _init_runtime(state: PipelineState) -> PipelineState:\n",
    "    cfg = state.get(\"_cfg\") or CFG\n",
    "    llm = LLMAdapter(cfg)\n",
    "    http = httpx.AsyncClient(timeout=httpx.Timeout(connect=cfg.timeout_s, read=cfg.timeout_s, write=cfg.timeout_s, pool=cfg.timeout_s)) if httpx is not None else None\n",
    "    cache = TTLCache(maxsize=1000, ttl=cfg.cache_ttl_s) if CACHE_AVAILABLE and TTLCache is not None else None\n",
    "    limiter = asyncio.Semaphore(cfg.concurrency)\n",
    "    sources = [\n",
    "        CrossrefClient(cfg, client=http, limiter=limiter, cache=cache),\n",
    "        OpenAlexClient(cfg, client=http, limiter=limiter, cache=cache),\n",
    "        SemanticScholarClient(cfg, client=http, limiter=limiter, cache=cache),\n",
    "        PubMedClient(cfg, client=http, limiter=limiter, cache=cache),\n",
    "        ArxivClient(cfg, client=http, limiter=limiter, cache=cache),\n",
    "    ]\n",
    "    state.update({\n",
    "        \"_cfg\": cfg, \"_llm\": llm, \"_http\": http, \"_cache\": cache,\n",
    "        \"_limiter\": limiter, \"_sources\": sources,\n",
    "    })\n",
    "    state.setdefault(\"hops\", 0)\n",
    "    state.setdefault(\"attempts\", 0)\n",
    "    state.setdefault(\"_ver_score\", -1)\n",
    "    state.setdefault(\"_stagnation\", 0)\n",
    "    state.setdefault(\"_fp\", \"\")\n",
    "    state.setdefault(\"_fp_history\", set())\n",
    "    state.setdefault(\"_loop_detected\", False)\n",
    "    state.setdefault(\"_made_changes_last_cycle\", False)\n",
    "    return state\n",
    "\n",
    "# --- Detect type using heuristics + LLM vote (once) ---\n",
    "async def node_detect_type_async(state: PipelineState) -> PipelineState:\n",
    "    ref = state[\"reference\"]\n",
    "    rtype = \"other\"\n",
    "    if re.search(r\"\\bvol\\.|no\\.|pp\\.\", ref, flags=re.I):\n",
    "        rtype = \"journal article\"\n",
    "    if re.search(r\"\\bin\\b.+(proc|conference|symposium|workshop)\", ref, flags=re.I):\n",
    "        rtype = \"conference paper\"\n",
    "    if re.search(r\"\\bISBN\\b\", ref, flags=re.I):\n",
    "        rtype = \"book\"\n",
    "    llm: LLMAdapter = state[\"_llm\"]\n",
    "    vote = await llm.json(\n",
    "        \"Classify this reference into one of: journal article, conference paper, book, book chapter, thesis, technical report, dataset, standard, software, other. \"\n",
    "        \"Return JSON {\\\"type\\\": \\\"...\\\"}. Ref:\\n\" + ref\n",
    "    )\n",
    "    state[\"_llm_type_vote\"] = (vote or {}).get(\"type\")\n",
    "    state[\"type\"] = reconcile_type(rtype, [], state[\"_llm_type_vote\"])\n",
    "    return state\n",
    "\n",
    "# --- Parse/extract using LLM-first with regex/arXiv/DOI fallback ---\n",
    "async def node_parse_extract_async(state: PipelineState) -> PipelineState:\n",
    "    ref, rtype = state[\"reference\"], state[\"type\"]\n",
    "    llm: LLMAdapter = state[\"_llm\"]\n",
    "    prompt = (\n",
    "        \"Parse the IEEE-style reference. Return STRICT JSON. Keys among:\\n\"\n",
    "        \"title, authors (list or string), journal_name, journal_abbrev, conference_name,\\n\"\n",
    "        \"volume, issue, pages, year, month, doi, publisher, location, edition, isbn, url.\\n\"\n",
    "        \"Omit unknown keys. JSON ONLY.\\n\\n\"\n",
    "        f\"Type hint: {rtype}\\nReference: {ref}\"\n",
    "    )\n",
    "    parsed = await llm.json(prompt)\n",
    "    if not parsed:\n",
    "        parsed = {}\n",
    "        m = re.search(r\"“([^”]{3,})”|\\\"([^\\\"]{3,})\\\"\", ref)\n",
    "        if m:\n",
    "            parsed[\"title\"] = normalize_text(m.group(1) or m.group(2))\n",
    "            prefix = ref[:m.start()]\n",
    "            parsed[\"authors\"] = authors_to_list(prefix)\n",
    "        dm = DOI_RE.search(ref)\n",
    "        if dm: parsed[\"doi\"] = dm.group(1)\n",
    "        am = ARXIV_RE.search(ref)\n",
    "        if am: parsed[\"arxiv_id\"] = am.group(2)\n",
    "        pm = re.search(r\"pp\\.?\\s*([\\d\\u2013\\u2014\\-]+)\", ref, flags=re.I)\n",
    "        if pm: parsed[\"pages\"] = pm.group(1).replace(\"\\u2013\",\"-\").replace(\"\\u2014\",\"-\")\n",
    "        vm = re.search(r\"vol\\.?\\s*([0-9A-Za-z]+)\", ref, flags=re.I)\n",
    "        if vm: parsed[\"volume\"] = vm.group(1)\n",
    "        im = re.search(r\"no\\.?\\s*([0-9A-Za-z]+)\", ref, flags=re.I)\n",
    "        if im: parsed[\"issue\"] = im.group(1)\n",
    "        y = re.search(r\"\\b(19|20)\\d{2}\\b\", ref)\n",
    "        if y: parsed[\"year\"] = y.group(0)\n",
    "        if m:\n",
    "            after = ref[m.end():]\n",
    "            jm = re.search(r\",\\s*([^,]+?),\", after)\n",
    "            if jm: parsed[\"journal_name\"] = normalize_text(jm.group(1))\n",
    "    if isinstance(parsed.get(\"authors\"), str):\n",
    "        parsed[\"authors\"] = authors_to_list(parsed[\"authors\"])\n",
    "    if parsed.get(\"month\"):\n",
    "        parsed[\"month\"] = normalize_month_field(parsed[\"month\"])\n",
    "    try:\n",
    "        parsed = ExtractedModel(**parsed).dict(exclude_none=True)\n",
    "    except ValidationError as ve:\n",
    "        _jlog(event=\"llm_parse_validation_error\", errors=str(ve))\n",
    "    state[\"extracted\"] = parsed\n",
    "    return state\n",
    "\n",
    "# --- Multi-source lookup (concurrent, top-N, arXiv by id) ---\n",
    "async def node_multisource_lookup_async(state: PipelineState) -> PipelineState:\n",
    "    ex, sources = state[\"extracted\"], state[\"_sources\"]\n",
    "    doi = normalize_text(ex.get(\"doi\") or \"\").lower().replace(\"doi:\",\"\")\n",
    "    title = normalize_text(ex.get(\"title\") or \"\")\n",
    "    arxiv_id = normalize_text(ex.get(\"arxiv_id\") or \"\")\n",
    "\n",
    "    tasks = []\n",
    "    for s in sources:\n",
    "        if arxiv_id and isinstance(s, ArxivClient):\n",
    "            tasks.append(s.by_id(arxiv_id))\n",
    "        if doi:\n",
    "            tasks.append(s.by_doi(doi))\n",
    "        if title:\n",
    "            tasks.append(s.by_title(title))\n",
    "\n",
    "    results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "    out_norm: List[Dict[str, Any]] = []\n",
    "\n",
    "    def _add(source_name, rec):\n",
    "        if isinstance(rec, list):\n",
    "            for r in rec:\n",
    "                if r: out_norm.append(normalize_candidate(source_name, r))\n",
    "        elif isinstance(rec, dict) and rec:\n",
    "            out_norm.append(normalize_candidate(source_name, rec))\n",
    "\n",
    "    idx = 0\n",
    "    for s in sources:\n",
    "        if arxiv_id and isinstance(s, ArxivClient):\n",
    "            _add(s.NAME, results[idx]); idx+=1\n",
    "        if doi:\n",
    "            _add(s.NAME, results[idx]); idx+=1\n",
    "        if title:\n",
    "            _add(s.NAME, results[idx]); idx+=1\n",
    "\n",
    "    dedup: Dict[Tuple[str, str], Dict[str, Any]] = {}\n",
    "    for c in out_norm:\n",
    "        key = (c[\"source\"], c.get(\"doi\") or c.get(\"title\") or \"\")\n",
    "        dedup[key] = c\n",
    "    state[\"candidates\"] = list(dedup.values())\n",
    "    return state\n",
    "\n",
    "def is_trustworthy_match(ex, cand) -> bool:\n",
    "    ex_doi = normalize_text(ex.get(\"doi\")).lower().replace(\"doi:\",\"\")\n",
    "    ca_doi = normalize_text(cand.get(\"doi\")).lower().replace(\"doi:\",\"\")\n",
    "    if ex_doi and ca_doi and ex_doi == ca_doi:\n",
    "        return True\n",
    "    t_sim = token_similarity(ex.get(\"title\",\"\"), cand.get(\"title\",\"\"))\n",
    "    ex_last = {a.split()[-1].lower() for a in authors_to_list(ex.get(\"authors\")) if a.split()}\n",
    "    ca_last = {a.split()[-1].lower() for a in authors_to_list(cand.get(\"authors\")) if a.split()}\n",
    "    return (t_sim >= 0.8) and bool(ex_last & ca_last)\n",
    "\n",
    "def node_select_best(state: PipelineState) -> PipelineState:\n",
    "    ex = state[\"extracted\"]; candidates = state.get(\"candidates\") or []\n",
    "    if not candidates:\n",
    "        state[\"best\"] = {}\n",
    "        return state\n",
    "    best, best_score = None, -1.0\n",
    "    for c in candidates:\n",
    "        sc = score_candidate(ex, c)\n",
    "        if sc > best_score:\n",
    "            best, best_score = c, sc\n",
    "    if best and not is_trustworthy_match(ex, best):\n",
    "        _jlog(event=\"reject_candidate\", reason=\"not_trustworthy\", score=best_score, title=best.get(\"title\"))\n",
    "        best = {}\n",
    "    state[\"best\"] = best or {}\n",
    "    return state\n",
    "\n",
    "def node_reconcile_type(state: PipelineState) -> PipelineState:\n",
    "    state[\"type\"] = reconcile_type(state.get(\"type\",\"other\"), state.get(\"candidates\") or [], state.get(\"_llm_type_vote\"))\n",
    "    return state\n",
    "\n",
    "def node_verify_agents(state: PipelineState) -> PipelineState:\n",
    "    ex = state[\"extracted\"]\n",
    "    be = state.get(\"best\") or {}\n",
    "\n",
    "    agents = [agent_journal, agent_authors, agent_title, agent_year_month, agent_vipd, agent_presence]\n",
    "    results = {}\n",
    "    with ThreadPoolExecutor(max_workers=CFG.agent_threads) as pool:\n",
    "        fut_map = {pool.submit(a, ex, be): a.__name__ for a in agents}\n",
    "        for fut in as_completed(fut_map):\n",
    "            name = fut_map[fut]\n",
    "            try:\n",
    "                results[name] = fut.result()\n",
    "            except Exception as e:\n",
    "                logger.exception(\"Agent %s failed: %s\", name, e)\n",
    "                results[name] = {\"ok\": False}\n",
    "\n",
    "    suggestions = {}\n",
    "    for out in results.values():\n",
    "        if out.get(\"correction\"):\n",
    "            suggestions.update(out[\"correction\"])\n",
    "\n",
    "    vipd_ok = results.get(\"agent_vipd\", {}).get(\"ok\", False)\n",
    "    ym_ok = results.get(\"agent_year_month\", {}).get(\"ok\", False)\n",
    "\n",
    "    verification = {\n",
    "        \"title\":          results.get(\"agent_title\", {}).get(\"ok\", False),\n",
    "        \"authors\":        results.get(\"agent_authors\", {}).get(\"ok\", False),\n",
    "        \"journal_name\":   results.get(\"agent_journal\", {}).get(\"ok\", False),\n",
    "        \"journal_abbrev\": results.get(\"agent_journal\", {}).get(\"ok\", False),\n",
    "        \"year\":           ym_ok,\n",
    "        \"month\":          ym_ok,\n",
    "        \"volume\":         vipd_ok,\n",
    "        \"issue\":          vipd_ok,\n",
    "        \"pages\":          vipd_ok,\n",
    "        \"doi\":            vipd_ok,\n",
    "        \"presence\":       results.get(\"agent_presence\", {}).get(\"ok\", False),\n",
    "    }\n",
    "\n",
    "    ver_score = sum(1 for v in verification.values() if v)\n",
    "    last_score = state.get(\"_ver_score\", -1)\n",
    "    stagnation = state.get(\"_stagnation\", 0)\n",
    "    stagnation = 0 if ver_score > last_score else (stagnation + 1)\n",
    "\n",
    "    state[\"_ver_score\"] = ver_score\n",
    "    state[\"_stagnation\"] = stagnation\n",
    "    state[\"verification\"] = verification\n",
    "    state[\"suggestions\"] = suggestions\n",
    "    state[\"hops\"] = (state.get(\"hops\") or 0) + 1\n",
    "\n",
    "    fp = fingerprint_state(ex, be, suggestions)\n",
    "    hist = state.get(\"_fp_history\") or set()\n",
    "    if fp in hist:\n",
    "        state[\"_loop_detected\"] = True\n",
    "    else:\n",
    "        hist.add(fp)\n",
    "        state[\"_fp_history\"] = hist\n",
    "        state[\"_loop_detected\"] = False\n",
    "\n",
    "    state[\"_fp\"] = fp\n",
    "    return state\n",
    "\n",
    "def node_apply_corrections(state: PipelineState) -> PipelineState:\n",
    "    ex = dict(state[\"extracted\"])\n",
    "    best = state.get(\"best\") or {}\n",
    "    suggestions = state.get(\"suggestions\") or {}\n",
    "    changes: List[Tuple[str, Any, Any]] = []\n",
    "\n",
    "    # authoritative merge\n",
    "    for k in (\"title\",\"authors\",\"journal_name\",\"journal_abbrev\",\"volume\",\"issue\",\"pages\",\"doi\",\"year\",\"month\",\"conference_name\",\"publisher\",\"location\",\"edition\",\"isbn\",\"url\"):\n",
    "        bv = best.get(k)\n",
    "        if bv and normalize_text(ex.get(k)) != normalize_text(bv):\n",
    "            changes.append((k, ex.get(k), bv)); ex[k] = bv\n",
    "\n",
    "    # sentence case & authors list\n",
    "    if ex.get(\"title\"):\n",
    "        sc = sentence_case(ex[\"title\"])\n",
    "        if sc != ex[\"title\"]:\n",
    "            changes.append((\"title_sentence_case\", ex[\"title\"], sc)); ex[\"title\"] = sc\n",
    "    if isinstance(ex.get(\"authors\"), str):\n",
    "        al = authors_to_list(ex[\"authors\"])\n",
    "        if al != ex[\"authors\"]:\n",
    "            changes.append((\"authors_list\", ex[\"authors\"], al)); ex[\"authors\"] = al\n",
    "\n",
    "    # agent suggestions\n",
    "    for k, v in suggestions.items():\n",
    "        if normalize_text(ex.get(k)) != normalize_text(v):\n",
    "            changes.append((k, ex.get(k), v)); ex[k] = v\n",
    "\n",
    "    # normalize month\n",
    "    if ex.get(\"month\"):\n",
    "        newm = normalize_month_field(ex[\"month\"])\n",
    "        if newm != ex[\"month\"]:\n",
    "            changes.append((\"month_normalized\", ex[\"month\"], newm)); ex[\"month\"] = newm\n",
    "\n",
    "    state[\"extracted\"] = ex\n",
    "    state[\"corrections\"] = (state.get(\"corrections\") or []) + changes\n",
    "    state[\"attempts\"] = (state.get(\"attempts\") or 0) + 1\n",
    "    state[\"_made_changes_last_cycle\"] = bool(changes)\n",
    "\n",
    "    # loop/stagnation update (fingerprint on new ex)\n",
    "    sugg = state.get(\"suggestions\") or {}\n",
    "    best_now = state.get(\"best\") or {}\n",
    "    new_fp = fingerprint_state(ex, best_now, sugg)\n",
    "    hist = state.get(\"_fp_history\") or set()\n",
    "    if new_fp in hist:\n",
    "        state[\"_loop_detected\"] = True\n",
    "    else:\n",
    "        hist.add(new_fp)\n",
    "        state[\"_fp_history\"] = hist\n",
    "        state[\"_loop_detected\"] = False\n",
    "    state[\"_fp\"] = new_fp\n",
    "    return state\n",
    "\n",
    "async def node_llm_correct_async(state: PipelineState) -> PipelineState:\n",
    "    ref = state[\"reference\"]; ex = state[\"extracted\"]; ver = state.get(\"verification\") or {}\n",
    "    llm: LLMAdapter = state[\"_llm\"]\n",
    "    prompt = (\n",
    "        \"You are an IEEE reference corrector. Given raw reference, current JSON, and verification booleans, \"\n",
    "        \"return STRICT JSON correcting the fields. Keys among: title, authors (list), journal_name, journal_abbrev, \"\n",
    "        \"conference_name, volume, issue, pages, year, month, doi, publisher, location, edition, isbn, url. \"\n",
    "        \"Omit unknown keys. JSON ONLY.\\n\\n\"\n",
    "        f\"Raw: {ref}\\n\\nCurrent: {json.dumps(ex, ensure_ascii=False)}\\n\\nVerification: {json.dumps(ver)}\"\n",
    "    )\n",
    "    patch = await llm.json(prompt)\n",
    "    if patch:\n",
    "        if isinstance(patch.get(\"authors\"), str):\n",
    "            patch[\"authors\"] = authors_to_list(patch[\"authors\"])\n",
    "        if patch.get(\"month\"):\n",
    "            patch[\"month\"] = normalize_month_field(patch[\"month\"])\n",
    "        try:\n",
    "            patch = ExtractedModel(**patch).dict(exclude_none=True)\n",
    "        except ValidationError as ve:\n",
    "            _jlog(event=\"llm_patch_validation_error\", errors=str(ve))\n",
    "            patch = {}\n",
    "        ex2 = dict(ex); changes = []\n",
    "        for k, v in patch.items():\n",
    "            if normalize_text(ex2.get(k)) != normalize_text(v):\n",
    "                changes.append((k, ex2.get(k), v)); ex2[k] = v\n",
    "        state[\"extracted\"] = ex2\n",
    "        state[\"corrections\"] = (state.get(\"corrections\") or []) + changes\n",
    "        state[\"_made_changes_last_cycle\"] = state.get(\"_made_changes_last_cycle\", False) or bool(changes)\n",
    "        best = state.get(\"best\") or {}\n",
    "        sugg = state.get(\"suggestions\") or {}\n",
    "        state[\"_fp\"] = fingerprint_state(ex2, best, sugg)\n",
    "    return state\n",
    "\n",
    "def node_enrich_from_best(state: PipelineState) -> PipelineState:\n",
    "    ex = dict(state[\"extracted\"]); be = state.get(\"best\") or {}\n",
    "    for k in (\"journal_abbrev\",\"journal_name\",\"volume\",\"issue\",\"pages\",\"year\",\"month\",\"doi\",\"conference_name\",\"publisher\",\"location\",\"edition\",\"isbn\",\"url\",\"title\",\"authors\"):\n",
    "        if not ex.get(k) and be.get(k):\n",
    "            ex[k] = be.get(k)\n",
    "    if ex.get(\"month\"):\n",
    "        ex[\"month\"] = normalize_month_field(ex[\"month\"])\n",
    "    state[\"extracted\"] = ex\n",
    "    return state\n",
    "\n",
    "def node_format_reference(state: PipelineState) -> PipelineState:\n",
    "    ex = state[\"extracted\"]; rtype = (state[\"type\"] or \"other\").lower()\n",
    "    A = authors_to_list(ex.get(\"authors\") or [])\n",
    "    A_fmt = format_authors_ieee_list(A)\n",
    "    title_raw = ex.get(\"title\") or \"\"\n",
    "    title = sentence_case(title_raw)\n",
    "    journal = ex.get(\"journal_abbrev\") or ex.get(\"journal_name\") or \"\"\n",
    "    vol = normalize_text(ex.get(\"volume\") or \"\")\n",
    "    issue = normalize_text(ex.get(\"issue\") or \"\")\n",
    "    pages_raw = normalize_text(ex.get(\"pages\") or \"\")\n",
    "    pages_norm, is_eloc = normalize_pages(pages_raw)\n",
    "    if \"-\" in pages_norm: pages_norm = pages_norm.replace(\"-\", \"–\")\n",
    "    year = normalize_text(ex.get(\"year\") or \"\")\n",
    "    month = normalize_month_field(ex.get(\"month\") or \"\")\n",
    "    month_disp = MONTHS_NAME.get(month, month) if month else \"\"\n",
    "    doi_link = format_doi_link(ex.get(\"doi\") or \"\")\n",
    "    conf = normalize_text(ex.get(\"conference_name\") or \"\")\n",
    "    loc = normalize_text(ex.get(\"location\") or \"\")\n",
    "    pub = normalize_text(ex.get(\"publisher\") or \"\")\n",
    "    edition = normalize_text(ex.get(\"edition\") or \"\")\n",
    "    isbn = normalize_text(ex.get(\"isbn\") or \"\")\n",
    "\n",
    "    parts: List[str] = []\n",
    "    if A_fmt: parts.append(A_fmt)\n",
    "\n",
    "    include_quoted_title = rtype not in (\"book\",)\n",
    "    if include_quoted_title and title:\n",
    "        parts.append(f\"\\\"{title}\\\"\")\n",
    "\n",
    "    if rtype in (\"journal article\",\"journal\"):\n",
    "        if journal: parts.append(f\"*{journal}*\")\n",
    "        if vol: parts.append(f\"vol. {vol}\")\n",
    "        if issue: parts.append(f\"no. {issue}\")\n",
    "        if pages_norm:\n",
    "            parts.append(f\"Art. no. {pages_norm}\" if is_eloc else f\"pp. {pages_norm}\")\n",
    "        date = \" \".join([m for m in [month_disp, year] if m]).strip()\n",
    "        if date: parts.append(date)\n",
    "        if doi_link: parts.append(doi_link)\n",
    "\n",
    "    elif rtype == \"conference paper\":\n",
    "        venue = conf or journal or \"Proceedings\"\n",
    "        if venue: parts.append(f\"in *{venue}*\")\n",
    "        if loc: parts.append(loc)\n",
    "        if pages_norm:\n",
    "            parts.append(f\"pp. {pages_norm}\")\n",
    "        date = \" \".join([m for m in [month_disp, year] if m]).strip()\n",
    "        if date: parts.append(date)\n",
    "        if doi_link: parts.append(doi_link)\n",
    "\n",
    "    elif rtype == \"preprint\":\n",
    "        parts.append(\"preprint\")\n",
    "        if journal and \"arxiv\" in journal.lower():\n",
    "            parts.append(journal)\n",
    "        date = \" \".join([m for m in [month_disp, year] if m]).strip()\n",
    "        if date: parts.append(date)\n",
    "        if doi_link: parts.append(doi_link)\n",
    "\n",
    "    elif rtype == \"book\":\n",
    "        if title: parts.append(f\"*{title}*\")\n",
    "        if edition: parts.append(f\"{edition} ed.\")\n",
    "        imprint = f\"{loc}: {pub}\" if (loc and pub) else (loc or pub)\n",
    "        if imprint: parts.append(imprint)\n",
    "        if year: parts.append(year)\n",
    "        if isbn: parts.append(f\"ISBN: {isbn}\")\n",
    "        if doi_link: parts.append(doi_link)\n",
    "\n",
    "    elif rtype in (\"book chapter\",\"chapter\"):\n",
    "        book_title = normalize_text(ex.get(\"book_title\") or conf or journal)\n",
    "        if book_title: parts.append(f\"in *{book_title}*\")\n",
    "        if pages_norm: parts.append(f\"pp. {pages_norm}\")\n",
    "        if pub: parts.append(pub)\n",
    "        date = \" \".join([m for m in [month_disp, year] if m]).strip()\n",
    "        if date: parts.append(date)\n",
    "        if doi_link: parts.append(doi_link)\n",
    "\n",
    "    else:\n",
    "        venue = journal or conf or pub\n",
    "        if venue: parts.append(venue)\n",
    "        date = \" \".join([m for m in [month_disp, year] if m]).strip()\n",
    "        if date: parts.append(date)\n",
    "        if vol: parts.append(f\"vol. {vol}\")\n",
    "        if issue: parts.append(f\"no. {issue}\")\n",
    "        if pages_norm: parts.append(f\"pp. {pages_norm}\")\n",
    "        if doi_link: parts.append(doi_link)\n",
    "\n",
    "    state[\"formatted\"] = (\", \".join([p for p in parts if p]) + \".\").replace(\" ,\", \",\")\n",
    "    return state\n",
    "\n",
    "def to_csl_json(ex: Dict[str, Any], rtype: str) -> Dict[str, Any]:\n",
    "    typemap = {\n",
    "        \"journal article\": \"article-journal\",\n",
    "        \"conference paper\": \"paper-conference\",\n",
    "        \"book\": \"book\",\n",
    "        \"book chapter\": \"chapter\",\n",
    "        \"thesis\": \"thesis\",\n",
    "        \"technical report\": \"report\",\n",
    "        \"dataset\": \"dataset\",\n",
    "        \"standard\": \"standard\",\n",
    "        \"software\": \"software\",\n",
    "        \"preprint\": \"article\",\n",
    "    }\n",
    "    t = typemap.get(rtype, \"article\")\n",
    "\n",
    "    authors_list = authors_to_list(ex.get(\"authors\"))\n",
    "    authors = []\n",
    "    for a in authors_list:\n",
    "        parts = a.split()\n",
    "        family = parts[-1] if parts else a\n",
    "        given = \" \".join(parts[:-1]) if len(parts) > 1 else \"\"\n",
    "        authors.append({\"family\": safe_str(family), \"given\": safe_str(given)})\n",
    "\n",
    "    year_raw = ex.get(\"year\")\n",
    "    month_raw = normalize_month_field(ex.get(\"month\") or \"\")\n",
    "    issued = None\n",
    "    try:\n",
    "        y = int(year_raw) if safe_str(year_raw).isdigit() else None\n",
    "        if y is not None:\n",
    "            if month_raw and month_raw.isdigit():\n",
    "                issued = {\"date-parts\": [[y, int(month_raw)]]}\n",
    "            else:\n",
    "                issued = {\"date-parts\": [[y]]}\n",
    "    except Exception:\n",
    "        issued = None\n",
    "\n",
    "    doi_link = format_doi_link(ex.get(\"doi\") or \"\")\n",
    "    csl = {\n",
    "        \"type\": t,\n",
    "        \"title\": safe_str(ex.get(\"title\")),\n",
    "        \"author\": authors if authors else None,\n",
    "        \"container-title\": safe_str(ex.get(\"journal_name\") or ex.get(\"conference_name\")),\n",
    "        \"container-title-short\": safe_str(ex.get(\"journal_abbrev\")) or None,\n",
    "        \"volume\": safe_str(ex.get(\"volume\")),\n",
    "        \"issue\": safe_str(ex.get(\"issue\")),\n",
    "        \"page\": safe_str(ex.get(\"pages\")),\n",
    "        \"DOI\": safe_str(ex.get(\"doi\")),\n",
    "        \"URL\": doi_link or safe_str(ex.get(\"url\")),\n",
    "        \"publisher\": safe_str(ex.get(\"publisher\")),\n",
    "        \"issued\": issued,\n",
    "    }\n",
    "    return {k: v for k, v in csl.items() if v}\n",
    "\n",
    "def to_bibtex(ex: Dict[str, Any], rtype: str) -> str:\n",
    "    def bibtex_escape(s: str) -> str:\n",
    "        return (\n",
    "            s.replace(\"\\\\\", \"\\\\textbackslash{}\")\n",
    "             .replace(\"{\", \"\\\\{\").replace(\"}\", \"\\\\}\")\n",
    "             .replace(\"&\", \"\\\\&\").replace(\"%\", \"\\\\%\")\n",
    "             .replace(\"$\", \"\\\\$\").replace(\"#\", \"\\\\#\").replace(\"_\", \"\\\\_\")\n",
    "        )\n",
    "\n",
    "    authors_list = authors_to_list(ex.get(\"authors\"))\n",
    "    first_author_last = \"\"\n",
    "    if authors_list:\n",
    "        parts = authors_list[0].split()\n",
    "        first_author_last = parts[-1] if parts else authors_list[0]\n",
    "\n",
    "    year_str = safe_str(ex.get(\"year\"))\n",
    "    fa_key = re.sub(r\"[^A-Za-z0-9]+\", \"\", safe_str(first_author_last)) or \"ref\"\n",
    "    yr_key = re.sub(r\"[^0-9]+\", \"\", year_str)\n",
    "    if not yr_key:\n",
    "        basis = safe_str(ex.get(\"doi\")) or safe_str(ex.get(\"title\"))\n",
    "        h = hashlib.sha1(basis.encode(\"utf-8\", \"ignore\")).hexdigest()[:6] if basis else \"000000\"\n",
    "        yr_key = h\n",
    "    key = f\"{fa_key}{yr_key}\"\n",
    "\n",
    "    entry_type = {\n",
    "        \"journal article\": \"article\",\n",
    "        \"conference paper\": \"inproceedings\",\n",
    "        \"book\": \"book\",\n",
    "        \"book chapter\": \"incollection\",\n",
    "        \"thesis\": \"phdthesis\",\n",
    "        \"technical report\": \"techreport\",\n",
    "        \"dataset\": \"misc\",\n",
    "        \"standard\": \"misc\",\n",
    "        \"software\": \"misc\",\n",
    "        \"preprint\": \"misc\",\n",
    "    }.get(rtype, \"misc\")\n",
    "\n",
    "    A = \" and \".join(authors_list)\n",
    "    title = safe_str(ex.get(\"title\"))\n",
    "    journal = safe_str(ex.get(\"journal_name\"))\n",
    "    conf = safe_str(ex.get(\"conference_name\"))\n",
    "    volume = safe_str(ex.get(\"volume\"))\n",
    "    number = safe_str(ex.get(\"issue\"))\n",
    "    pages = safe_str(ex.get(\"pages\"))\n",
    "    year = safe_str(ex.get(\"year\"))\n",
    "    doi = safe_str(ex.get(\"doi\"))\n",
    "    publisher = safe_str(ex.get(\"publisher\"))\n",
    "    isbn = safe_str(ex.get(\"isbn\"))\n",
    "    url_or_doi = format_doi_link(doi) if doi else safe_str(ex.get(\"url\"))\n",
    "\n",
    "    fields: List[Tuple[str, str]] = []\n",
    "    if entry_type == \"article\":\n",
    "        fields += [(\"author\", A), (\"title\", title), (\"journal\", journal),\n",
    "                   (\"volume\", volume), (\"number\", number), (\"pages\", pages),\n",
    "                   (\"year\", year)]\n",
    "        if url_or_doi: fields.append((\"url\", url_or_doi))\n",
    "        if doi and not url_or_doi: fields.append((\"doi\", doi))\n",
    "    elif entry_type == \"inproceedings\":\n",
    "        fields += [(\"author\", A), (\"title\", title), (\"booktitle\", conf or journal),\n",
    "                   (\"pages\", pages), (\"year\", year)]\n",
    "        if url_or_doi: fields.append((\"url\", url_or_doi))\n",
    "    elif entry_type == \"book\":\n",
    "        fields += [(\"author\", A), (\"title\", title), (\"publisher\", publisher),\n",
    "                   (\"year\", year), (\"isbn\", isbn)]\n",
    "        if url_or_doi: fields.append((\"url\", url_or_doi))\n",
    "    elif entry_type == \"incollection\":\n",
    "        fields += [(\"author\", A), (\"title\", title), (\"booktitle\", conf or journal),\n",
    "                   (\"pages\", pages), (\"publisher\", publisher), (\"year\", year)]\n",
    "        if url_or_doi: fields.append((\"url\", url_or_doi))\n",
    "    elif entry_type == \"phdthesis\":\n",
    "        fields += [(\"author\", A), (\"title\", title), (\"school\", publisher or conf or journal),\n",
    "                   (\"year\", year)]\n",
    "        if url_or_doi: fields.append((\"url\", url_or_doi))\n",
    "    elif entry_type == \"techreport\":\n",
    "        fields += [(\"author\", A), (\"title\", title), (\"institution\", publisher or conf or journal),\n",
    "                   (\"year\", year)]\n",
    "        if url_or_doi: fields.append((\"url\", url_or_doi))\n",
    "    else:  # misc\n",
    "        fields += [(\"author\", A), (\"title\", title), (\"howpublished\", conf or journal or publisher),\n",
    "                   (\"year\", year)]\n",
    "        if url_or_doi: fields.append((\"url\", url_or_doi))\n",
    "\n",
    "    fields = [(k, bibtex_escape(v)) for k, v in fields if v]\n",
    "    body = \",\\n  \".join([f\"{k} = {{{v}}}\" for k, v in fields])\n",
    "    return f\"@{entry_type}{{{key},\\n  {body}\\n}}\"\n",
    "\n",
    "def node_build_exports(state: PipelineState) -> PipelineState:\n",
    "    ex = state[\"extracted\"]; rtype = (state[\"type\"] or \"other\").lower()\n",
    "    state[\"csl_json\"] = to_csl_json(ex, rtype)\n",
    "    state[\"bibtex\"] = to_bibtex(ex, rtype)\n",
    "    return state\n",
    "\n",
    "def node_build_report(state: PipelineState) -> PipelineState:\n",
    "    changes = state.get(\"corrections\") or []\n",
    "    ver = state.get(\"verification\") or {}\n",
    "    lines = []\n",
    "    if not changes:\n",
    "        lines.append(\"No corrections were necessary; reference matched authoritative sources.\")\n",
    "    else:\n",
    "        lines.append(\"Corrections (field: old → new):\")\n",
    "        for f, old, new in changes:\n",
    "            lines.append(f\"- {f}: '{old}' → '{new}'\")\n",
    "    failed = [k for k, v in ver.items() if not v]\n",
    "    if failed:\n",
    "        lines.append(\"Fields still needing attention: \" + \", \".join(sorted(failed)))\n",
    "    else:\n",
    "        lines.append(\"All verification checks passed after corrections.\")\n",
    "    state[\"report\"] = \"\\n\".join(lines)\n",
    "    return state\n",
    "\n",
    "# --- Cleanup node to close clients\n",
    "async def node_cleanup_async(state: PipelineState) -> PipelineState:\n",
    "    try:\n",
    "        if state.get(\"_http\") is not None:\n",
    "            await state[\"_http\"].aclose()\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        llm = state.get(\"_llm\")\n",
    "        if llm and llm.provider == \"ollama\" and getattr(llm, \"_client\", None) is not None:\n",
    "            await llm._client.aclose()\n",
    "    except Exception:\n",
    "        pass\n",
    "    return state\n",
    "\n",
    "# ============================ Graph Build & Routing ============================\n",
    "\n",
    "def should_exit(state: PipelineState) -> bool:\n",
    "    cfg = state.get(\"_cfg\") or CFG\n",
    "    if state.get(\"_loop_detected\"): return True\n",
    "    if (state.get(\"hops\") or 0) >= cfg.max_hops: return True\n",
    "    if (state.get(\"attempts\") or 0) >= cfg.max_correction_rounds: return True\n",
    "    if (state.get(\"_stagnation\") or 0) >= cfg.stagnation_patience: return True\n",
    "    if not state.get(\"_made_changes_last_cycle\") and (state.get(\"_stagnation\",0) >= 1):\n",
    "        return True\n",
    "    ver = state.get(\"verification\") or {}\n",
    "    return bool(ver) and all(ver.values())\n",
    "\n",
    "def build_graph(cfg: PipelineConfig = CFG) -> StateGraph:\n",
    "    g = StateGraph(PipelineState)\n",
    "\n",
    "    # nodes\n",
    "    g.add_node(\"InitRuntime\", _init_runtime)\n",
    "    g.add_node(\"DetectType\", node_detect_type_async)\n",
    "    g.add_node(\"ParseExtract\", node_parse_extract_async)\n",
    "    g.add_node(\"MultiSourceLookup\", node_multisource_lookup_async)\n",
    "    g.add_node(\"SelectBest\", node_select_best)\n",
    "    g.add_node(\"ReconcileType\", node_reconcile_type)\n",
    "    g.add_node(\"VerifyAgents\", node_verify_agents)\n",
    "\n",
    "    g.add_node(\"ApplyCorrections\", node_apply_corrections)\n",
    "    g.add_node(\"LLMCorrect\", node_llm_correct_async)\n",
    "    g.add_node(\"EnrichFromBest\", node_enrich_from_best)\n",
    "\n",
    "    g.add_node(\"FormatReference\", node_format_reference)\n",
    "    g.add_node(\"BuildExports\", node_build_exports)\n",
    "    g.add_node(\"BuildReport\", node_build_report)\n",
    "    g.add_node(\"Cleanup\", node_cleanup_async)\n",
    "\n",
    "    # linear backbone\n",
    "    g.add_edge(START, \"InitRuntime\")\n",
    "    g.add_edge(\"InitRuntime\", \"DetectType\")\n",
    "    g.add_edge(\"DetectType\", \"ParseExtract\")\n",
    "    g.add_edge(\"ParseExtract\", \"MultiSourceLookup\")\n",
    "    g.add_edge(\"MultiSourceLookup\", \"SelectBest\")\n",
    "    g.add_edge(\"SelectBest\", \"ReconcileType\")\n",
    "    g.add_edge(\"ReconcileType\", \"VerifyAgents\")\n",
    "\n",
    "    # conditional: if done -> format; else repair loop\n",
    "    def route_after_verify(state: PipelineState) -> str:\n",
    "        decision = \"FormatReference\" if should_exit(state) else \"ApplyCorrections\"\n",
    "        _jlog(event=\"route_after_verify\", decision=decision, ver_score=state.get(\"_ver_score\"))\n",
    "        return decision\n",
    "    g.add_conditional_edges(\"VerifyAgents\", route_after_verify, {\n",
    "        \"FormatReference\": \"FormatReference\",\n",
    "        \"ApplyCorrections\": \"ApplyCorrections\",\n",
    "    })\n",
    "\n",
    "    # repair path (bounded by guards)\n",
    "    g.add_edge(\"ApplyCorrections\", \"LLMCorrect\")\n",
    "    g.add_edge(\"LLMCorrect\", \"EnrichFromBest\")\n",
    "    g.add_edge(\"EnrichFromBest\", \"MultiSourceLookup\")\n",
    "\n",
    "    # terminal leg\n",
    "    g.add_edge(\"FormatReference\", \"BuildExports\")\n",
    "    g.add_edge(\"BuildExports\", \"BuildReport\")\n",
    "    g.add_edge(\"BuildReport\", \"Cleanup\")\n",
    "    g.add_edge(\"Cleanup\", END)\n",
    "\n",
    "    return g\n",
    "\n",
    "# ============================ Mermaid Rendering ============================\n",
    "\n",
    "MERMAID_DAG = r'''flowchart TD\n",
    "A[Init Runtime] --> B[Detect Type (Heuristics + LLM)]\n",
    "B --> C[Parse & Extract (LLM-first)]\n",
    "C --> D[Fetch Candidates (Crossref/OpenAlex/S2/PubMed/arXiv)]\n",
    "D --> E[Select Best (Consensus Scoring)]\n",
    "E --> T[Reconcile Type]\n",
    "T --> F[Verification Agents (Threaded) + Progress Metrics]\n",
    "F -->|repair| G[Apply Corrections (Authority + Agents)]\n",
    "G --> I[LLM Correction (JSON-only)]\n",
    "I --> X[Enrich From Best]\n",
    "X --> D2[Re-Fetch Candidates]\n",
    "D2 --> E2[Re-Select Best]\n",
    "E2 --> T2[Reconcile Type]\n",
    "T2 --> F2[Re-Verify + loop/stagnation guards]\n",
    "F -->|exit| H[Format IEEE]\n",
    "H --> J[Build CSL-JSON & BibTeX]\n",
    "J --> R[Human Report]\n",
    "style H fill:#e0f7fa,stroke:#006064,stroke-width:1px\n",
    "style R fill:#f1f8e9,stroke:#33691e,stroke-width:1px\n",
    "style D fill:#fff3e0,stroke:#e65100,stroke-width:1px\n",
    "style F fill:#ede7f6,stroke:#4527a0,stroke-width:1px\n",
    "style T fill:#e8f5e9,stroke:#2e7d32,stroke-width:1px\n",
    "'''\n",
    "\n",
    "def show_mermaid_inline(mermaid_code: str) -> None:\n",
    "    html = f\"\"\"\n",
    "    <div class=\"mermaid\">\n",
    "    {mermaid_code}\n",
    "    </div>\n",
    "    <script>\n",
    "      (function() {{\n",
    "        function init() {{\n",
    "          mermaid.initialize({{startOnLoad:true}});\n",
    "        }}\n",
    "        if (!window.mermaid) {{\n",
    "          var s = document.createElement('script');\n",
    "          s.src = 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js';\n",
    "          s.onload = init;\n",
    "          document.head.appendChild(s);\n",
    "        }} else {{\n",
    "          init();\n",
    "        }}\n",
    "      }})();\n",
    "    </script>\n",
    "    \"\"\"\n",
    "    display(HTML(html))\n",
    "\n",
    "def show_mermaid_kroki(mermaid_code: str) -> None:\n",
    "    display(Markdown(f\"```mermaid\\n{mermaid_code}\\n```\"))\n",
    "    if httpx is None:\n",
    "        return\n",
    "    try:\n",
    "        r = httpx.post(\"https://kroki.io/mermaid/svg\", content=mermaid_code.encode(\"utf-8\"), timeout=10.0,\n",
    "                       headers={\"Content-Type\": \"text/plain\",\"User-Agent\":DEFAULT_UA})\n",
    "        _jlog(event=\"http_request\", url=\"https://kroki.io/mermaid/svg\", code=r.status_code)\n",
    "        if r.status_code == 200:\n",
    "            display(SVG(r.content))\n",
    "        else:\n",
    "            if r.status_code == 400:\n",
    "                print(\"Kroki 400: Mermaid syntax or styling may be invalid. Try removing `style` lines or simplify labels.\")\n",
    "            else:\n",
    "                print(f\"Kroki error: {r.status_code}: {r.text[:200]}\")\n",
    "    except Exception as e:\n",
    "        _jlog(event=\"kroki_render_failed\", error=str(e))\n",
    "\n",
    "# ============================ Build/compile ============================\n",
    "\n",
    "graph = build_graph(CFG)\n",
    "compiled = graph.compile()\n",
    "\n",
    "# Show diagram (respect env)\n",
    "if os.environ.get(\"NO_EXTERNAL_JS\",\"0\") == \"1\":\n",
    "    show_mermaid_kroki(MERMAID_DAG)\n",
    "else:\n",
    "    show_mermaid_inline(MERMAID_DAG)\n",
    "    show_mermaid_kroki(MERMAID_DAG)\n",
    "\n",
    "# ============================ Runner & Examples ============================\n",
    "\n",
    "examples = [\n",
    "    'F.-J. Lin, P.-H. Shen, S.-L. Yang, and P. H. Chou, “Recurrent radial basis function network-based fuzzy neural network control for permanent-magnet linear synchronous motor servo drive,” IEEE Trans. on Magnetics, vol. 42, no. 11, Nov. 2006.',\n",
    "    'P. S. Sastry, G. Santharam, and K. P. Unnikrishnan, “Memory neuron networks for identification and control of dynamical systems,” IEEE Trans. Neural Netw., vol. 5, no. 2, pp. 306–319, May 1994, doi: 10.1109/72.279193.',\n",
    "    'K. C. Apaza and J. M. López, “The non-linear relationship between carbon dioxide emissions, financial development and energy consumption in developing European and Central Asian economies,” Environ. Sci. Pollut. Res. Int., vol. 28, no. 44, pp. 63330–63345, Jul. 2021, doi: 10.1007/s11356-021-15225-2.',\n",
    "    'A. Vaswani et al., \"Attention Is All You Need\", in NeurIPS, 2017.'\n",
    "]\n",
    "\n",
    "async def run_one(reference: str, recursion_limit: Optional[int] = None) -> Dict[str, Any]:\n",
    "    state: PipelineState = {\"reference\": reference}\n",
    "    out = await compiled.ainvoke(state, config={\"recursion_limit\": recursion_limit or CFG.recursion_limit})\n",
    "    return out\n",
    "\n",
    "async def run_examples():\n",
    "    for ref in examples:\n",
    "        result = await run_one(ref)\n",
    "        print(\"\\n=== Result ===\")\n",
    "        print(\"Resolved Type:\", result.get(\"type\"))\n",
    "        print(\"Formatted:\", result.get(\"formatted\"))\n",
    "        print(\"Verification OK:\", all((result.get(\"verification\") or {}).values()) if result.get(\"verification\") else False)\n",
    "        print(\"Report:\\n\", result.get(\"report\"))\n",
    "        print(\"CSL-JSON:\", json.dumps(result.get(\"csl_json\"), indent=2, ensure_ascii=False))\n",
    "        print(\"BibTeX:\\n\", result.get(\"bibtex\"))\n",
    "\n",
    "ENV_SAMPLE = \"\"\"\n",
    "# One of these providers is enough:\n",
    "OPENAI_API_KEY=sk-...\n",
    "OPENAI_MODEL=gpt-4o-mini\n",
    "\n",
    "# OR Azure OpenAI:\n",
    "AZURE_OPENAI_API_KEY=...\n",
    "AZURE_OPENAI_ENDPOINT=https://<your-resource>.openai.azure.com\n",
    "AZURE_OPENAI_DEPLOYMENT=gpt-4o-base\n",
    "OPENAI_API_VERSION=2024-06-01\n",
    "\n",
    "# OR Anthropic:\n",
    "ANTHROPIC_API_KEY=...\n",
    "ANTHROPIC_MODEL=claude-3-5-sonnet-20240620\n",
    "\n",
    "# OR Ollama (local):\n",
    "OLLAMA_BASE_URL=http://localhost:11434\n",
    "OLLAMA_MODEL=llama3.2\n",
    "\n",
    "# Optional tuning:\n",
    "IEEE_REF_TIMEOUT=12\n",
    "IEEE_REF_CONCURRENCY=8\n",
    "IEEE_REF_CACHE_TTL=3600\n",
    "IEEE_REF_MAX_CORR=3\n",
    "IEEE_REF_MAX_HOPS=12\n",
    "IEEE_REF_STAGNATION=2\n",
    "IEEE_REF_AGENT_THREADS=6\n",
    "IEEE_REF_LOG_LEVEL=INFO\n",
    "IEEE_REF_RECURSION_LIMIT=60\n",
    "\n",
    "# Optional:\n",
    "SEMANTIC_SCHOLAR_API_KEY=...\n",
    "\"\"\".strip()\n",
    "\n",
    "print(\"\\nTip: create a .env with the keys you use. Example:\\n\", ENV_SAMPLE)\n",
    "\n",
    "# Run sample set\n",
    "await run_examples()\n",
    "\n",
    "# OR Ollama (local):\n",
    "OLLAMA_BASE_URL=http://localhost:11434\n",
    "OLLAMA_MODEL=llama3.2\n",
    "\n",
    "# Optional tuning:\n",
    "IEEE_REF_TIMEOUT=12\n",
    "IEEE_REF_CONCURRENCY=8\n",
    "IEEE_REF_CACHE_TTL=3600\n",
    "IEEE_REF_MAX_CORR=3\n",
    "IEEE_REF_MAX_HOPS=12\n",
    "IEEE_REF_STAGNATION=2\n",
    "IEEE_REF_AGENT_THREADS=6\n",
    "IEEE_REF_LOG_LEVEL=INFO\n",
    "IEEE_REF_RECURSION_LIMIT=60\n",
    "\n",
    "# Optional:\n",
    "SEMANTIC_SCHOLAR_API_KEY=...\n",
    "\"\"\".strip()\n",
    "\n",
    "print(\"\\nTip: create a .env with the keys you use. Example:\\n\", ENV_SAMPLE)\n",
    "\n",
    "# Run sample set\n",
    "await run_examples()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f466aca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Agentic IEEE Reference Pipeline (LangGraph) — Single Jupyter Cell (Production Grade)\n",
    "# # - Robust LLM adapter (OpenAI/Azure/Anthropic/Ollama), JSON-only\n",
    "# # - Async lookups: Crossref/OpenAlex/Semantic Scholar/PubMed/arXiv\n",
    "# # - Verification agents + repair loop with loop/stagnation guards\n",
    "# # - Mermaid graph rendering via inline JS (no extension) + Kroki SVG fallback\n",
    "# # - Exports: IEEE string, CSL-JSON, BibTeX\n",
    "# # - Safe handling for rate limits / 429s and resource cleanup\n",
    "\n",
    "# import os, re, json, asyncio, logging, textwrap, hashlib, time, sys\n",
    "# from dataclasses import dataclass\n",
    "# from typing import Any, Dict, List, Optional, Tuple\n",
    "# from typing_extensions import TypedDict\n",
    "\n",
    "# # ---------- Optional deps ----------\n",
    "# try:\n",
    "#     from dotenv import load_dotenv; load_dotenv()\n",
    "# except Exception:\n",
    "#     pass\n",
    "\n",
    "# try:\n",
    "#     import httpx\n",
    "# except Exception:\n",
    "#     httpx = None\n",
    "\n",
    "# try:\n",
    "#     from cachetools import TTLCache\n",
    "#     CACHE_AVAILABLE = True\n",
    "# except Exception:\n",
    "#     TTLCache = None\n",
    "#     CACHE_AVAILABLE = False\n",
    "\n",
    "# try:\n",
    "#     from rapidfuzz import fuzz\n",
    "#     RF_AVAILABLE = True\n",
    "# except Exception:\n",
    "#     fuzz = None\n",
    "#     RF_AVAILABLE = False\n",
    "\n",
    "# from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "# from IPython.display import display, Markdown, Image, SVG, HTML\n",
    "\n",
    "# # LangGraph\n",
    "# from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# # ============================ Configuration & Logging ============================\n",
    "\n",
    "# @dataclass\n",
    "# class PipelineConfig:\n",
    "#     timeout_s: float = float(os.getenv(\"IEEE_REF_TIMEOUT\", \"12\"))\n",
    "#     concurrency: int = int(os.getenv(\"IEEE_REF_CONCURRENCY\", \"8\"))\n",
    "#     cache_ttl_s: int = int(os.getenv(\"IEEE_REF_CACHE_TTL\", \"3600\"))\n",
    "#     max_correction_rounds: int = int(os.getenv(\"IEEE_REF_MAX_CORR\", \"3\"))\n",
    "#     max_hops: int = int(os.getenv(\"IEEE_REF_MAX_HOPS\", \"12\"))\n",
    "#     stagnation_patience: int = int(os.getenv(\"IEEE_REF_STAGNATION\", \"2\"))\n",
    "#     llm_provider: str = os.getenv(\"IEEE_REF_LLM\", \"auto\")  # auto|openai|azure|anthropic|ollama|dummy\n",
    "#     openai_model: str = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n",
    "#     ollama_model: str = os.getenv(\"OLLAMA_MODEL\", \"llama3.2\")\n",
    "#     ollama_base: str = os.getenv(\"OLLAMA_BASE_URL\", os.getenv(\"OLLAMA_HOST\", \"http://localhost:11434\"))\n",
    "#     agent_threads: int = int(os.getenv(\"IEEE_REF_AGENT_THREADS\", \"6\"))\n",
    "#     recursion_limit: int = int(os.getenv(\"IEEE_REF_RECURSION_LIMIT\", \"60\"))  # safer default\n",
    "\n",
    "# LOG_LEVEL = os.getenv(\"IEEE_REF_LOG_LEVEL\", \"INFO\").upper()\n",
    "# logging.basicConfig(level=getattr(logging, LOG_LEVEL, logging.INFO),\n",
    "#                     format=\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\")\n",
    "# logger = logging.getLogger(\"ieee-ref-langgraph\")\n",
    "# CFG = PipelineConfig()\n",
    "\n",
    "# # ============================ Utility Functions ============================\n",
    "\n",
    "# def safe_json_load(s: Any) -> Optional[Dict[str, Any]]:\n",
    "#     if s is None:\n",
    "#         return None\n",
    "#     if isinstance(s, dict):\n",
    "#         return s\n",
    "#     try:\n",
    "#         sx = s.decode(\"utf-8\", \"ignore\") if isinstance(s, (bytes, bytearray)) else str(s)\n",
    "#     except Exception:\n",
    "#         sx = str(s)\n",
    "#     sx = sx.strip()\n",
    "#     try:\n",
    "#         if sx.startswith(\"{\"):\n",
    "#             return json.loads(sx)\n",
    "#     except Exception:\n",
    "#         pass\n",
    "#     # brace-balanced extraction\n",
    "#     i, n = 0, len(sx)\n",
    "#     while i < n and sx[i] != \"{\": i += 1\n",
    "#     if i >= n: return None\n",
    "#     stack = 0; in_str = False; esc = False; start = None\n",
    "#     for j in range(i, n):\n",
    "#         ch = sx[j]\n",
    "#         if in_str:\n",
    "#             if esc: esc = False\n",
    "#             elif ch == \"\\\\\": esc = True\n",
    "#             elif ch == '\"': in_str = False\n",
    "#         else:\n",
    "#             if ch == '\"': in_str = True\n",
    "#             elif ch == \"{\":\n",
    "#                 if stack == 0: start = j\n",
    "#                 stack += 1\n",
    "#             elif ch == \"}\":\n",
    "#                 stack -= 1\n",
    "#                 if stack == 0 and start is not None:\n",
    "#                     candidate = sx[start:j+1]\n",
    "#                     try: return json.loads(candidate)\n",
    "#                     except Exception: start = None\n",
    "#     return None\n",
    "\n",
    "# def normalize_text(x: Any) -> str:\n",
    "#     if x is None: return \"\"\n",
    "#     s = str(x).strip()\n",
    "#     s = re.sub(r\"\\s+\", \" \", s)\n",
    "#     return s\n",
    "\n",
    "# def norm_for_compare(x: Any) -> str:\n",
    "#     s = normalize_text(x).lower()\n",
    "#     s = re.sub(r\"[^\\w\\s]\", \" \", s)\n",
    "#     s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "#     return s\n",
    "\n",
    "# def token_similarity(a: str, b: str) -> float:\n",
    "#     a = norm_for_compare(a); b = norm_for_compare(b)\n",
    "#     if not a or not b: return 0.0\n",
    "#     if RF_AVAILABLE and fuzz is not None:\n",
    "#         return fuzz.token_sort_ratio(a, b) / 100.0\n",
    "#     sa, sb = set(a.split()), set(b.split())\n",
    "#     inter = sa & sb\n",
    "#     union = sa | sb\n",
    "#     return len(inter) / max(1, len(union))\n",
    "\n",
    "# def authors_to_list(a: Any) -> List[str]:\n",
    "#     if not a: return []\n",
    "#     if isinstance(a, list):\n",
    "#         return [normalize_text(x) for x in a if normalize_text(x)]\n",
    "#     parts = re.split(r\",\\s*|\\s+&\\s+| and \", str(a))\n",
    "#     return [normalize_text(p) for p in parts if normalize_text(p)]\n",
    "\n",
    "# def format_author_ieee(name: str) -> str:\n",
    "#     n = normalize_text(name)\n",
    "#     if not n: return \"\"\n",
    "#     if \",\" in n:\n",
    "#         last, given = [p.strip() for p in n.split(\",\", 1)]\n",
    "#     else:\n",
    "#         toks = n.split()\n",
    "#         if len(toks) == 1: return toks[0]\n",
    "#         last = toks[-1]; given = \" \".join(toks[:-1])\n",
    "#     initials = []\n",
    "#     for part in re.split(r\"\\s+\", given):\n",
    "#         if not part: continue\n",
    "#         if re.match(r\"^[A-Za-z]\\.$\", part):\n",
    "#             initials.append(part)\n",
    "#         else:\n",
    "#             initials.append(part[0].upper() + \".\")\n",
    "#     return ((\" \".join(initials) + \" \" + last).strip()) if initials else n\n",
    "\n",
    "# def format_authors_ieee_list(auths: List[str]) -> str:\n",
    "#     items = [format_author_ieee(a) for a in auths if a]\n",
    "#     if not items: return \"\"\n",
    "#     if len(items) == 1: return items[0]\n",
    "#     if len(items) == 2: return f\"{items[0]} and {items[1]}\"\n",
    "#     return \", \".join(items[:-1]) + \", and \" + items[-1]\n",
    "\n",
    "# def sentence_case(title: str) -> str:\n",
    "#     t = normalize_text(title)\n",
    "#     if not t: return \"\"\n",
    "#     if t.isupper(): t = t.lower()\n",
    "#     tokens = t.split(); out = []\n",
    "#     for i, tok in enumerate(tokens):\n",
    "#         if tok.isupper() and len(tok) > 1:\n",
    "#             out.append(tok)\n",
    "#         else:\n",
    "#             out.append(tok[:1].upper() + tok[1:].lower() if i == 0 else tok.lower())\n",
    "#     res = \" \".join(out)\n",
    "#     res = re.sub(r\"\\bieee\\b\", \"IEEE\", res, flags=re.I)\n",
    "#     return res\n",
    "\n",
    "# def heuristic_abbrev(fullname: str) -> str:\n",
    "#     fullname = normalize_text(fullname)\n",
    "#     if not fullname: return \"\"\n",
    "#     tokens = [t for t in re.split(r\"[\\s,]+\", fullname) if t.lower() not in {\"on\",\"of\",\"and\",\"the\",\"in\",\"for\",\"to\"}]\n",
    "#     out = []\n",
    "#     for t in tokens[:8]:\n",
    "#         if len(t) <= 4 and t.isupper(): out.append(t)\n",
    "#         elif len(t) <= 3: out.append(t.capitalize() + \".\")\n",
    "#         else: out.append(t[:4].capitalize() + \".\")\n",
    "#     return \" \".join(out)\n",
    "\n",
    "# def ensure_doi_prefix(doi: str) -> str:\n",
    "#     d = normalize_text(doi)\n",
    "#     if not d: return \"\"\n",
    "#     return d if d.lower().startswith(\"doi:\") else f\"doi:{d}\"\n",
    "\n",
    "# MONTHS_NAME = {\n",
    "#     \"1\":\"Jan\",\"2\":\"Feb\",\"3\":\"Mar\",\"4\":\"Apr\",\"5\":\"May\",\"6\":\"Jun\",\n",
    "#     \"7\":\"Jul\",\"8\":\"Aug\",\"9\":\"Sep\",\"10\":\"Oct\",\"11\":\"Nov\",\"12\":\"Dec\",\n",
    "# }\n",
    "# def normalize_month_field(m: Any) -> str:\n",
    "#     s = normalize_text(m)\n",
    "#     if not s: return \"\"\n",
    "#     m_map = {\"jan\":\"1\",\"feb\":\"2\",\"mar\":\"3\",\"apr\":\"4\",\"may\":\"5\",\"jun\":\"6\",\"jul\":\"7\",\"aug\":\"8\",\"sep\":\"9\",\"sept\":\"9\",\"oct\":\"10\",\"nov\":\"11\",\"dec\":\"12\"}\n",
    "#     sl = s.strip(\". \").lower()\n",
    "#     if sl in m_map: return m_map[sl]\n",
    "#     if re.fullmatch(r\"0?[1-9]|1[0-2]\", sl): return str(int(sl))\n",
    "#     return s  # leave as-is (rare)\n",
    "\n",
    "# def fingerprint_state(ex: Dict[str, Any], best: Dict[str, Any], sugg: Dict[str, Any]) -> str:\n",
    "#     payload = json.dumps({\"ex\": ex, \"best\": best, \"sugg\": sugg}, sort_keys=True, ensure_ascii=False)\n",
    "#     return hashlib.sha256(payload.encode(\"utf-8\", \"ignore\")).hexdigest()\n",
    "\n",
    "# def safe_str(v: Any) -> str:\n",
    "#     try:\n",
    "#         if v is None:\n",
    "#             return \"\"\n",
    "#         return str(v).strip()\n",
    "#     except Exception:\n",
    "#         return \"\"\n",
    "\n",
    "# # ============================ LLM Adapter ============================\n",
    "\n",
    "# class LLMAdapter:\n",
    "#     \"\"\"LLM JSON-mode adapter supporting OpenAI, Azure OpenAI, Anthropic, Ollama; falls back to dummy.\"\"\"\n",
    "#     def __init__(self, cfg: PipelineConfig):\n",
    "#         self.cfg = cfg\n",
    "#         self.provider = self._auto_provider(cfg.llm_provider)\n",
    "#         self._client = None\n",
    "#         self._init_client()\n",
    "\n",
    "#     def _auto_provider(self, p: str) -> str:\n",
    "#         if p != \"auto\":\n",
    "#             return p\n",
    "#         if os.getenv(\"OPENAI_API_KEY\"): return \"openai\"\n",
    "#         if os.getenv(\"AZURE_OPENAI_API_KEY\"): return \"azure\"\n",
    "#         if os.getenv(\"ANTHROPIC_API_KEY\"): return \"anthropic\"\n",
    "#         if os.getenv(\"OLLAMA_BASE_URL\") or os.getenv(\"OLLAMA_HOST\"): return \"ollama\"\n",
    "#         return \"dummy\"\n",
    "\n",
    "#     def _init_client(self):\n",
    "#         prov = self.provider\n",
    "#         try:\n",
    "#             if prov == \"openai\":\n",
    "#                 from openai import OpenAI\n",
    "#                 base = os.getenv(\"OPENAI_API_BASE\")\n",
    "#                 self._client = OpenAI(base_url=base) if base else OpenAI()\n",
    "#             elif prov == \"azure\":\n",
    "#                 from openai import AzureOpenAI\n",
    "#                 ep = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "#                 ver = os.getenv(\"OPENAI_API_VERSION\", \"2024-06-01\")\n",
    "#                 if not ep:\n",
    "#                     raise RuntimeError(\"AZURE_OPENAI_ENDPOINT is not set\")\n",
    "#                 self._client = AzureOpenAI(azure_endpoint=ep, api_version=ver)\n",
    "#             elif prov == \"anthropic\":\n",
    "#                 import anthropic\n",
    "#                 self._client = anthropic.AsyncAnthropic()\n",
    "#             elif prov == \"ollama\" and httpx is not None:\n",
    "#                 base = os.getenv(\"OLLAMA_BASE_URL\") or os.getenv(\"OLLAMA_HOST\") or self.cfg.ollama_base\n",
    "#                 self._client = httpx.AsyncClient(base_url=base, timeout=self.cfg.timeout_s)\n",
    "#             else:\n",
    "#                 self._client = None\n",
    "#         except Exception as e:\n",
    "#             logger.warning(\"LLM init failed: %s\", e)\n",
    "#             self._client = None\n",
    "#             self.provider = \"dummy\"\n",
    "\n",
    "#     async def _openai_json(self, prompt: str) -> str:\n",
    "#         model = self.cfg.openai_model\n",
    "#         resp = self._client.chat.completions.create(\n",
    "#             model=model,\n",
    "#             messages=[{\"role\":\"system\",\"content\":\"Return STRICT JSON only. No prose.\"},\n",
    "#                       {\"role\":\"user\",\"content\":prompt}],\n",
    "#             temperature=0.1,\n",
    "#             response_format={\"type\":\"json_object\"},\n",
    "#         )\n",
    "#         return resp.choices[0].message.content\n",
    "\n",
    "#     async def _azure_json(self, prompt: str) -> str:\n",
    "#         deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\") or self.cfg.openai_model\n",
    "#         resp = self._client.chat.completions.create(\n",
    "#             model=deployment,\n",
    "#             messages=[{\"role\":\"system\",\"content\":\"Return STRICT JSON only. No prose.\"},\n",
    "#                       {\"role\":\"user\",\"content\":prompt}],\n",
    "#             temperature=0.1,\n",
    "#             response_format={\"type\":\"json_object\"},\n",
    "#         )\n",
    "#         return resp.choices[0].message.content\n",
    "\n",
    "#     async def _anthropic_json(self, prompt: str) -> str:\n",
    "#         msg = await self._client.messages.create(\n",
    "#             model=os.getenv(\"ANTHROPIC_MODEL\",\"claude-3-5-sonnet-20240620\"),\n",
    "#             system=\"Return STRICT JSON only. No prose.\",\n",
    "#             max_tokens=1024, temperature=0.1,\n",
    "#             messages=[{\"role\":\"user\",\"content\":prompt}],\n",
    "#         )\n",
    "#         texts = []\n",
    "#         for c in msg.content:\n",
    "#             if getattr(c, \"type\", None) == \"text\":\n",
    "#                 texts.append(c.text)\n",
    "#         return \"\".join(texts)\n",
    "\n",
    "#     async def _ollama_json(self, prompt: str) -> str:\n",
    "#         data = {\"model\": self.cfg.ollama_model, \"prompt\": \"Return STRICT JSON only.\\n\\n\" + prompt, \"stream\": False}\n",
    "#         r = await self._client.post(\"/api/generate\", json=data)\n",
    "#         r.raise_for_status()\n",
    "#         return r.json().get(\"response\",\"\")\n",
    "\n",
    "#     async def json(self, prompt: str) -> Dict[str, Any]:\n",
    "#         try:\n",
    "#             if self.provider == \"openai\":\n",
    "#                 raw = await self._openai_json(prompt)\n",
    "#             elif self.provider == \"azure\":\n",
    "#                 raw = await self._azure_json(prompt)\n",
    "#             elif self.provider == \"anthropic\":\n",
    "#                 raw = await self._anthropic_json(prompt)\n",
    "#             elif self.provider == \"ollama\":\n",
    "#                 raw = await self._ollama_json(prompt)\n",
    "#             else:\n",
    "#                 return {}\n",
    "#             return safe_json_load(raw) or {}\n",
    "#         except Exception as e:\n",
    "#             logger.warning(\"LLM json() failed: %s\", e)\n",
    "#             return {}\n",
    "\n",
    "# # ============================ Async Source Clients ============================\n",
    "\n",
    "# class SourceClient:\n",
    "#     NAME: str = \"base\"\n",
    "#     def __init__(self, cfg: PipelineConfig, client=None, limiter=None, cache=None):\n",
    "#         self.cfg = cfg\n",
    "#         self.client = client or (httpx.AsyncClient(timeout=cfg.timeout_s) if httpx is not None else None)\n",
    "#         self.limiter = limiter or asyncio.Semaphore(cfg.concurrency)\n",
    "#         self.cache = cache\n",
    "\n",
    "#     def _cache_get(self, key: str):\n",
    "#         if not self.cache: return None\n",
    "#         return self.cache.get((self.NAME, key))\n",
    "\n",
    "#     def _cache_set(self, key: str, val: Dict[str, Any]):\n",
    "#         if not self.cache: return\n",
    "#         self.cache[(self.NAME, key)] = val\n",
    "\n",
    "#     async def _get_json(self, url: str, params: Optional[Dict[str, Any]] = None, headers: Optional[Dict[str, str]] = None) -> Dict[str, Any]:\n",
    "#         if self.client is None:\n",
    "#             raise RuntimeError(\"HTTP client unavailable.\")\n",
    "#         async with self.limiter:\n",
    "#             r = await self.client.get(url, params=params, headers=headers)\n",
    "#             logger.info(\"HTTP Request: %s %s\", r.request.method, str(r.request.url))\n",
    "#             r.raise_for_status()\n",
    "#             ct = r.headers.get(\"content-type\",\"\")\n",
    "#             if \"application/json\" in ct or ct.startswith(\"text/json\"):\n",
    "#                 return r.json()\n",
    "#             return {\"_raw\": r.text}\n",
    "\n",
    "#     async def by_doi(self, doi: str) -> Optional[Dict[str, Any]]: raise NotImplementedError\n",
    "#     async def by_title(self, title: str) -> Optional[Dict[str, Any]]: raise NotImplementedError\n",
    "\n",
    "# class CrossrefClient(SourceClient):\n",
    "#     NAME = \"crossref\"; BASE_URL = \"https://api.crossref.org/works\"\n",
    "#     async def by_doi(self, doi: str) -> Optional[Dict[str, Any]]:\n",
    "#         key = f\"doi:{doi.lower().strip()}\"\n",
    "#         if (c := self._cache_get(key)): return c\n",
    "#         try:\n",
    "#             data = await self._get_json(f\"{self.BASE_URL}/{doi}\")\n",
    "#             msg = data.get(\"message\"); \n",
    "#             if msg: self._cache_set(key, msg)\n",
    "#             return msg\n",
    "#         except Exception: return None\n",
    "#     async def by_title(self, title: str) -> Optional[Dict[str, Any]]:\n",
    "#         key = f\"title:{norm_for_compare(title)}\"\n",
    "#         if (c := self._cache_get(key)): return c\n",
    "#         params = {\"query.title\": title, \"rows\": 1, \"select\":\"title,author,container-title,short-container-title,issued,DOI,page,volume,issue,published-print,published-online,type\"}\n",
    "#         try:\n",
    "#             data = await self._get_json(self.BASE_URL, params=params)\n",
    "#             items = data.get(\"message\", {}).get(\"items\", [])\n",
    "#             it = items[0] if items else None\n",
    "#             if it: self._cache_set(key, it)\n",
    "#             return it\n",
    "#         except Exception: return None\n",
    "\n",
    "# class OpenAlexClient(SourceClient):\n",
    "#     NAME = \"openalex\"; BASE_URL = \"https://api.openalex.org/works\"\n",
    "#     async def by_doi(self, doi: str) -> Optional[Dict[str, Any]]:\n",
    "#         key = f\"doi:{doi.lower().strip()}\"\n",
    "#         if (c := self._cache_get(key)): return c\n",
    "#         try:\n",
    "#             headers = {\"User-Agent\": \"ieee-ref-agent/1.0 (mailto:you@example.com)\"}\n",
    "#             data = await self._get_json(self.BASE_URL, params={\"filter\": f\"doi:{doi}\"}, headers=headers)\n",
    "#             items = data.get(\"results\", [])\n",
    "#             it = items[0] if items else None\n",
    "#             if it: self._cache_set(key, it)\n",
    "#             return it\n",
    "#         except Exception: return None\n",
    "#     async def by_title(self, title: str) -> Optional[Dict[str, Any]]:\n",
    "#         key = f\"title:{norm_for_compare(title)}\"\n",
    "#         if (c := self._cache_get(key)): return c\n",
    "#         try:\n",
    "#             headers = {\"User-Agent\": \"ieee-ref-agent/1.0 (mailto:you@example.com)\"}\n",
    "#             data = await self._get_json(self.BASE_URL, params={\"filter\": f\"title.search:{title}\", \"per-page\":1}, headers=headers)\n",
    "#             items = data.get(\"results\", [])\n",
    "#             it = items[0] if items else None\n",
    "#             if it: self._cache_set(key, it)\n",
    "#             return it\n",
    "#         except Exception: return None\n",
    "\n",
    "# class SemanticScholarClient(SourceClient):\n",
    "#     NAME = \"semanticscholar\"; BASE_URL = \"https://api.semanticscholar.org/graph/v1/paper\"\n",
    "#     async def by_doi(self, doi: str) -> Optional[Dict[str, Any]]:\n",
    "#         key = f\"doi:{doi.lower().strip()}\"\n",
    "#         if (c := self._cache_get(key)): return c\n",
    "#         try:\n",
    "#             data = await self._get_json(f\"{self.BASE_URL}/DOI:{doi}\", params={\"fields\":\"title,venue,year,authors,externalIds,publicationVenue,publicationTypes\"})\n",
    "#             if data and not data.get(\"error\"): self._cache_set(key, data)\n",
    "#             return data if data and not data.get(\"error\") else None\n",
    "#         except Exception: return None\n",
    "#     async def by_title(self, title: str) -> Optional[Dict[str, Any]]:\n",
    "#         key = f\"title:{norm_for_compare(title)}\"\n",
    "#         if (c := self._cache_get(key)): return c\n",
    "#         try:\n",
    "#             data = await self._get_json(f\"{self.BASE_URL}/search\", params={\"query\": title, \"limit\":1, \"fields\":\"title,venue,year,authors,externalIds,publicationVenue,publicationTypes\"})\n",
    "#             it = (data.get(\"data\") or [None])[0]\n",
    "#             if it: self._cache_set(key, it)\n",
    "#             return it\n",
    "#         except Exception: return None\n",
    "\n",
    "# class PubMedClient(SourceClient):\n",
    "#     NAME = \"pubmed\"\n",
    "#     ESEARCH = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n",
    "#     ESUMMARY = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi\"\n",
    "#     async def by_doi(self, doi: str) -> Optional[Dict[str, Any]]:\n",
    "#         # PubMed DOI mapping is non-trivial; keep None to avoid false positives\n",
    "#         return None\n",
    "#     async def by_title(self, title: str) -> Optional[Dict[str, Any]]:\n",
    "#         key = f\"title:{norm_for_compare(title)}\"\n",
    "#         if (c := self._cache_get(key)): return c\n",
    "#         try:\n",
    "#             d = await self._get_json(self.ESEARCH, params={\"db\":\"pubmed\",\"term\":title,\"retmode\":\"json\",\"retmax\":\"1\"})\n",
    "#             ids = d.get(\"esearchresult\", {}).get(\"idlist\", [])\n",
    "#             if not ids: return None\n",
    "#             pmid = ids[0]\n",
    "#             d2 = await self._get_json(self.ESUMMARY, params={\"db\":\"pubmed\",\"id\":pmid,\"retmode\":\"json\"})\n",
    "#             res = d2.get(\"result\", {}).get(pmid)\n",
    "#             if res: self._cache_set(key, res)\n",
    "#             return res\n",
    "#         except Exception: return None\n",
    "\n",
    "# class ArxivClient(SourceClient):\n",
    "#     NAME = \"arxiv\"; BASE_URL = \"https://export.arXiv.org/api/query\"\n",
    "#     async def by_doi(self, doi: str) -> Optional[Dict[str, Any]]:\n",
    "#         return None\n",
    "#     async def by_title(self, title: str) -> Optional[Dict[str, Any]]:\n",
    "#         try:\n",
    "#             if self.client is None:\n",
    "#                 return None\n",
    "#             async with self.limiter:\n",
    "#                 r = await self.client.get(self.BASE_URL, params={\"search_query\": f\"ti:\\\"{title}\\\"\", \"start\":0, \"max_results\":1}, headers={\"Accept\":\"application/atom+xml\"})\n",
    "#                 logger.info(\"HTTP Request: %s %s\", r.request.method, str(r.request.url))\n",
    "#                 r.raise_for_status()\n",
    "#                 xml = r.text\n",
    "#                 tmatch = re.search(r\"<title>(.*?)</title>\", xml, flags=re.DOTALL)\n",
    "#                 if not tmatch: return None\n",
    "#                 title0 = normalize_text(re.sub(r\"\\s+\", \" \", tmatch.group(1)))\n",
    "#                 auths = [normalize_text(a) for a in re.findall(r\"<name>(.*?)</name>\", xml)]\n",
    "#                 ymatch = re.search(r\"<published>(\\d{4})-\", xml)\n",
    "#                 year0 = ymatch.group(1) if ymatch else \"\"\n",
    "#                 return {\"title\": title0, \"authors\": auths, \"journal_name\":\"arXiv\", \"year\": year0, \"doi\":\"\"}\n",
    "#         except Exception:\n",
    "#             return None\n",
    "\n",
    "# # ============================ Candidate normalization & scoring ============================\n",
    "\n",
    "# def normalize_candidate(source: str, rec: Dict[str, Any]) -> Dict[str, Any]:\n",
    "#     out: Dict[str, Any] = {\"source\": source, \"raw\": rec}\n",
    "#     if source == \"crossref\":\n",
    "#         out[\"title\"] = normalize_text((rec.get(\"title\") or [\"\"])[0]) if rec.get(\"title\") else \"\"\n",
    "#         out[\"authors\"] = [normalize_text(f\"{a.get('given','')} {a.get('family','')}\".strip()) for a in rec.get(\"author\", [])] if rec.get(\"author\") else []\n",
    "#         out[\"journal_name\"] = normalize_text((rec.get(\"container-title\") or [\"\"])[0]) if rec.get(\"container-title\") else \"\"\n",
    "#         out[\"journal_abbrev\"] = normalize_text((rec.get(\"short-container-title\") or [\"\"])[0]) if rec.get(\"short-container-title\") else \"\"\n",
    "#         out[\"volume\"] = normalize_text(rec.get(\"volume\") or \"\")\n",
    "#         out[\"issue\"]  = normalize_text(rec.get(\"issue\") or \"\")\n",
    "#         out[\"pages\"]  = normalize_text(rec.get(\"page\") or \"\")\n",
    "#         out[\"doi\"]    = normalize_text(rec.get(\"DOI\") or \"\")\n",
    "#         out[\"cr_type\"]= normalize_text(rec.get(\"type\") or \"\")\n",
    "#         y, m = \"\", \"\"\n",
    "#         for src in (\"issued\",\"published-print\",\"published-online\"):\n",
    "#             dp = (rec.get(src) or {}).get(\"date-parts\")\n",
    "#             if dp:\n",
    "#                 y = str(dp[0][0])\n",
    "#                 if len(dp[0])>1: m = str(dp[0][1])\n",
    "#                 break\n",
    "#         out[\"year\"], out[\"month\"] = y, normalize_month_field(m)\n",
    "#     elif source == \"openalex\":\n",
    "#         out[\"title\"] = normalize_text(rec.get(\"display_name\") or rec.get(\"title\") or \"\")\n",
    "#         out[\"authors\"] = [normalize_text(a.get(\"author\", {}).get(\"display_name\") or \"\") for a in rec.get(\"authorships\", [])] if rec.get(\"authorships\") else []\n",
    "#         hv = rec.get(\"host_venue\", {}) if isinstance(rec.get(\"host_venue\"), dict) else {}\n",
    "#         out[\"journal_name\"] = normalize_text(hv.get(\"display_name\") or \"\")\n",
    "#         out[\"journal_abbrev\"] = normalize_text(hv.get(\"abbrev\") or \"\")\n",
    "#         out[\"doi\"] = normalize_text(rec.get(\"doi\") or \"\")\n",
    "#         out[\"volume\"] = normalize_text(rec.get(\"biblio\", {}).get(\"volume\") or \"\")\n",
    "#         out[\"issue\"]  = normalize_text(rec.get(\"biblio\", {}).get(\"issue\") or \"\")\n",
    "#         fp = rec.get(\"biblio\", {}).get(\"first_page\") or \"\"\n",
    "#         lp = rec.get(\"biblio\", {}).get(\"last_page\") or \"\"\n",
    "#         out[\"pages\"] = f\"{fp}-{lp}\" if fp and lp else normalize_text(fp or \"\")\n",
    "#         out[\"year\"]   = str(rec.get(\"publication_year\") or (rec.get(\"from_publication_date\") or \"\")[:4] or \"\")\n",
    "#         out[\"month\"]  = \"\"\n",
    "#         out[\"oa_is_proceedings\"] = \"proceedings\" in norm_for_compare(hv.get(\"display_name\") or \"\")\n",
    "#     elif source == \"semanticscholar\":\n",
    "#         out[\"title\"] = normalize_text(rec.get(\"title\") or \"\")\n",
    "#         out[\"authors\"] = [normalize_text(a.get(\"name\") or \"\") for a in rec.get(\"authors\", [])] if rec.get(\"authors\") else []\n",
    "#         out[\"journal_name\"] = normalize_text(rec.get(\"venue\") or (rec.get(\"publicationVenue\") or {}).get(\"name\") or \"\")\n",
    "#         out[\"journal_abbrev\"] = \"\"\n",
    "#         eid = rec.get(\"externalIds\") or {}\n",
    "#         out[\"doi\"] = normalize_text(eid.get(\"DOI\") or rec.get(\"doi\") or \"\")\n",
    "#         out[\"year\"]   = normalize_text(rec.get(\"year\") or \"\")\n",
    "#         out[\"month\"]  = \"\"\n",
    "#         out[\"s2_types\"] = [normalize_text(t) for t in (rec.get(\"publicationTypes\") or [])]\n",
    "#     elif source == \"pubmed\":\n",
    "#         out[\"title\"] = normalize_text(rec.get(\"title\") or rec.get(\"sorttitle\") or \"\")\n",
    "#         out[\"authors\"] = [normalize_text(a.get(\"name\")) for a in rec.get(\"authors\", []) if a.get(\"name\")] if rec.get(\"authors\") else []\n",
    "#         out[\"journal_name\"] = normalize_text((rec.get(\"fulljournalname\") or rec.get(\"source\") or \"\"))\n",
    "#         out[\"journal_abbrev\"] = normalize_text(rec.get(\"source\") or \"\")\n",
    "#         out[\"doi\"] = normalize_text((rec.get(\"elocationid\") or \"\").replace(\"doi:\",\"\").strip())\n",
    "#         out[\"volume\"] = normalize_text(rec.get(\"volume\") or \"\")\n",
    "#         out[\"issue\"]  = normalize_text(rec.get(\"issue\") or \"\")\n",
    "#         out[\"pages\"]  = normalize_text(rec.get(\"pages\") or \"\")\n",
    "#         out[\"year\"]   = normalize_text((rec.get(\"pubdate\") or \"\").split(\" \")[0])\n",
    "#         out[\"month\"]  = \"\"\n",
    "#     elif source == \"arxiv\":\n",
    "#         out[\"title\"] = normalize_text(rec.get(\"title\") or \"\")\n",
    "#         out[\"authors\"] = [normalize_text(a) for a in rec.get(\"authors\", [])]\n",
    "#         out[\"journal_name\"] = \"arXiv\"\n",
    "#         out[\"journal_abbrev\"] = \"arXiv\"\n",
    "#         out[\"doi\"] = normalize_text(rec.get(\"doi\") or \"\")\n",
    "#         out[\"year\"] = normalize_text(rec.get(\"year\") or \"\")\n",
    "#         out[\"month\"] = \"\"\n",
    "#         out[\"volume\"] = \"\"\n",
    "#         out[\"issue\"] = \"\"\n",
    "#         out[\"pages\"] = \"\"\n",
    "#     else:\n",
    "#         out.update({k:\"\" for k in (\"title\",\"authors\",\"journal_name\",\"journal_abbrev\",\"doi\",\"volume\",\"issue\",\"pages\",\"year\",\"month\")})\n",
    "#     return out\n",
    "\n",
    "# def score_candidate(extracted: Dict[str, Any], cand: Dict[str, Any]) -> float:\n",
    "#     score = 0.0\n",
    "#     ex_doi = normalize_text(extracted.get(\"doi\") or \"\").lower().replace(\"doi:\",\"\")\n",
    "#     ca_doi = normalize_text(cand.get(\"doi\") or \"\").lower().replace(\"doi:\",\"\")\n",
    "#     if ex_doi and ca_doi and ex_doi == ca_doi: score += 1.0\n",
    "#     score += 0.6 * token_similarity(extracted.get(\"title\") or \"\", cand.get(\"title\") or \"\")\n",
    "#     ex_auth = [a.split()[-1].lower() for a in authors_to_list(extracted.get(\"authors\")) if a.split()]\n",
    "#     ca_auth = [a.split()[-1].lower() for a in authors_to_list(cand.get(\"authors\")) if a.split()]\n",
    "#     if ex_auth and ca_auth:\n",
    "#         inter = len(set(ex_auth) & set(ca_auth))\n",
    "#         score += 0.2 * (inter / max(1, len(set(ex_auth) | set(ca_auth))))\n",
    "#     ey = str(extracted.get(\"year\") or \"\").strip()\n",
    "#     cy = str(cand.get(\"year\") or \"\").strip()\n",
    "#     if ey and cy and ey == cy: score += 0.1\n",
    "#     src_weight = {\"crossref\": 0.12, \"openalex\": 0.08, \"semanticscholar\": 0.06, \"pubmed\": 0.05, \"arxiv\": 0.03}\n",
    "#     score += src_weight.get(cand.get(\"source\",\"\"), 0.0)\n",
    "#     return score\n",
    "\n",
    "# # ============================ Type reconciliation ============================\n",
    "\n",
    "# TYPE_CANON = {\n",
    "#     \"journal-article\": \"journal article\",\n",
    "#     \"paper-conference\": \"conference paper\",\n",
    "#     \"proceedings-article\": \"conference paper\",\n",
    "#     \"book-chapter\": \"book chapter\",\n",
    "#     \"book\": \"book\",\n",
    "#     \"dataset\": \"dataset\",\n",
    "#     \"standard\": \"standard\",\n",
    "#     \"report\": \"technical report\",\n",
    "#     \"thesis\": \"thesis\",\n",
    "# }\n",
    "\n",
    "# def reconcile_type(initial_type: str, candidates: List[Dict[str, Any]], llm_vote: Optional[str]) -> str:\n",
    "#     votes = []\n",
    "#     if initial_type: votes.append(initial_type)\n",
    "#     if llm_vote: votes.append(llm_vote.lower())\n",
    "#     for c in candidates or []:\n",
    "#         if c[\"source\"] == \"crossref\":\n",
    "#             t = c.get(\"cr_type\",\"\")\n",
    "#             if t: votes.append(TYPE_CANON.get(t, t))\n",
    "#         elif c[\"source\"] == \"openalex\":\n",
    "#             if c.get(\"oa_is_proceedings\"): votes.append(\"conference paper\")\n",
    "#         elif c[\"source\"] == \"semanticscholar\":\n",
    "#             types = c.get(\"s2_types\") or []\n",
    "#             if any(\"conference\" in t for t in types): votes.append(\"conference paper\")\n",
    "#             if any(\"journal\" in t for t in types): votes.append(\"journal article\")\n",
    "#             if any(\"book\" in t for t in types): votes.append(\"book\")\n",
    "#         elif c[\"source\"] == \"arxiv\":\n",
    "#             votes.append(\"preprint\")\n",
    "#     from collections import Counter\n",
    "#     cnt = Counter([v.lower() for v in votes if v])\n",
    "#     if not cnt: return initial_type or \"other\"\n",
    "#     return cnt.most_common(1)[0][0]\n",
    "\n",
    "# # ============================ Verification Agents (threaded) ============================\n",
    "\n",
    "# def agent_journal(extracted: Dict[str, Any], best: Dict[str, Any]) -> Dict[str, Any]:\n",
    "#     ex_j = normalize_text(extracted.get(\"journal_name\") or \"\")\n",
    "#     ex_ab = normalize_text(extracted.get(\"journal_abbrev\") or \"\")\n",
    "#     be_j = normalize_text(best.get(\"journal_name\") or \"\")\n",
    "#     be_ab = normalize_text(best.get(\"journal_abbrev\") or \"\")\n",
    "#     sim_full = token_similarity(ex_j, be_j) if ex_j and be_j else 0.0\n",
    "#     sim_ab   = token_similarity(ex_ab, be_ab) if ex_ab and be_ab else 0.0\n",
    "#     ok = (sim_full >= 0.6) or (sim_ab >= 0.6) or (bool(ex_j) and not be_j)\n",
    "#     corr = {}\n",
    "#     if be_j and be_j != ex_j: corr[\"journal_name\"] = be_j\n",
    "#     if (be_ab and be_ab != ex_ab) or (not ex_ab and (be_ab or be_j)):\n",
    "#         corr[\"journal_abbrev\"] = be_ab or heuristic_abbrev(be_j or ex_j)\n",
    "#     return {\"ok\": ok, \"correction\": corr or None}\n",
    "\n",
    "# def agent_authors(extracted: Dict[str, Any], best: Dict[str, Any]) -> Dict[str, Any]:\n",
    "#     ex = authors_to_list(extracted.get(\"authors\"))\n",
    "#     be = authors_to_list(best.get(\"authors\"))\n",
    "#     if be:\n",
    "#         matches = 0\n",
    "#         for ea in ex:\n",
    "#             last = ea.split()[-1].lower() if ea.split() else \"\"\n",
    "#             if any((ba.split()[-1].lower() if ba.split() else \"\") == last for ba in be):\n",
    "#                 matches += 1\n",
    "#         required = max(1, int(0.5 * len(ex))) if ex else 1\n",
    "#         if matches >= required:\n",
    "#             if any(re.match(r\"^[A-Z]\\.\", p.split()[0]) if p.split() else False for p in ex[:3]):\n",
    "#                 return {\"ok\": True, \"correction\": {\"authors\": be}}\n",
    "#             return {\"ok\": True}\n",
    "#         return {\"ok\": False, \"correction\": {\"authors\": be}}\n",
    "#     return {\"ok\": bool(ex)}\n",
    "\n",
    "# def agent_title(extracted: Dict[str, Any], best: Dict[str, Any]) -> Dict[str, Any]:\n",
    "#     ex_t = normalize_text(extracted.get(\"title\") or \"\")\n",
    "#     be_t = normalize_text(best.get(\"title\") or \"\")\n",
    "#     desired = sentence_case(ex_t) if ex_t else \"\"\n",
    "#     if be_t:\n",
    "#         sim = token_similarity(ex_t, be_t)\n",
    "#         if sim >= 0.7:\n",
    "#             if ex_t != desired:\n",
    "#                 return {\"ok\": True, \"correction\": {\"title\": desired}}\n",
    "#             return {\"ok\": True}\n",
    "#         return {\"ok\": False, \"correction\": {\"title\": be_t}}\n",
    "#     else:\n",
    "#         if ex_t and ex_t != desired:\n",
    "#             return {\"ok\": False, \"correction\": {\"title\": desired}}\n",
    "#         return {\"ok\": bool(ex_t)}\n",
    "\n",
    "# def agent_year_month(extracted: Dict[str, Any], best: Dict[str, Any]) -> Dict[str, Any]:\n",
    "#     ex_y = str(extracted.get(\"year\") or \"\")\n",
    "#     ex_m = normalize_month_field(extracted.get(\"month\") or \"\")\n",
    "#     be_y = str(best.get(\"year\") or \"\")\n",
    "#     be_m = normalize_month_field(best.get(\"month\") or \"\")\n",
    "#     ok = True; corr = {}\n",
    "#     if be_y and be_y != ex_y: corr[\"year\"] = be_y; ok = False\n",
    "#     if be_m and be_m != ex_m: corr[\"month\"] = be_m; ok = False\n",
    "#     return {\"ok\": ok, \"correction\": corr or None}\n",
    "\n",
    "# def agent_vipd(extracted: Dict[str, Any], best: Dict[str, Any]) -> Dict[str, Any]:\n",
    "#     exv, exi, exp, exd = [normalize_text(extracted.get(k) or \"\") for k in (\"volume\",\"issue\",\"pages\",\"doi\")]\n",
    "#     bev, bei, bep, bed = [normalize_text(best.get(k) or \"\") for k in (\"volume\",\"issue\",\"pages\",\"doi\")]\n",
    "#     ok = True; corr = {}\n",
    "#     if bev and bev != exv: corr[\"volume\"] = bev; ok = False\n",
    "#     if bei and bei != exi: corr[\"issue\"]  = bei; ok = False\n",
    "#     if bep and bep != exp: corr[\"pages\"]  = bep; ok = False\n",
    "#     if bed and bed.lower().replace(\"doi:\",\"\") != exd.lower().replace(\"doi:\",\"\"):\n",
    "#         corr[\"doi\"] = bed; ok = False\n",
    "#     return {\"ok\": ok, \"correction\": corr or None}\n",
    "\n",
    "# def agent_presence(extracted: Dict[str, Any], best: Dict[str, Any]) -> Dict[str, Any]:\n",
    "#     return {\"ok\": bool(extracted.get(\"title\")) and bool(extracted.get(\"authors\"))}\n",
    "\n",
    "# # ============================ LangGraph State & Nodes ============================\n",
    "\n",
    "# class PipelineState(TypedDict, total=False):\n",
    "#     reference: str\n",
    "#     type: str\n",
    "#     extracted: Dict[str, Any]\n",
    "#     candidates: List[Dict[str, Any]]\n",
    "#     best: Dict[str, Any]\n",
    "#     verification: Dict[str, bool]\n",
    "#     suggestions: Dict[str, Any]\n",
    "#     corrections: List[Tuple[str, Any, Any]]\n",
    "#     formatted: str\n",
    "#     report: str\n",
    "#     attempts: int\n",
    "#     hops: int\n",
    "#     _made_changes_last_cycle: bool\n",
    "#     _cfg: PipelineConfig\n",
    "#     _llm: Any\n",
    "#     _http: Any\n",
    "#     _cache: Any\n",
    "#     _limiter: Any\n",
    "#     _sources: Any\n",
    "#     _llm_type_vote: Optional[str]\n",
    "#     csl_json: Dict[str, Any]\n",
    "#     bibtex: str\n",
    "#     _ver_score: int\n",
    "#     _stagnation: int\n",
    "#     _fp: str\n",
    "#     _fp_history: set\n",
    "#     _loop_detected: bool\n",
    "\n",
    "# async def _init_runtime(state: PipelineState) -> PipelineState:\n",
    "#     cfg = state.get(\"_cfg\") or CFG\n",
    "#     llm = LLMAdapter(cfg)\n",
    "#     http = httpx.AsyncClient(timeout=cfg.timeout_s) if httpx is not None else None\n",
    "#     cache = TTLCache(maxsize=1000, ttl=cfg.cache_ttl_s) if CACHE_AVAILABLE and TTLCache is not None else None\n",
    "#     limiter = asyncio.Semaphore(cfg.concurrency)\n",
    "#     sources = [\n",
    "#         CrossrefClient(cfg, client=http, limiter=limiter, cache=cache),\n",
    "#         OpenAlexClient(cfg, client=http, limiter=limiter, cache=cache),\n",
    "#         SemanticScholarClient(cfg, client=http, limiter=limiter, cache=cache),\n",
    "#         PubMedClient(cfg, client=http, limiter=limiter, cache=cache),\n",
    "#         ArxivClient(cfg, client=http, limiter=limiter, cache=cache),\n",
    "#     ]\n",
    "#     state.update({\n",
    "#         \"_cfg\": cfg, \"_llm\": llm, \"_http\": http, \"_cache\": cache,\n",
    "#         \"_limiter\": limiter, \"_sources\": sources,\n",
    "#     })\n",
    "#     state.setdefault(\"hops\", 0)\n",
    "#     state.setdefault(\"attempts\", 0)\n",
    "#     state.setdefault(\"_ver_score\", -1)\n",
    "#     state.setdefault(\"_stagnation\", 0)\n",
    "#     state.setdefault(\"_fp\", \"\")\n",
    "#     state.setdefault(\"_fp_history\", set())\n",
    "#     state.setdefault(\"_loop_detected\", False)\n",
    "#     state.setdefault(\"_made_changes_last_cycle\", False)\n",
    "#     return state\n",
    "\n",
    "# # --- Detect type using heuristics + LLM vote (once) ---\n",
    "# async def node_detect_type_async(state: PipelineState) -> PipelineState:\n",
    "#     ref = state[\"reference\"]\n",
    "#     rtype = \"other\"\n",
    "#     if re.search(r\"\\bvol\\.|no\\.|pp\\.\", ref, flags=re.I):\n",
    "#         rtype = \"journal article\"\n",
    "#     if re.search(r\"\\bin\\b.+(proc|conference|symposium|workshop)\", ref, flags=re.I):\n",
    "#         rtype = \"conference paper\"\n",
    "#     if re.search(r\"\\bISBN\\b\", ref, flags=re.I):\n",
    "#         rtype = \"book\"\n",
    "#     llm: LLMAdapter = state[\"_llm\"]\n",
    "#     vote = await llm.json(\n",
    "#         \"Classify this reference into one of: journal article, conference paper, book, book chapter, thesis, technical report, dataset, standard, software, other. \"\n",
    "#         \"Return JSON {\\\"type\\\": \\\"...\\\"}. Ref:\\n\" + ref\n",
    "#     )\n",
    "#     state[\"_llm_type_vote\"] = (vote or {}).get(\"type\")\n",
    "#     state[\"type\"] = reconcile_type(rtype, [], state[\"_llm_type_vote\"])\n",
    "#     return state\n",
    "\n",
    "# # --- Parse/extract using LLM-first with regex fallback ---\n",
    "# async def node_parse_extract_async(state: PipelineState) -> PipelineState:\n",
    "#     ref, rtype = state[\"reference\"], state[\"type\"]\n",
    "#     llm: LLMAdapter = state[\"_llm\"]\n",
    "#     prompt = (\n",
    "#         \"Parse the IEEE-style reference. Return STRICT JSON. Keys among:\\n\"\n",
    "#         \"title, authors (list or string), journal_name, journal_abbrev, conference_name,\\n\"\n",
    "#         \"volume, issue, pages, year, month, doi, publisher, location, edition, isbn, url.\\n\"\n",
    "#         \"Omit unknown keys. JSON ONLY.\\n\\n\"\n",
    "#         f\"Type hint: {rtype}\\nReference: {ref}\"\n",
    "#     )\n",
    "#     parsed = await llm.json(prompt)\n",
    "#     if not parsed:\n",
    "#         parsed = {}\n",
    "#         m = re.search(r\"“([^”]{3,})”|\\\"([^\\\"]{3,})\\\"\", ref)\n",
    "#         if m:\n",
    "#             parsed[\"title\"] = normalize_text(m.group(1) or m.group(2))\n",
    "#             prefix = ref[:m.start()]\n",
    "#             parsed[\"authors\"] = authors_to_list(prefix)\n",
    "#         dm = re.search(r\"(10\\.\\d{4,9}/[^\\s,;]+)\", ref, flags=re.I)\n",
    "#         if dm: parsed[\"doi\"] = dm.group(1)\n",
    "#         pm = re.search(r\"pp\\.?\\s*([\\d\\u2013\\u2014\\-]+)\", ref, flags=re.I)\n",
    "#         if pm: parsed[\"pages\"] = pm.group(1).replace(\"\\u2013\",\"-\").replace(\"\\u2014\",\"-\")\n",
    "#         vm = re.search(r\"vol\\.?\\s*([0-9A-Za-z]+)\", ref, flags=re.I)\n",
    "#         if vm: parsed[\"volume\"] = vm.group(1)\n",
    "#         im = re.search(r\"no\\.?\\s*([0-9A-Za-z]+)\", ref, flags=re.I)\n",
    "#         if im: parsed[\"issue\"] = im.group(1)\n",
    "#         y = re.search(r\"\\b(19|20)\\d{2}\\b\", ref)\n",
    "#         if y: parsed[\"year\"] = y.group(0)\n",
    "#         if m:\n",
    "#             after = ref[m.end():]\n",
    "#             jm = re.search(r\",\\s*([^,]+?),\", after)\n",
    "#             if jm: parsed[\"journal_name\"] = normalize_text(jm.group(1))\n",
    "#     if isinstance(parsed.get(\"authors\"), str):\n",
    "#         parsed[\"authors\"] = authors_to_list(parsed[\"authors\"])\n",
    "#     if parsed.get(\"month\"):\n",
    "#         parsed[\"month\"] = normalize_month_field(parsed[\"month\"])\n",
    "#     state[\"extracted\"] = parsed\n",
    "#     return state\n",
    "\n",
    "# # --- Multi-source lookup ---\n",
    "# async def node_multisource_lookup_async(state: PipelineState) -> PipelineState:\n",
    "#     ex = state[\"extracted\"]; sources = state[\"_sources\"]\n",
    "#     doi = normalize_text(ex.get(\"doi\") or \"\").lower().replace(\"doi:\",\"\")\n",
    "#     title = normalize_text(ex.get(\"title\") or \"\")\n",
    "#     out_norm: List[Dict[str, Any]] = []\n",
    "#     for s in sources:\n",
    "#         doi_ok = False\n",
    "#         try:\n",
    "#             if doi:\n",
    "#                 rec = await s.by_doi(doi)\n",
    "#                 if rec:\n",
    "#                     out_norm.append(normalize_candidate(s.NAME, rec))\n",
    "#                     doi_ok = True\n",
    "#         except Exception:\n",
    "#             pass\n",
    "#         try:\n",
    "#             # Avoid redundant title hits that can cause 429s if DOI already succeeded for this source\n",
    "#             if title and not doi_ok:\n",
    "#                 rec = await s.by_title(title)\n",
    "#                 if rec: out_norm.append(normalize_candidate(s.NAME, rec))\n",
    "#         except Exception:\n",
    "#             pass\n",
    "#     # dedup within same source\n",
    "#     dedup: Dict[Tuple[str, str], Dict[str, Any]] = {}\n",
    "#     for c in out_norm:\n",
    "#         key = (c[\"source\"], c.get(\"doi\") or c.get(\"title\") or \"\")\n",
    "#         dedup[key] = c\n",
    "#     state[\"candidates\"] = list(dedup.values())\n",
    "#     return state\n",
    "\n",
    "# def node_select_best(state: PipelineState) -> PipelineState:\n",
    "#     ex = state[\"extracted\"]; candidates = state.get(\"candidates\") or []\n",
    "#     if not candidates:\n",
    "#         state[\"best\"] = {}\n",
    "#         return state\n",
    "#     best, best_score = None, -1.0\n",
    "#     for c in candidates:\n",
    "#         sc = score_candidate(ex, c)\n",
    "#         if sc > best_score: best, best_score = c, sc\n",
    "#     state[\"best\"] = best or {}\n",
    "#     return state\n",
    "\n",
    "# def node_reconcile_type(state: PipelineState) -> PipelineState:\n",
    "#     state[\"type\"] = reconcile_type(state.get(\"type\",\"other\"), state.get(\"candidates\") or [], state.get(\"_llm_type_vote\"))\n",
    "#     return state\n",
    "\n",
    "# def node_verify_agents(state: PipelineState) -> PipelineState:\n",
    "#     ex = state[\"extracted\"]\n",
    "#     be = state.get(\"best\") or {}\n",
    "\n",
    "#     agents = [agent_journal, agent_authors, agent_title, agent_year_month, agent_vipd, agent_presence]\n",
    "#     results = {}\n",
    "#     with ThreadPoolExecutor(max_workers=CFG.agent_threads) as pool:\n",
    "#         fut_map = {pool.submit(a, ex, be): a.__name__ for a in agents}\n",
    "#         for fut in as_completed(fut_map):\n",
    "#             name = fut_map[fut]\n",
    "#             try:\n",
    "#                 results[name] = fut.result()\n",
    "#             except Exception as e:\n",
    "#                 logger.exception(\"Agent %s failed: %s\", name, e)\n",
    "#                 results[name] = {\"ok\": False}\n",
    "\n",
    "#     suggestions = {}\n",
    "#     for out in results.values():\n",
    "#         if out.get(\"correction\"):\n",
    "#             suggestions.update(out[\"correction\"])\n",
    "\n",
    "#     # trust agent VIPD result only (no truthy shortcut)\n",
    "#     vipd_ok = results.get(\"agent_vipd\", {}).get(\"ok\", False)\n",
    "#     ym_ok = results.get(\"agent_year_month\", {}).get(\"ok\", False)\n",
    "\n",
    "#     verification = {\n",
    "#         \"title\":          results.get(\"agent_title\", {}).get(\"ok\", False),\n",
    "#         \"authors\":        results.get(\"agent_authors\", {}).get(\"ok\", False),\n",
    "#         \"journal_name\":   results.get(\"agent_journal\", {}).get(\"ok\", False),\n",
    "#         \"journal_abbrev\": results.get(\"agent_journal\", {}).get(\"ok\", False),\n",
    "#         \"year\":           ym_ok,\n",
    "#         \"month\":          ym_ok,\n",
    "#         \"volume\":         vipd_ok,\n",
    "#         \"issue\":          vipd_ok,\n",
    "#         \"pages\":          vipd_ok,\n",
    "#         \"doi\":            vipd_ok,\n",
    "#         \"presence\":       results.get(\"agent_presence\", {}).get(\"ok\", False),\n",
    "#     }\n",
    "\n",
    "#     ver_score = sum(1 for v in verification.values() if v)\n",
    "#     last_score = state.get(\"_ver_score\", -1)\n",
    "#     stagnation = state.get(\"_stagnation\", 0)\n",
    "#     stagnation = 0 if ver_score > last_score else (stagnation + 1)\n",
    "\n",
    "#     state[\"_ver_score\"] = ver_score\n",
    "#     state[\"_stagnation\"] = stagnation\n",
    "#     state[\"verification\"] = verification\n",
    "#     state[\"suggestions\"] = suggestions\n",
    "#     state[\"hops\"] = (state.get(\"hops\") or 0) + 1\n",
    "\n",
    "#     # loop detection via fingerprint\n",
    "#     fp = fingerprint_state(ex, be, suggestions)\n",
    "#     hist = state.get(\"_fp_history\") or set()\n",
    "#     if fp in hist:\n",
    "#         state[\"_loop_detected\"] = True\n",
    "#     else:\n",
    "#         hist.add(fp)\n",
    "#         state[\"_fp_history\"] = hist\n",
    "#         state[\"_loop_detected\"] = False\n",
    "\n",
    "#     state[\"_fp\"] = fp\n",
    "#     return state\n",
    "\n",
    "# def node_apply_corrections(state: PipelineState) -> PipelineState:\n",
    "#     ex = dict(state[\"extracted\"])\n",
    "#     best = state.get(\"best\") or {}\n",
    "#     suggestions = state.get(\"suggestions\") or {}\n",
    "#     prev_fp = state.get(\"_fp\",\"\")\n",
    "#     changes: List[Tuple[str, Any, Any]] = []\n",
    "\n",
    "#     # authoritative merge\n",
    "#     for k in (\"title\",\"authors\",\"journal_name\",\"journal_abbrev\",\"volume\",\"issue\",\"pages\",\"doi\",\"year\",\"month\",\"conference_name\",\"publisher\",\"location\",\"edition\",\"isbn\",\"url\"):\n",
    "#         bv = best.get(k)\n",
    "#         if bv and normalize_text(ex.get(k)) != normalize_text(bv):\n",
    "#             changes.append((k, ex.get(k), bv)); ex[k] = bv\n",
    "\n",
    "#     # sentence case & authors list\n",
    "#     if ex.get(\"title\"):\n",
    "#         sc = sentence_case(ex[\"title\"])\n",
    "#         if sc != ex[\"title\"]:\n",
    "#             changes.append((\"title_sentence_case\", ex[\"title\"], sc)); ex[\"title\"] = sc\n",
    "#     if isinstance(ex.get(\"authors\"), str):\n",
    "#         al = authors_to_list(ex[\"authors\"])\n",
    "#         if al != ex[\"authors\"]:\n",
    "#             changes.append((\"authors_list\", ex[\"authors\"], al)); ex[\"authors\"] = al\n",
    "\n",
    "#     # agent suggestions\n",
    "#     for k, v in suggestions.items():\n",
    "#         if normalize_text(ex.get(k)) != normalize_text(v):\n",
    "#             changes.append((k, ex.get(k), v)); ex[k] = v\n",
    "\n",
    "#     # normalize month consistently\n",
    "#     if ex.get(\"month\"):\n",
    "#         newm = normalize_month_field(ex[\"month\"])\n",
    "#         if newm != ex[\"month\"]:\n",
    "#             changes.append((\"month_normalized\", ex[\"month\"], newm)); ex[\"month\"] = newm\n",
    "\n",
    "#     state[\"extracted\"] = ex\n",
    "#     state[\"corrections\"] = (state.get(\"corrections\") or []) + changes\n",
    "#     state[\"attempts\"] = (state.get(\"attempts\") or 0) + 1\n",
    "#     state[\"_made_changes_last_cycle\"] = bool(changes)\n",
    "\n",
    "#     # loop/stagnation update (fingerprint on new ex)\n",
    "#     sugg = state.get(\"suggestions\") or {}\n",
    "#     best_now = state.get(\"best\") or {}\n",
    "#     new_fp = fingerprint_state(ex, best_now, sugg)\n",
    "#     hist = state.get(\"_fp_history\") or set()\n",
    "#     if new_fp in hist:\n",
    "#         state[\"_loop_detected\"] = True\n",
    "#     else:\n",
    "#         hist.add(new_fp)\n",
    "#         state[\"_fp_history\"] = hist\n",
    "#         state[\"_loop_detected\"] = False\n",
    "#     state[\"_fp\"] = new_fp\n",
    "#     return state\n",
    "\n",
    "# async def node_llm_correct_async(state: PipelineState) -> PipelineState:\n",
    "#     ref = state[\"reference\"]; ex = state[\"extracted\"]; ver = state.get(\"verification\") or {}\n",
    "#     llm: LLMAdapter = state[\"_llm\"]\n",
    "#     prompt = (\n",
    "#         \"You are an IEEE reference corrector. Given raw reference, current JSON, and verification booleans, \"\n",
    "#         \"return STRICT JSON correcting the fields. Keys among: title, authors (list), journal_name, journal_abbrev, \"\n",
    "#         \"conference_name, volume, issue, pages, year, month, doi, publisher, location, edition, isbn, url. \"\n",
    "#         \"Omit unknown keys. JSON ONLY.\\n\\n\"\n",
    "#         f\"Raw: {ref}\\n\\nCurrent: {json.dumps(ex, ensure_ascii=False)}\\n\\nVerification: {json.dumps(ver)}\"\n",
    "#     )\n",
    "#     patch = await llm.json(prompt)\n",
    "#     if patch:\n",
    "#         if isinstance(patch.get(\"authors\"), str):\n",
    "#             patch[\"authors\"] = authors_to_list(patch[\"authors\"])\n",
    "#         if patch.get(\"month\"):\n",
    "#             patch[\"month\"] = normalize_month_field(patch[\"month\"])\n",
    "#         ex2 = dict(ex); changes = []\n",
    "#         for k, v in patch.items():\n",
    "#             if normalize_text(ex2.get(k)) != normalize_text(v):\n",
    "#                 changes.append((k, ex2.get(k), v)); ex2[k] = v\n",
    "#         state[\"extracted\"] = ex2\n",
    "#         state[\"corrections\"] = (state.get(\"corrections\") or []) + changes\n",
    "#         state[\"_made_changes_last_cycle\"] = state.get(\"_made_changes_last_cycle\", False) or bool(changes)\n",
    "#         best = state.get(\"best\") or {}\n",
    "#         sugg = state.get(\"suggestions\") or {}\n",
    "#         state[\"_fp\"] = fingerprint_state(ex2, best, sugg)\n",
    "#     return state\n",
    "\n",
    "# def node_enrich_from_best(state: PipelineState) -> PipelineState:\n",
    "#     ex = dict(state[\"extracted\"]); be = state.get(\"best\") or {}\n",
    "#     for k in (\"journal_abbrev\",\"journal_name\",\"volume\",\"issue\",\"pages\",\"year\",\"month\",\"doi\",\"conference_name\",\"publisher\",\"location\",\"edition\",\"isbn\",\"url\",\"title\",\"authors\"):\n",
    "#         if not ex.get(k) and be.get(k):\n",
    "#             ex[k] = be.get(k)\n",
    "#     if ex.get(\"month\"):\n",
    "#         ex[\"month\"] = normalize_month_field(ex[\"month\"])\n",
    "#     state[\"extracted\"] = ex\n",
    "#     return state\n",
    "\n",
    "# def node_format_reference(state: PipelineState) -> PipelineState:\n",
    "#     ex = state[\"extracted\"]; rtype = (state[\"type\"] or \"other\").lower()\n",
    "#     A = authors_to_list(ex.get(\"authors\") or [])\n",
    "#     A_fmt = format_authors_ieee_list(A)\n",
    "#     title_raw = ex.get(\"title\") or \"\"\n",
    "#     title = sentence_case(title_raw)\n",
    "#     journal = ex.get(\"journal_abbrev\") or ex.get(\"journal_name\") or \"\"\n",
    "#     vol = normalize_text(ex.get(\"volume\") or \"\")\n",
    "#     issue = normalize_text(ex.get(\"issue\") or \"\")\n",
    "#     pages = normalize_text(ex.get(\"pages\") or \"\").replace(\"–\",\"-\").replace(\"—\",\"-\")\n",
    "#     if \"-\" in pages: pages = pages.replace(\"-\", \"–\")\n",
    "#     year = normalize_text(ex.get(\"year\") or \"\")\n",
    "#     month = normalize_month_field(ex.get(\"month\") or \"\")\n",
    "#     month_disp = MONTHS_NAME.get(month, month) if month else \"\"\n",
    "#     doi = normalize_text(ex.get(\"doi\") or \"\")\n",
    "#     if doi: doi = ensure_doi_prefix(doi)\n",
    "#     conf = normalize_text(ex.get(\"conference_name\") or \"\")\n",
    "#     loc = normalize_text(ex.get(\"location\") or \"\")\n",
    "#     pub = normalize_text(ex.get(\"publisher\") or \"\")\n",
    "#     edition = normalize_text(ex.get(\"edition\") or \"\")\n",
    "#     isbn = normalize_text(ex.get(\"isbn\") or \"\")\n",
    "\n",
    "#     parts: List[str] = []\n",
    "#     if A_fmt: parts.append(A_fmt)\n",
    "\n",
    "#     # Title rendering varies by type (avoid duplication for books)\n",
    "#     include_quoted_title = rtype not in (\"book\",)\n",
    "#     if include_quoted_title and title:\n",
    "#         parts.append(f\"\\\"{title}\\\"\")\n",
    "\n",
    "#     if rtype in (\"journal article\",\"journal\"):\n",
    "#         if journal: parts.append(f\"*{journal}*\")\n",
    "#         if vol: parts.append(f\"vol. {vol}\")\n",
    "#         if issue: parts.append(f\"no. {issue}\")\n",
    "#         if pages: parts.append(f\"pp. {pages}\")\n",
    "#         date = \" \".join([m for m in [month_disp, year] if m]).strip()\n",
    "#         if date: parts.append(date)\n",
    "#         if doi: parts.append(doi)\n",
    "\n",
    "#     elif rtype == \"conference paper\":\n",
    "#         venue = conf or journal or \"Proceedings\"\n",
    "#         if venue: parts.append(f\"in *{venue}*\")\n",
    "#         if loc: parts.append(loc)\n",
    "#         if pages: parts.append(f\"pp. {pages}\")\n",
    "#         date = \" \".join([m for m in [month_disp, year] if m]).strip()\n",
    "#         if date: parts.append(date)\n",
    "#         if doi: parts.append(doi)\n",
    "\n",
    "#     elif rtype == \"preprint\":\n",
    "#         # arXiv-style preprint\n",
    "#         if title and not include_quoted_title:  # not reached, but safe\n",
    "#             parts.append(f\"\\\"{title}\\\"\")\n",
    "#         parts.append(\"preprint\")\n",
    "#         if journal and \"arxiv\" in journal.lower():\n",
    "#             parts.append(journal)\n",
    "#         date = \" \".join([m for m in [month_disp, year] if m]).strip()\n",
    "#         if date: parts.append(date)\n",
    "#         if doi: parts.append(doi)\n",
    "\n",
    "#     elif rtype == \"book\":\n",
    "#         # For books, italicize title as the main element\n",
    "#         if title: parts.append(f\"*{title}*\")\n",
    "#         if edition: parts.append(f\"{edition} ed.\")\n",
    "#         imprint = f\"{loc}: {pub}\" if (loc and pub) else (loc or pub)\n",
    "#         if imprint: parts.append(imprint)\n",
    "#         if year: parts.append(year)\n",
    "#         if isbn: parts.append(f\"ISBN: {isbn}\")\n",
    "#         if doi: parts.append(doi)\n",
    "\n",
    "#     elif rtype in (\"book chapter\",\"chapter\"):\n",
    "#         book_title = normalize_text(ex.get(\"book_title\") or conf or journal)\n",
    "#         if book_title: parts.append(f\"in *{book_title}*\")\n",
    "#         if pages: parts.append(f\"pp. {pages}\")\n",
    "#         if pub: parts.append(pub)\n",
    "#         date = \" \".join([m for m in [month_disp, year] if m]).strip()\n",
    "#         if date: parts.append(date)\n",
    "#         if doi: parts.append(doi)\n",
    "\n",
    "#     else:\n",
    "#         venue = journal or conf or pub\n",
    "#         if venue: parts.append(venue)\n",
    "#         date = \" \".join([m for m in [month_disp, year] if m]).strip()\n",
    "#         if date: parts.append(date)\n",
    "#         if vol: parts.append(f\"vol. {vol}\")\n",
    "#         if issue: parts.append(f\"no. {issue}\")\n",
    "#         if pages: parts.append(f\"pp. {pages}\")\n",
    "#         if doi: parts.append(doi)\n",
    "\n",
    "#     state[\"formatted\"] = (\", \".join([p for p in parts if p]) + \".\").replace(\" ,\", \",\")\n",
    "#     return state\n",
    "\n",
    "# def to_csl_json(ex: Dict[str, Any], rtype: str) -> Dict[str, Any]:\n",
    "#     typemap = {\n",
    "#         \"journal article\": \"article-journal\",\n",
    "#         \"conference paper\": \"paper-conference\",\n",
    "#         \"book\": \"book\",\n",
    "#         \"book chapter\": \"chapter\",\n",
    "#         \"thesis\": \"thesis\",\n",
    "#         \"technical report\": \"report\",\n",
    "#         \"dataset\": \"dataset\",\n",
    "#         \"standard\": \"standard\",\n",
    "#         \"software\": \"software\",\n",
    "#         \"preprint\": \"article\",  # many tools treat arXiv as article/preprint\n",
    "#     }\n",
    "#     t = typemap.get(rtype, \"article\")\n",
    "\n",
    "#     authors_list = authors_to_list(ex.get(\"authors\"))\n",
    "#     authors = []\n",
    "#     for a in authors_list:\n",
    "#         parts = a.split()\n",
    "#         family = parts[-1] if parts else a\n",
    "#         given = \" \".join(parts[:-1]) if len(parts) > 1 else \"\"\n",
    "#         authors.append({\"family\": safe_str(family), \"given\": safe_str(given)})\n",
    "\n",
    "#     year_raw = ex.get(\"year\")\n",
    "#     month_raw = normalize_month_field(ex.get(\"month\") or \"\")\n",
    "#     issued = None\n",
    "#     try:\n",
    "#         y = int(year_raw) if safe_str(year_raw).isdigit() else None\n",
    "#         if y is not None:\n",
    "#             if month_raw and month_raw.isdigit():\n",
    "#                 issued = {\"date-parts\": [[y, int(month_raw)]]}\n",
    "#             else:\n",
    "#                 issued = {\"date-parts\": [[y]]}\n",
    "#     except Exception:\n",
    "#         issued = None\n",
    "\n",
    "#     csl = {\n",
    "#         \"type\": t,\n",
    "#         \"title\": safe_str(ex.get(\"title\")),\n",
    "#         \"author\": authors if authors else None,\n",
    "#         \"container-title\": safe_str(ex.get(\"journal_name\") or ex.get(\"conference_name\")),\n",
    "#         \"volume\": safe_str(ex.get(\"volume\")),\n",
    "#         \"issue\": safe_str(ex.get(\"issue\")),\n",
    "#         \"page\": safe_str(ex.get(\"pages\")),\n",
    "#         \"DOI\": safe_str(ex.get(\"doi\")),\n",
    "#         \"publisher\": safe_str(ex.get(\"publisher\")),\n",
    "#         \"issued\": issued,\n",
    "#     }\n",
    "#     return {k: v for k, v in csl.items() if v}\n",
    "\n",
    "# def to_bibtex(ex: Dict[str, Any], rtype: str) -> str:\n",
    "#     def bibtex_escape(s: str) -> str:\n",
    "#         return (\n",
    "#             s.replace(\"\\\\\", \"\\\\textbackslash{}\")\n",
    "#              .replace(\"{\", \"\\\\{\").replace(\"}\", \"\\\\}\")\n",
    "#              .replace(\"&\", \"\\\\&\").replace(\"%\", \"\\\\%\")\n",
    "#              .replace(\"$\", \"\\\\$\").replace(\"#\", \"\\\\#\").replace(\"_\", \"\\\\_\")\n",
    "#         )\n",
    "\n",
    "#     # robust key\n",
    "#     authors_list = authors_to_list(ex.get(\"authors\"))\n",
    "#     first_author_last = \"\"\n",
    "#     if authors_list:\n",
    "#         parts = authors_list[0].split()\n",
    "#         first_author_last = parts[-1] if parts else authors_list[0]\n",
    "\n",
    "#     year_str = safe_str(ex.get(\"year\"))\n",
    "#     fa_key = re.sub(r\"[^A-Za-z0-9]+\", \"\", safe_str(first_author_last)) or \"ref\"\n",
    "#     yr_key = re.sub(r\"[^0-9]+\", \"\", year_str)\n",
    "#     if not yr_key:\n",
    "#         basis = safe_str(ex.get(\"doi\")) or safe_str(ex.get(\"title\"))\n",
    "#         h = hashlib.sha1(basis.encode(\"utf-8\", \"ignore\")).hexdigest()[:6] if basis else \"000000\"\n",
    "#         yr_key = h\n",
    "#     key = f\"{fa_key}{yr_key}\"\n",
    "\n",
    "#     entry_type = {\n",
    "#         \"journal article\": \"article\",\n",
    "#         \"conference paper\": \"inproceedings\",\n",
    "#         \"book\": \"book\",\n",
    "#         \"book chapter\": \"incollection\",\n",
    "#         \"thesis\": \"phdthesis\",\n",
    "#         \"technical report\": \"techreport\",\n",
    "#         \"dataset\": \"misc\",\n",
    "#         \"standard\": \"misc\",\n",
    "#         \"software\": \"misc\",\n",
    "#         \"preprint\": \"misc\",\n",
    "#     }.get(rtype, \"misc\")\n",
    "\n",
    "#     A = \" and \".join(authors_list)\n",
    "#     title = safe_str(ex.get(\"title\"))\n",
    "#     journal = safe_str(ex.get(\"journal_name\"))\n",
    "#     conf = safe_str(ex.get(\"conference_name\"))\n",
    "#     volume = safe_str(ex.get(\"volume\"))\n",
    "#     number = safe_str(ex.get(\"issue\"))\n",
    "#     pages = safe_str(ex.get(\"pages\"))\n",
    "#     year = safe_str(ex.get(\"year\"))\n",
    "#     doi = safe_str(ex.get(\"doi\"))\n",
    "#     publisher = safe_str(ex.get(\"publisher\"))\n",
    "#     isbn = safe_str(ex.get(\"isbn\"))\n",
    "\n",
    "#     fields: List[Tuple[str, str]] = []\n",
    "#     if entry_type == \"article\":\n",
    "#         fields += [(\"author\", A), (\"title\", title), (\"journal\", journal),\n",
    "#                    (\"volume\", volume), (\"number\", number), (\"pages\", pages),\n",
    "#                    (\"year\", year), (\"doi\", doi)]\n",
    "#     elif entry_type == \"inproceedings\":\n",
    "#         fields += [(\"author\", A), (\"title\", title), (\"booktitle\", conf or journal),\n",
    "#                    (\"pages\", pages), (\"year\", year), (\"doi\", doi)]\n",
    "#     elif entry_type == \"book\":\n",
    "#         fields += [(\"author\", A), (\"title\", title), (\"publisher\", publisher),\n",
    "#                    (\"year\", year), (\"isbn\", isbn), (\"doi\", doi)]\n",
    "#     elif entry_type == \"incollection\":\n",
    "#         fields += [(\"author\", A), (\"title\", title), (\"booktitle\", conf or journal),\n",
    "#                    (\"pages\", pages), (\"publisher\", publisher), (\"year\", year), (\"doi\", doi)]\n",
    "#     elif entry_type == \"phdthesis\":\n",
    "#         fields += [(\"author\", A), (\"title\", title), (\"school\", publisher or conf or journal),\n",
    "#                    (\"year\", year), (\"doi\", doi)]\n",
    "#     elif entry_type == \"techreport\":\n",
    "#         fields += [(\"author\", A), (\"title\", title), (\"institution\", publisher or conf or journal),\n",
    "#                    (\"year\", year), (\"doi\", doi)]\n",
    "#     else:  # misc\n",
    "#         fields += [(\"author\", A), (\"title\", title), (\"howpublished\", conf or journal or publisher),\n",
    "#                    (\"year\", year), (\"note\", doi)]\n",
    "\n",
    "#     fields = [(k, bibtex_escape(v)) for k, v in fields if v]\n",
    "#     body = \",\\n  \".join([f\"{k} = {{{v}}}\" for k, v in fields])\n",
    "#     return f\"@{entry_type}{{{key},\\n  {body}\\n}}\"\n",
    "\n",
    "# def node_build_exports(state: PipelineState) -> PipelineState:\n",
    "#     ex = state[\"extracted\"]; rtype = (state[\"type\"] or \"other\").lower()\n",
    "#     state[\"csl_json\"] = to_csl_json(ex, rtype)\n",
    "#     state[\"bibtex\"] = to_bibtex(ex, rtype)\n",
    "#     return state\n",
    "\n",
    "# def node_build_report(state: PipelineState) -> PipelineState:\n",
    "#     changes = state.get(\"corrections\") or []\n",
    "#     ver = state.get(\"verification\") or {}\n",
    "#     lines = []\n",
    "#     if not changes:\n",
    "#         lines.append(\"No corrections were necessary; reference matched authoritative sources.\")\n",
    "#     else:\n",
    "#         lines.append(\"Corrections (field: old → new):\")\n",
    "#         for f, old, new in changes:\n",
    "#             lines.append(f\"- {f}: '{old}' → '{new}'\")\n",
    "#     failed = [k for k, v in ver.items() if not v]\n",
    "#     if failed:\n",
    "#         lines.append(\"Fields still needing attention: \" + \", \".join(sorted(failed)))\n",
    "#     else:\n",
    "#         lines.append(\"All verification checks passed after corrections.\")\n",
    "#     state[\"report\"] = \"\\n\".join(lines)\n",
    "#     return state\n",
    "\n",
    "# # --- Cleanup node to close clients\n",
    "# async def node_cleanup_async(state: PipelineState) -> PipelineState:\n",
    "#     try:\n",
    "#         if state.get(\"_http\") is not None:\n",
    "#             await state[\"_http\"].aclose()\n",
    "#     except Exception:\n",
    "#         pass\n",
    "#     try:\n",
    "#         llm = state.get(\"_llm\")\n",
    "#         if llm and llm.provider == \"ollama\" and getattr(llm, \"_client\", None) is not None:\n",
    "#             await llm._client.aclose()\n",
    "#     except Exception:\n",
    "#         pass\n",
    "#     return state\n",
    "\n",
    "# # ============================ Graph Build & Routing ============================\n",
    "\n",
    "# def should_exit(state: PipelineState) -> bool:\n",
    "#     cfg = state.get(\"_cfg\") or CFG\n",
    "#     if state.get(\"_loop_detected\"): return True\n",
    "#     if (state.get(\"hops\") or 0) >= cfg.max_hops: return True\n",
    "#     if (state.get(\"attempts\") or 0) >= cfg.max_correction_rounds: return True\n",
    "#     if (state.get(\"_stagnation\") or 0) >= cfg.stagnation_patience: return True\n",
    "#     ver = state.get(\"verification\") or {}\n",
    "#     return bool(ver) and all(ver.values())\n",
    "\n",
    "# def build_graph(cfg: PipelineConfig = CFG) -> StateGraph:\n",
    "#     g = StateGraph(PipelineState)\n",
    "\n",
    "#     # nodes\n",
    "#     g.add_node(\"InitRuntime\", _init_runtime)\n",
    "#     g.add_node(\"DetectType\", node_detect_type_async)\n",
    "#     g.add_node(\"ParseExtract\", node_parse_extract_async)\n",
    "#     g.add_node(\"MultiSourceLookup\", node_multisource_lookup_async)\n",
    "#     g.add_node(\"SelectBest\", node_select_best)\n",
    "#     g.add_node(\"ReconcileType\", node_reconcile_type)\n",
    "#     g.add_node(\"VerifyAgents\", node_verify_agents)\n",
    "\n",
    "#     g.add_node(\"ApplyCorrections\", node_apply_corrections)\n",
    "#     g.add_node(\"LLMCorrect\", node_llm_correct_async)\n",
    "#     g.add_node(\"EnrichFromBest\", node_enrich_from_best)\n",
    "\n",
    "#     g.add_node(\"FormatReference\", node_format_reference)\n",
    "#     g.add_node(\"BuildExports\", node_build_exports)\n",
    "#     g.add_node(\"BuildReport\", node_build_report)\n",
    "#     g.add_node(\"Cleanup\", node_cleanup_async)\n",
    "\n",
    "#     # linear backbone\n",
    "#     g.add_edge(START, \"InitRuntime\")\n",
    "#     g.add_edge(\"InitRuntime\", \"DetectType\")\n",
    "#     g.add_edge(\"DetectType\", \"ParseExtract\")\n",
    "#     g.add_edge(\"ParseExtract\", \"MultiSourceLookup\")\n",
    "#     g.add_edge(\"MultiSourceLookup\", \"SelectBest\")\n",
    "#     g.add_edge(\"SelectBest\", \"ReconcileType\")\n",
    "#     g.add_edge(\"ReconcileType\", \"VerifyAgents\")\n",
    "\n",
    "#     # conditional: if done -> format; else repair loop\n",
    "#     def route_after_verify(state: PipelineState) -> str:\n",
    "#         return \"FormatReference\" if should_exit(state) else \"ApplyCorrections\"\n",
    "#     g.add_conditional_edges(\"VerifyAgents\", route_after_verify, {\n",
    "#         \"FormatReference\": \"FormatReference\",\n",
    "#         \"ApplyCorrections\": \"ApplyCorrections\",\n",
    "#     })\n",
    "\n",
    "#     # repair path (bounded by guards)\n",
    "#     g.add_edge(\"ApplyCorrections\", \"LLMCorrect\")\n",
    "#     g.add_edge(\"LLMCorrect\", \"EnrichFromBest\")\n",
    "#     g.add_edge(\"EnrichFromBest\", \"MultiSourceLookup\")\n",
    "\n",
    "#     # terminal leg\n",
    "#     g.add_edge(\"FormatReference\", \"BuildExports\")\n",
    "#     g.add_edge(\"BuildExports\", \"BuildReport\")\n",
    "#     g.add_edge(\"BuildReport\", \"Cleanup\")\n",
    "#     g.add_edge(\"Cleanup\", END)\n",
    "\n",
    "#     return g\n",
    "\n",
    "# # ============================ Mermaid Rendering ============================\n",
    "\n",
    "# MERMAID_DAG = r'''flowchart TD\n",
    "# A[Init Runtime] --> B[Detect Type (Heuristics + LLM)]\n",
    "# B --> C[Parse & Extract (LLM-first)]\n",
    "# C --> D[Fetch Candidates (Crossref/OpenAlex/S2/PubMed/arXiv)]\n",
    "# D --> E[Select Best (Consensus Scoring)]\n",
    "# E --> T[Reconcile Type]\n",
    "# T --> F[Verification Agents (Threaded) + Progress Metrics]\n",
    "# F -->|repair| G[Apply Corrections (Authority + Agents)]\n",
    "# G --> I[LLM Correction (JSON-only)]\n",
    "# I --> X[Enrich From Best]\n",
    "# X --> D2[Re-Fetch Candidates]\n",
    "# D2 --> E2[Re-Select Best]\n",
    "# E2 --> T2[Reconcile Type]\n",
    "# T2 --> F2[Re-Verify + loop/stagnation guards]\n",
    "# F -->|exit| H[Format IEEE]\n",
    "# H --> J[Build CSL-JSON & BibTeX]\n",
    "# J --> R[Human Report]\n",
    "# style H fill:#e0f7fa,stroke:#006064,stroke-width:1px\n",
    "# style R fill:#f1f8e9,stroke:#33691e,stroke-width:1px\n",
    "# style D fill:#fff3e0,stroke:#e65100,stroke-width:1px\n",
    "# style F fill:#ede7f6,stroke:#4527a0,stroke-width:1px\n",
    "# style T fill:#e8f5e9,stroke:#2e7d32,stroke-width:1px\n",
    "# '''\n",
    "\n",
    "# def show_mermaid_inline(mermaid_code: str) -> None:\n",
    "#     html = f\"\"\"\n",
    "#     <div class=\"mermaid\">\n",
    "#     {mermaid_code}\n",
    "#     </div>\n",
    "#     <script>\n",
    "#       (function() {{\n",
    "#         function init() {{\n",
    "#           mermaid.initialize({{startOnLoad:true}});\n",
    "#         }}\n",
    "#         if (!window.mermaid) {{\n",
    "#           var s = document.createElement('script');\n",
    "#           s.src = 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js';\n",
    "#           s.onload = init;\n",
    "#           document.head.appendChild(s);\n",
    "#         }} else {{\n",
    "#           init();\n",
    "#         }}\n",
    "#       }})();\n",
    "#     </script>\n",
    "#     \"\"\"\n",
    "#     display(HTML(html))\n",
    "\n",
    "# def show_mermaid_kroki(mermaid_code: str) -> None:\n",
    "#     \"\"\"Try Kroki SVG; always also show fenced mermaid for environments that support it.\"\"\"\n",
    "#     display(Markdown(f\"```mermaid\\n{mermaid_code}\\n```\"))\n",
    "#     if httpx is None:\n",
    "#         return\n",
    "#     try:\n",
    "#         r = httpx.post(\"https://kroki.io/mermaid/svg\", content=mermaid_code.encode(\"utf-8\"), timeout=10.0,\n",
    "#                        headers={\"Content-Type\": \"text/plain\"})\n",
    "#         logger.info(\"HTTP Request: POST https://kroki.io/mermaid/svg %s\", r.status_code)\n",
    "#         if r.status_code == 200:\n",
    "#             display(SVG(r.content))\n",
    "#         else:\n",
    "#             if r.status_code == 400:\n",
    "#                 print(\"Kroki 400: Mermaid syntax or styling may be invalid. \"\n",
    "#                       \"If so, try removing `style` lines or simplify labels.\")\n",
    "#             else:\n",
    "#                 print(f\"Kroki error: {r.status_code}: {r.text[:200]}\")\n",
    "#     except Exception as e:\n",
    "#         logger.info(\"Mermaid SVG render failed: %s\", e)\n",
    "\n",
    "# # ============================ Build/compile ============================\n",
    "\n",
    "# graph = build_graph(CFG)\n",
    "# compiled = graph.compile()\n",
    "\n",
    "# # Show both inline JS and Kroki fallback (one of them will work)\n",
    "# show_mermaid_inline(MERMAID_DAG)\n",
    "# show_mermaid_kroki(MERMAID_DAG)\n",
    "\n",
    "\n",
    "# examples = [\n",
    "#     'F.-J. Lin, P.-H. Shen, S.-L. Yang, and P. H. Chou, “Recurrent radial basis function network-based fuzzy neural network control for permanent-magnet linear synchronous motor servo drive,” IEEE Trans. on Magnetics, vol. 42, no. 11, Nov. 2006.',\n",
    "#     'P. S. Sastry, G. Santharam, and K. P. Unnikrishnan, “Memory neuron networks for identification and control of dynamical systems,” IEEE Trans. Neural Netw., vol. 5, no. 2, pp. 306–319, May 1994, doi: 10.1109/72.279193.',\n",
    "#     'K. C. Apaza and J. M. López, “The non-linear relationship between carbon dioxide emissions, financial development and energy consumption in developing European and Central Asian economies,” Environ. Sci. Pollut. Res. Int., vol. 28, no. 44, pp. 63330–63345, Jul. 2021, doi: 10.1007/s11356-021-15225-2.',\n",
    "#     'A. Vaswani et al., \"Attention Is All You Need\", in NeurIPS, 2017.'\n",
    "# ]\n",
    "\n",
    "# async def run_one(reference: str, recursion_limit: Optional[int] = None) -> Dict[str, Any]:\n",
    "#     state: PipelineState = {\"reference\": reference}\n",
    "#     out = await compiled.ainvoke(state, config={\"recursion_limit\": recursion_limit or CFG.recursion_limit})\n",
    "#     return out\n",
    "\n",
    "# async def run_examples():\n",
    "#     for ref in examples:\n",
    "#         result = await run_one(ref)\n",
    "#         print(\"\\n=== Result ===\")\n",
    "#         print(\"Resolved Type:\", result.get(\"type\"))\n",
    "#         print(\"Formatted:\", result.get(\"formatted\"))\n",
    "#         print(\"Verification OK:\", all((result.get(\"verification\") or {}).values()) if result.get(\"verification\") else False)\n",
    "#         print(\"Report:\\n\", result.get(\"report\"))\n",
    "#         print(\"CSL-JSON:\", json.dumps(result.get(\"csl_json\"), indent=2, ensure_ascii=False))\n",
    "#         print(\"BibTeX:\\n\", result.get(\"bibtex\"))\n",
    "\n",
    "# ENV_SAMPLE = \"\"\"\n",
    "# # One of these providers is enough:\n",
    "# OPENAI_API_KEY=sk-...\n",
    "# OPENAI_MODEL=gpt-4o-mini\n",
    "\n",
    "# # OR Azure OpenAI:\n",
    "# AZURE_OPENAI_API_KEY=...\n",
    "# AZURE_OPENAI_ENDPOINT=https://<your-resource>.openai.azure.com\n",
    "# AZURE_OPENAI_DEPLOYMENT=gpt-4o-base\n",
    "# OPENAI_API_VERSION=2024-06-01\n",
    "\n",
    "# # OR Anthropic:\n",
    "# ANTHROPIC_API_KEY=...\n",
    "# ANTHROPIC_MODEL=claude-3-5-sonnet-20240620\n",
    "\n",
    "# # OR Ollama (local):\n",
    "# OLLAMA_BASE_URL=http://localhost:11434\n",
    "# OLLAMA_MODEL=llama3.2\n",
    "\n",
    "# # Optional tuning:\n",
    "# IEEE_REF_TIMEOUT=12\n",
    "# IEEE_REF_CONCURRENCY=8\n",
    "# IEEE_REF_CACHE_TTL=3600\n",
    "# IEEE_REF_MAX_CORR=3\n",
    "# IEEE_REF_MAX_HOPS=12\n",
    "# IEEE_REF_STAGNATION=2\n",
    "# IEEE_REF_AGENT_THREADS=6\n",
    "# IEEE_REF_LOG_LEVEL=INFO\n",
    "# IEEE_REF_RECURSION_LIMIT=60\n",
    "# \"\"\".strip()\n",
    "\n",
    "# print(\"\\nTip: create a .env with the keys you use. Example:\\n\", ENV_SAMPLE)\n",
    "\n",
    "# await run_examples()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
